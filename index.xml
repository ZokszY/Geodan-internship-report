<?xml version="1.0" encoding="utf-8"?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving
and Interchange DTD v1.2 20190208//EN" "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
  <front>
    <article-meta>
      <title-group>
        <article-title>Tree object detection using airborne images and LiDAR
point clouds</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Bry</surname>
            <given-names>Alexandre</given-names>
          </name>
          <string-name>Alexandre Bry</string-name>
          <email>alexandre.bry.21@polytechnique.org</email>
          <role vocab="https://credit.niso.org" vocab-term="writing – original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">writing</role>
          <xref ref-type="aff" rid="aff-1">a</xref>
          <xref ref-type="aff" rid="aff-2">b</xref>
          <xref ref-type="corresp" rid="cor-1">*</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1">
        <institution content-type="dept">Département
d’informatique</institution>
        <institution-wrap>
          <institution>École polytechnique</institution>
        </institution-wrap>
        <city>Palaiseau</city>
        <country>France</country>
        <ext-link ext-link-type="uri" xlink:href="https://portail.polytechnique.edu/informatique/fr/page-daccueil">https://portail.polytechnique.edu/informatique/fr/page-daccueil</ext-link>
      </aff>
      <aff id="aff-2">
        <institution content-type="dept">Research</institution>
        <institution-wrap>
          <institution>Geodan B.V.</institution>
        </institution-wrap>
        <city>Amsterdam</city>
        <country>Netherlands</country>
        <ext-link ext-link-type="uri" xlink:href="https://research.geodan.nl/">https://research.geodan.nl/</ext-link>
      </aff>
      <author-notes>
        <corresp id="cor-1">alexandre.bry.21@polytechnique.org</corresp>
      </author-notes>
      <history/>
      <abstract>
        <p>This is the abstract. It can be on multiple lines and contain
<bold>Markdown</bold>.</p>
      </abstract>
      <kwd-group kwd-group-type="author">
        <kwd>tree detection</kwd>
        <kwd>deep learning</kwd>
      </kwd-group>
    </article-meta>
  </front>
  <body>
    <sec id="introduction">
      <title>Introduction</title>
      <p>The goal of the internship was to study the possibility of
  combining LiDAR point clouds and aerial images in a deep learning
  model to identify individual trees. The two types of data are indeed
  complementary, as point clouds capture geometric shapes, while images
  capture colors. However, combining them into a format that allows a
  model to handle them simultaneously is not a straightforward task
  because they inherently have a very different spatial repartition and
  encoding.</p>
      <p>In this work, I focused on one specific deep learning model, and
  tried to improve it by using more information from the LiDAR point
  cloud. To do this, I had to create my own tree annotations dataset,
  with which I also tried to study the ability of this new model to
  detect trees that are covered by other trees.</p>
    </sec>
    <sec id="state-of-the-art">
      <title>State-of-the-art</title>
      <sec id="computer-vision-tasks-related-to-trees">
        <title>Computer vision tasks related to trees</title>
        <p>Before talking about models and datasets, let’s define properly
    the task that this project focused on, in the midst of all the
    various computer vision tasks, and specifically those related to
    tree detection.</p>
        <p>The first main differentiation between tree recognition tasks
    comes from the acquisition of the data. There are some very
    different tasks and methods using either ground data or
    aerial/satellite data. This is especially true when focusing on
    urban trees, since a lot of street view data is available
    (<xref alt="Arevalo-Ramirez et al. 2024" rid="ref-urban-trees" ref-type="bibr">Arevalo-Ramirez
    et al. 2024</xref>).</p>
        <p>This leads to the second variation, which is related to the kind
    of environment that we are interested in. There are mainly three
    types of environments, which among other things, influence the
    organization of the trees in space: urban areas, tree plantations
    and forests. This is important, because the tasks and the difficulty
    depends on the type of environment. Tree plantations are much easier
    to work with than completely wild forests, while urban areas contain
    various levels of difficulty ranging from alignment trees to private
    and disorganized gardens and parks. For this project, we mainly
    focused on urban areas, but everything should still be applicable to
    tree plantations and forests.</p>
        <p>Then, the four fundamental computer vision tasks have their
    application when dealing with trees
    (<xref alt="Safonova et al. 2021" rid="ref-olive-tree" ref-type="bibr">Safonova
    et al. 2021</xref>):</p>
        <list list-type="bullet">
          <list-item>
            <p>Classification, although this is quite rare for airborne tree
        applications since there are multiple trees on each image most
        of the time</p>
          </list-item>
          <list-item>
            <p>Detection, which consists in detecting objects and placing
        boxes around them</p>
          </list-item>
          <list-item>
            <p>Semantic segmentation, which consists in associating a label
        to every pixel of an image</p>
          </list-item>
          <list-item>
            <p>Instance segmentation, which consists in adding a layer of
        complexity to semantic segmentation by also differentiating
        between the different instances of each class</p>
          </list-item>
        </list>
        <p>These generic tasks can be extended by trying to get more
    information about the trees. The most common information are the
    species and the height, but some models also try to predict the
    health of the trees, or their carbon stock.</p>
        <p>In this work, the task that is tackled is the detection of trees,
    with a special classification between several labels related to the
    discrepancies between the different kinds of data. The kind of model
    that is used would also have allowed to focus on some more advanced
    tasks, by replacing detection with instance segmentation and asking
    the model to also predict the species. But due to the difficulties
    regarding the dataset, a simpler task with a simpler dataset was
    used, without compromising the ability to experiment with different
    possible improvements of the model. The difficulties and the
    experiments are developed below.</p>
      </sec>
      <sec id="datasets">
        <title>Datasets</title>
        <sec id="sec-sota-datasets-requirements">
          <title>Requirements</title>
          <p>Before presenting the different promising datasets and the
      reasons why they were not fully usable for the project, let’s
      enumerate the different conditions and requirements for the tree
      instance segmentation task:</p>
          <list list-type="bullet">
            <list-item>
              <p>Multiple types of data:</p>
              <list list-type="bullet">
                <list-item>
                  <p>Aerial RGB images</p>
                </list-item>
                <list-item>
                  <p>LiDAR point clouds (preferably aerial)</p>
                </list-item>
                <list-item>
                  <p>(Optional) Aerial infrared (CIR) images</p>
                </list-item>
              </list>
            </list-item>
            <list-item>
              <p>Tree crown annotations or bounding boxes</p>
            </list-item>
            <list-item>
              <p>High-enough resolution:</p>
              <list list-type="bullet">
                <list-item>
                  <p>For images, about 25 cm</p>
                </list-item>
                <list-item>
                  <p>For point clouds, about 10 cm</p>
                </list-item>
              </list>
            </list-item>
          </list>
          <p>Here are the explanations for these requirements. As for the
      types of data, RGB images and point clouds are required to
      experiment on the ability of the model to combine the two very
      different kinds of information they hold. Having infrared data as
      well could be beneficial, but it was not necessary. Regarding tree
      annotations, it was necessary to have a way to spatially identify
      them individually, using crown contours or simply bounding boxes.
      Since the model outputs bounding boxes, any kind of other format
      could easily be transformed to bounding boxes. Finally, the
      resolution had to be high enough to identify individual trees and
      be able to really use the data. For the point clouds especially,
      the whole idea was to see if and how the topology of the trees
      could be learnt, using at least the trunks and even the biggest
      branches if possible. Therefore, even if they are not really
      comparable, this is the reason why the required resolution is more
      precise for the point clouds.</p>
          <p>Unfortunately, none of the datasets that I found matched all
      these criteria. Furthermore, I didn’t find any overlapping
      datasets that I could merge to create a dataset with all the
      required types of data. In the next parts, I will go through the
      different kinds of datasets that exist, the reasons why they did
      not really fit for the project and the ideas I got when searching
      for a way to use them.</p>
        </sec>
        <sec id="existing-tree-datasets">
          <title>Existing tree datasets</title>
          <p>As explained above, there were quite a lot of requirements to
      fulfill to have a complete dataset usable for the task. This means
      that almost all the available datasets were missing something, as
      they were mainly focusing on using one kind of data and trying to
      make the most out of it, instead of trying to use all the types of
      data together.</p>
          <p>The most comprehensive list of tree annotations datasets was
      published in OpenForest
      (<xref alt="Ouaknine et al. 2023" rid="ref-OpenForest" ref-type="bibr">Ouaknine
      et al. 2023</xref>). FoMo-Bench
      (<xref alt="Bountos, Ouaknine, and Rolnick 2023" rid="ref-FoMo-Bench" ref-type="bibr">Bountos,
      Ouaknine, and Rolnick 2023</xref>) also lists several interesting
      datasets, even though most of them can also be found in
      OpenForest. Without enumerating all of them, there were multiple
      kinds of datasets that all have their own flaws regarding the
      requirements I was looking for.</p>
          <p>Firstly, there are the forest inventories. TALLO
      (<xref alt="Jucker et al. 2022" rid="ref-TALLO" ref-type="bibr">Jucker
      et al. 2022</xref>) is probably the most interesting one in this
      category, because it contains a lot of spatial information about
      almost 500K trees, with their locations, their crown radii and
      their heights. Therefore, everything needed to localize trees is
      in the dataset. However, I didn’t manage to find RGB images or
      LiDAR point clouds of the areas where the trees are located,
      making it impossible to use these annotations to train tree
      detection.</p>
          <p>Secondly, there are the RGB datasets. ReforesTree
      (<xref alt="Reiersen et al. 2022" rid="ref-ReforesTree" ref-type="bibr">Reiersen
      et al. 2022</xref>) and MillionTrees
      (<xref alt="B. Weinstein 2023" rid="ref-MillionTrees" ref-type="bibr">B.
      Weinstein 2023</xref>) are two of them and the quality of their
      images are high. The only drawback of these datasets is obviously
      that they don’t provide any kind of point cloud, which make them
      unsuitable for the task.</p>
          <p>Thirdly, there are the LiDAR datasets, such as
      (<xref alt="Kalinicheva et al. 2022" rid="ref-WildForest3D" ref-type="bibr">Kalinicheva
      et al. 2022</xref>) and
      (<xref alt="Puliti et al. 2023" rid="ref-FOR-instance" ref-type="bibr">Puliti
      et al. 2023</xref>). Similarly to RGB datasets, they lack one of
      the data source for the task I worked on. But unlike them, they
      have the advantage that the missing data could be much easier to
      acquire from another source, since RGB aerial or satellite images
      are much more common than LiDAR point clouds. However, this
      solution was abandoned for two main reasons. First it is quite
      challenging to find the exact locations where the point clouds
      were acquired. Then, even when the location is known, it is often
      in the middle of a forest where the quality of satellite imagery
      is very low.</p>
          <p>Finally, I also found two datasets that had RGB and LiDAR
      components. The first one is MDAS
      (<xref alt="Hu et al. 2023" rid="ref-MDAS" ref-type="bibr">Hu et
      al. 2023</xref>). This benchmark dataset encompasses RGB images,
      hyperspectral images and Digital Surface Models (DSM). There were
      however two major flaws. The obvious one was that this dataset was
      created with land semantic segmentation tasks in mind, so there
      was no tree annotations. The less obvious one was that a DSM is
      not a point cloud, even though it is some kind of 3D information
      and was often created using a LiDAR point cloud. As a consequence,
      I would have been very limited in my ability to use the point
      cloud.</p>
          <p>The only real dataset with RGB and LiDAR came from NEON
      (<xref alt="B. Weinstein, Marconi, and White 2022" rid="ref-NEONdata" ref-type="bibr">B.
      Weinstein, Marconi, and White 2022</xref>). This dataset contains
      exactly all the data I was looking for, with RGB images,
      hyperspectral images and LiDAR point clouds. With 30975 tree
      annotations, it is also a quite large dataset, spanning across
      multiple various forests. The reason why I decided not to use it
      despite all this is that at the beginning of the project, I
      thought that the quality of the images and the point clouds was
      too low. Looking back on this decision, I think that I probably
      could have worked with this dataset and gotten great results. This
      would have saved me the time spent annotating the trees for my own
      dataset, which I will talk more about later. My decision was also
      influenced by the quality of the images and the point clouds
      available in the Netherlands, which I will talk about in the next
      section.</p>
        </sec>
        <sec id="public-data">
          <title>Public data</title>
          <p>After rejecting all the available datasets I had found, the
      only solution I had left was to create my own dataset. I won’t
      dive too much in this process that I will explain in
      <xref alt="Section 3" rid="sec-dataset">Section 3</xref>. I just
      want to mention all the publicly available datasets that I used or
      could have used to create this custom dataset.</p>
          <p>For practical reasons, the two countries where I mostly
      searched for available data are France and the Netherlands. I was
      looking for three different data types independently:</p>
          <list list-type="bullet">
            <list-item>
              <p>RGB (and if possible CIR) images</p>
            </list-item>
            <list-item>
              <p>LiDAR point clouds</p>
            </list-item>
            <list-item>
              <p>Tree annotations</p>
            </list-item>
          </list>
          <p>These three types of data are available in similar ways in both
      countries, although the Netherlands have a small edge over France.
      RGB images are really easy to find in France with the BD ORTHO
      (<xref alt="Institut national de l’information géographique et forestière (IGN) 2021" rid="ref-IGN_BD_ORTHO" ref-type="bibr">Institut
      national de l’information géographique et forestière (IGN)
      2021</xref>) and in the Netherlands with the Luchtfotos
      (<xref alt="Beeldmateriaal Nederland 2024" rid="ref-Luchtfotos" ref-type="bibr">Beeldmateriaal
      Nederland 2024</xref>), but the resolution is better in the
      Netherlands (8 cm vs 20 cm). Hyperspectral images are also
      available in both countries, although for those the resolution is
      only 25 cm in the Netherlands.</p>
          <p>As for LiDAR point clouds, the Netherlands have a small edge
      over France, because they have already completed their forth
      version covering the whole country with AHN4
      (<xref alt="Actueel Hoogtebestand Nederland 2020" rid="ref-AHN4" ref-type="bibr">Actueel
      Hoogtebestand Nederland 2020</xref>), and are working on the fifth
      version. In France, data acquisition for the first LiDAR point
      cloud covering the whole country started a few years ago
      (<xref alt="Institut national de l’information géographique et forestière (IGN) 2020" rid="ref-IGN_LiDAR_HD" ref-type="bibr">Institut
      national de l’information géographique et forestière (IGN)
      2020</xref>). It is not yet finished, even though data is already
      available for half of the country. The other advantage of the data
      from Netherlands regarding LiDAR point clouds is that all flights
      are performed during winter, which allows light beams to penetrate
      more deeply in trees and reach trunks and branches. This is not
      the case in France.</p>
          <p>The part that is missing in both countries is related to tree
      annotations. Many municipalities have datasets containing
      information about all the public trees they handle. This is for
      example the case for Amsterdam
      (<xref alt="Gemeente Amsterdam 2024" rid="ref-amsterdam_trees" ref-type="bibr">Gemeente
      Amsterdam 2024</xref>) and Bordeaux
      (<xref alt="Bordeaux Métropole 2024" rid="ref-bordeaux_trees" ref-type="bibr">Bordeaux
      Métropole 2024</xref>). However, these datasets cannot really be
      used as ground truth for a custom dataset for several reasons.
      First, many of them do not contain coordinates indicating the
      position of each tree in the city. Then, even those that contain
      coordinates are most of the time missing any kind of information
      allowing to deduce a bounding box for the tree crowns. Finally,
      even if they did contain everything, they only focus on public
      trees, and are missing every single tree located in a private
      area. Since public and private areas are obviously imbricated in
      all cities, it means that any area we try to train the model on
      would be missing all the private trees, making the training
      process impossible because we cannot have only a partial
      annotation of images.</p>
          <p>The other tree annotation source that we could have used is
      Boomregister
      (<xref alt="Coöperatief Boomregister U.A. 2014" rid="ref-boomregister" ref-type="bibr">Coöperatief
      Boomregister U.A. 2014</xref>). This work covers the whole of the
      Netherlands, including public and private trees. However, the
      precision of the masks is far from perfect, and many trees are
      missing or incorrectly segmented, especially when they are less
      than 9 m heigh or have a crown diameter smaller than 4 m.
      Therefore, even it is a very impressive piece of work, we thought
      that it could not be used as training data for a deep learning
      models due to its biases and imperfections.</p>
        </sec>
        <sec id="sec-sota-dataset-augment">
          <title>Dataset augmentation techniques</title>
          <p>When a dataset is too small to train a model, there are several
      ways of artificially enlarging it.</p>
          <p>The most common way to do it is to randomly apply deterministic
      or random transformations to the data, during the training
      process, to be able to generate several unique and different
      realistic data instances from one real data instance. There are a
      lot of different transformations that can be applied to images,
      divided into two categories: pixel-level and spatial-level
      (<xref alt="Buslaev et al. 2020" rid="ref-albumentations" ref-type="bibr">Buslaev
      et al. 2020</xref>). Pixel-level transformations modify the value
      of individual pixels, by applying different filters, such as
      random noise, color shifts and more complex effects like fog and
      sun flare. Spatial-level transformations modify the spatial
      arrangement of the image, without changing the pixel values. In
      other words, these transformations move the pixels in the image.
      The transformations range from simple rotations and croppings to
      complex spatial distortions. In the end, all these transformations
      are simply producing one artificial image out of one real
      image.</p>
          <p>Another way to enlarge a dataset is to instead generate
      completely new input data sharing the same properties as the
      initial dataset. This can be done using Generative Adversarial
      Networks (GAN). These models usually have two parts, a generator
      and a discriminator, which are trained in parallel. The generator
      learns to produce realistic artificial data, while the
      discriminator learns to identify real data and artificial data
      produced by the generator. If the training is successful, we can
      then use the generator and random seeds to generate random but
      realistic artificial data similar to the dataset. This method has
      for example been successfully used to generate artificial tree
      height maps
      (<xref alt="Sun et al. 2022" rid="ref-gan_data_augment" ref-type="bibr">Sun
      et al. 2022</xref>).</p>
        </sec>
      </sec>
      <sec id="algorithms-and-models">
        <title>Algorithms and models</title>
        <p>In this section, the different algorithms and methods are grouped
    according to the type of data they use as input.</p>
        <sec id="images-only">
          <title>Images only</title>
          <p>Then, there are methods that perform tree detection using only
      visible or hyperspectral images or both. Several different
      algorithms have been developed to analytically delineate tree
      crowns from RGB images, by using the particular shape of the trees
      and its effect on images
      (<xref alt="Gomes and Maillard 2016" rid="ref-rgb_analytical" ref-type="bibr">Gomes
      and Maillard 2016</xref>). Without diving into the details, here
      are a few of them. The watershed algorithm identifies trees to
      inverted watersheds in the grey-scale image and tree crowns
      frontiers are found by incrementally flooding the watersheds
      (<xref alt="Vincent and Soille 1991" rid="ref-watershed" ref-type="bibr">Vincent
      and Soille 1991</xref>). The local maxima filtering uses the
      intensity of the pixels in the grey-scale image to identify the
      brightest points locally and use them as treetops
      (<xref alt="Wulder, Niemann, and Goodenough 2000" rid="ref-local-maximum" ref-type="bibr">Wulder,
      Niemann, and Goodenough 2000</xref>). Reversely, the
      valley-following algorithm uses the darkest pixels which are
      considered as the junctions between the trees since shaded areas
      are the lower part of the tree crowns
      (<xref alt="Gougeon et al. 1998" rid="ref-valley-following" ref-type="bibr">Gougeon
      et al. 1998</xref>). Another interesting algorithm is template
      matching. This algorithm simulates the appearance of simple tree
      templates with the light effects, and tries to identify similar
      patterns in the grey-scale image
      (<xref alt="Pollock 1996" rid="ref-template-matching" ref-type="bibr">Pollock
      1996</xref>). Combinations of these techniques and others have
      also been proposed.</p>
          <p>But with the recent developments of deep learning in image
      analysis, deep learning models are increasingly used to detect
      trees using RGB images. In some cases, deep learning is used to
      extract features that can then be the input of one of the
      algorithms described above. One example is the use of two neural
      networks to predict masks, outlines and distance transforms which
      can then be the input of a watershed algorithm
      (<xref alt="Freudenberg, Magdon, and Nölke 2022" rid="ref-rgb-dl-watershed" ref-type="bibr">Freudenberg,
      Magdon, and Nölke 2022</xref>). In other cases, a deep learning
      model is responsible of directly detecting tree masks or bounding
      boxes, often using CNNs, given the images
      (<xref alt="B. G. Weinstein et al. 2020" rid="ref-DeepForest" ref-type="bibr">B.
      G. Weinstein et al. 2020</xref>).</p>
        </sec>
        <sec id="lidar-only">
          <title>LiDAR only</title>
          <p>Some of the methods to identify individual trees use LiDAR data
      only. There are a lot of different ways to use and analyze point
      clouds, but the one that is mostly used for trees is based on
      height maps, or Canopy Height Models (CHM).</p>
          <p>A CHM is a raster computed as the subtraction of the Digital
      Terrain Model (DTM) to the Digital Surface Model (DSM). What it
      means is that a CHM contains the height above ground of the
      highest point in the area corresponding to each pixel. This CHM
      can for example be used as the input raster for the watershed
      algorithm, as it contains the height values that can be used to
      determine local maxima
      (<xref alt="Kwak et al. 2007" rid="ref-lidar_watershed" ref-type="bibr">Kwak
      et al. 2007</xref>). A lot of different analytical methods and
      variations of the simple CHM were proposed to perform individual
      tree detection, but in the end, most of them still the concept of
      local maxima
      (<xref alt="Eysn et al. 2015" rid="ref-lidar_benchmark" ref-type="bibr">Eysn
      et al. 2015</xref>;
      <xref alt="Wang et al. 2016" rid="ref-lidar_benchmark_2" ref-type="bibr">Wang
      et al. 2016</xref>). A CHM can also be used as the input of any
      kind of convolutional neural network (CNN) because it is shaped
      exactly like any image. This allows to use a lot of different
      techniques usually applied to object detection in images.</p>
          <p>Then, even though I finally used an approach similar to the
      CHM, I want to mention other kinds of deep learning techniques
      that exist and could potentially leverage all the information
      contained in a point cloud. These techniques can be divided in two
      categories: projection-based and point-based methods
      (<xref alt="Diab, Kashef, and Shaker 2022" rid="ref-lidar_classification" ref-type="bibr">Diab,
      Kashef, and Shaker 2022</xref>). The main difference between the
      two is that projection-based techniques are based on grids while
      point-based methods take unstructured point clouds as input. Among
      projection-based methods, the most basic method is 2D CNN, which
      is how CHM can be processed. Then, multiview representation tries
      to tackle the 3D aspect by projecting the point cloud in multiple
      directions before merging them together. To really deal with 3D
      data, volumetric grid representation consists in using 3D
      occupancy grids, which are processed using 3D CNNs. Among
      point-based methods, there are methods based on PointNet, which
      are able to extract features and perform the classical computer
      vision tasks by taking point clouds as input. Finally,
      Convolutional Point Networks use a continuous generalization of
      convolutions to apply convolution kernels to arbitrarily
      distributed point clouds.</p>
        </sec>
        <sec id="lidar-and-images">
          <title>LiDAR and images</title>
          <p>Let’s now talk about the models of interest for this work,
      which are machine learning pipelines using both LiDAR point cloud
      data and RGB images.</p>
          <p>The first pipeline
      (<xref alt="Qin et al. 2022" rid="ref-lidar_rgb_wst" ref-type="bibr">Qin
      et al. 2022</xref>) uses a watershed algorithm to extract crown
      boundaries, before extracting individual tree features from the
      LiDAR point cloud, hyperspectral and RGB images. These features
      are then used by a random forest classifier to identify which
      species the tree belongs to. This pipeline therefore makes the
      most out of all data to identify species, but sticks to an
      improved variant of the watershed for individual tree
      segmentation, which only uses a CHM raster.</p>
          <p>Other works focused on using only one model that is able to
      take both the CHM and the RGB data as input and combine them to
      make the most out of all the available data. Among other models,
      there are for example ACE R-CNN
      (<xref alt="Li et al. 2022" rid="ref-lidar_rgb_acnet" ref-type="bibr">Li
      et al. 2022</xref>), an evolution of Mask region-based convolution
      neural network (Mask R-CNN) and AMF GD YOLOv8
      (<xref alt="Zhong et al. 2024" rid="ref-amf_gd_yolov8" ref-type="bibr">Zhong
      et al. 2024</xref>), an evolution of YOLOv8. These two models have
      proven to give much better results when using both the images and
      the LiDAR data as a CHM thant when using only one of them.</p>
        </sec>
      </sec>
    </sec>
    <sec id="objectives-and-motivations">
      <title>Objectives and motivations</title>
      <p>In this section, I will explain the objectives that I set for this
  internship and the motivations that led to them.</p>
      <sec id="data-and-model">
        <title>Data and model</title>
        <p>The basis for this internship was to look at deep learning models
    to detect trees using LiDAR and aerial images. In fourth months, it
    would have been difficult to dive into the literature, think about a
    completely new approach and develop it. Therefore, I wanted to find
    an interesting and not too complicated deep learning model, and try
    a few changes that would hopefully improve the results.</p>
        <p>This idea was also reinforced by the decision to create my own
    dataset for two reasons. The first reason was the small number of
    openly available tree annotation datasets which contained both LiDAR
    and RGB data. I therefore thought that creating a new dataset and
    making it available could be a great contribution. The second reason
    was to have more control over the definition and the characteristics
    of the dataset, to be able to experiment on the detection of
    specific trees.</p>
      </sec>
      <sec id="sec-obj-covered_trees">
        <title>Covered trees</title>
        <p>The main thing that I wanted to experiment on was the possibility
    to make a better use of the LiDAR point cloud to be able to detect
    covered trees. Covered trees are the trees which are located
    partially or completely under another tree’s crown. This makes them
    impossible to completely delineate when using only data that is
    visible from above. These trees are not meaningless or negligible,
    because as demonstrated in this paper
    (<xref alt="Wang et al. 2016" rid="ref-lidar_benchmark_2" ref-type="bibr">Wang
    et al. 2016</xref>), they can represent up to 50% of the trees in a
    forest.</p>
        <p>However, doing this implied being able to process them on the
    whole pipeline. In practice, covered trees are never annotated in
    all the datasets that are created using only RGB images, because
    they are simply not visible. This means that creating my own dataset
    was the only solution to have a dataset containing really all trees
    including covered trees and be able to easily identify them.</p>
      </sec>
      <sec id="multiple-layers-of-chm">
        <title>Multiple layers of CHM</title>
        <p>Being able to find covered trees meant finding a way to find more
    information out of the LiDAR point cloud than what is contained by
    the CHM. In fact, the CHM only contains a very small part of the
    point cloud and doesn’t really benefit from the 3D shape that is
    contained in the point cloud. This is particularly true when the
    point cloud is acquired in a season where trees don’t have their
    leaves, because the LiDAR then goes deep into the tree more easily,
    and can find the trunk and many of the largest branches.</p>
        <p>Therefore, getting information below the tree crown surface was
    mandatory to find covered trees. But it could also be helpful for
    the model to find better separations between each tree, thanks to
    having access to the branches and the trunks. Even though I didn’t
    end up asking the model to also identify the species, this is
    another task that could have been improved a lot if the model could
    use the architecture of the branches.</p>
        <p>To do this, I wanted to stick with a simple solution that would
    integrate well with the initial model and wouldn’t require too many
    changes. The idea I implemented is therefore very simple. Instead of
    having only one CHM raster, I would have multiple layers, each
    focusing on a different height interval. There are many ways to do
    this, but due to a lack of time, I only really tried what seemed to
    me the easiest and most straightforward way to do it, which consists
    in removing all the points above a certain height threshold, and
    compute the CHM with the points that are left. When doing this for
    multiple height thresholds, we get an interesting view of what the
    point cloud looks like at multiple levels, which gives a lot more
    information about the organization of the point cloud. Another way
    to do this, which is used in the third method of this paper
    (<xref alt="Eysn et al. 2015" rid="ref-lidar_benchmark" ref-type="bibr">Eysn
    et al. 2015</xref>), would be instead to use the previous CHM by
    removing all the points that are in the interval between the CHM
    height and 0.5 m below, before computing an additional layer. It
    could be interesting to see if this method works better than
    dropping the points at pre-determined heights.</p>
      </sec>
    </sec>
    <sec id="sec-dataset">
      <title>Dataset creation</title>
      <p>The highest resolution of the CHM which keeps a high enough quality
  depends entirely on the density of the point cloud. Also, depending on
  the season when the point cloud is acquired, using a CHM might imply
  throwing away the majority of the information contained in the point
  cloud.</p>
      <sec id="definition-and-content">
        <title>Definition and content</title>
        <p>As explained in the section
    <xref alt="Section 1.2.1" rid="sec-sota-datasets-requirements">Section 1.2.1</xref>,
    the main requirements of the dataset that I wanted to create were to
    contain at the same time LiDAR data, RGB data and CIR data, with
    simple bounding box annotations for all trees. And as explained in
    <xref alt="Section 2.2" rid="sec-obj-covered_trees">Section 2.2</xref>,
    all trees means also annotating trees that are partially or
    completely covered by other trees.</p>
        <p>Then, to make the most out of the point cloud resolution and the
    RGB images resolution, I decided to use a CHM resolution of 8 cm,
    which is also the resolution of the RGB images. However, the
    resolution of CIR images is 25 cm, which made it less optimal, but
    still usable.</p>
        <p>To be able to get results even with a small dataset, I decided to
    focus on one specific area, to limit the diversity of trees and
    environments to something that could hopefully still be learnt with
    a small dataset. Therefore, the whole dataset is currently inside of
    a 1 km × 1 km square around Geodan office, in Amsterdam. It contains
    2726 annotated trees spread over 241 images of size 640 px × 640 px
    i.e. 51.2 m × 51.2 m. All tree annotations have at least a bounding
    box, and some of them have a more accurate polygon representing the
    shape of the crown. There are four classes, which I will detail in
    the next section
    <xref alt="Section 3.2" rid="sec-dataset-challenges">Section 3.2</xref>,
    and each tree belongs to one class.</p>
        <p>Annotating all these trees took me about 100 hours, with a very
    high variation of the time spent on each tree depending on the
    complexity of the area.</p>
      </sec>
      <sec id="sec-dataset-challenges">
        <title>Challenges and solutions</title>
        <p>The creation of this dataset raised a number of challenges. The
    first one was the interval of time between the acquisition of the
    different types of data. While the point cloud data dated from 2020,
    the RGB images were acquired in 2023. It would have been possible to
    use images from 2021 or 2022 with the same resolution, but the
    quality of the 2023 images was much better. Consequently, there were
    a certain amount of changes regarding trees between these two
    periods of acquisition. Some large trees were cut off, while small
    trees were planted, sometimes even at the position of old trees that
    were previously cut off in the same time frame. For this reason, a
    non negligible number of trees were either present only in the point
    cloud, or only in the images. To try to handle this situation, I
    created two new class labels corresponding to these situation. This
    amounted up to 4 class labels:</p>
        <list list-type="bullet">
          <list-item>
            <p>“Tree”: trees which are visible in the point cloud and the
        images</p>
          </list-item>
          <list-item>
            <p>“Tree_LiDAR”: trees which are visible in the point cloud only
        but would be visible in the images if they had been there during
        the acquisition</p>
          </list-item>
          <list-item>
            <p>“Tree_RGB”: trees which are visible in the images only but
        would be visible in the point cloud if they had been there
        during the acquisition</p>
          </list-item>
          <list-item>
            <p>“Tree_covered”: trees that are visible in the point cloud
        only because they are covered by other trees.</p>
          </list-item>
        </list>
        <p>All these labels theoretically correspond to situations that have
    no intersection, even though it is more complicated in practice.</p>
        <p>The next challenge was the misalignment of images and point
    cloud. This misalignment comes from the images not being perfectly
    orthonormal. Point clouds don’t have this problem, because the data
    is acquired and represented in 3D, but images have to be projected
    to a 2D plane after being acquired with an angle that is not
    perfectly orthogonal to the plane. Despite the post-processing that
    was surely performed on the images, they are therefore not perfect,
    and there is a shift between the positions of each object in the
    point cloud and in the images. This shift cannot really be solved,
    because it depends on the position. Because of this misalignment, a
    choice had to be made as to where tree annotations should be placed,
    using either the point clouds or the RGB images. I chose to the RGB
    images as it is simpler to visualize and annotate, but there was not
    really a perfect choice.</p>
        <p>Finally, the last challenge comes from the definition of what we
    consider as a tree and what we don’t. There are two main
    sub-problems. The first one comes from the threshold to set between
    bushes and trees. Large bushes can be much larger than small trees,
    and sometimes have a similar shape. Therefore, it is hard to keep
    coherent rules when annotating them. The second sub-problem comes
    from multi-stemmed and close trees. It can be very difficult to see,
    even with the point cloud, if a there is only one tree with two or
    more trunks dividing at the bottom, or multiple trees which are
    simply close to one another. (Un)fortunately I know that I was not
    the only one to face this problem because it was also mentioned in
    another paper
    (<xref alt="B. G. Weinstein et al. 2019" rid="ref-DeepForestBefore" ref-type="bibr">B.
    G. Weinstein et al. 2019</xref>). In the end, it was just an
    unsolvable problem for which the most important was to remain
    consistent in the whole dataset.</p>
      </sec>
      <sec id="augmentation-methods">
        <title>Augmentation methods</title>
        <p>Dataset augmentation methods are in the middle between dataset
    creation and deep learning model training, because they are a way to
    enhance the dataset but depend on the objective for which the model
    is trained. Their importance is inversely proportional with the size
    of the dataset, which made them very important for my small
    dataset.</p>
        <p>As it was already explained in
    <xref alt="Section 1.2.4" rid="sec-sota-dataset-augment">Section 1.2.4</xref>,
    I used Albumentations
    (<xref alt="Buslaev et al. 2020" rid="ref-albumentations" ref-type="bibr">Buslaev
    et al. 2020</xref>) to apply two types of augmentations: pixel-level
    and spatial-level.</p>
        <p>Spatial-level augmentations had to be in the exact same way to
    the whole dataset, to maintain the spatial coherence between RGB
    images, CIR images and the CHM layers. I used three different
    spatial transformations, applied with random parameters. The first
    one chooses one of the eight possible images we can get when
    flipping and rotating the image by angles that are multiples of 90°.
    The second one adds a perspective effect to the images. The third
    one adds a small distortion to the image.</p>
        <p>On the contrary, Pixel-level augmentations must be applied
    differently to RGB images and CHM layers because they represent
    different kinds of data, so the values of the pixels do not have the
    same meaning. In practice, a lot of transformations were conceived
    to reproduce camera effects on RGB images or to shift the color
    spectrum. Among others, I used random modifications of the
    brightness, the gamma value and added noise and a blurring effect
    randomly to RGB images. For both types of data, a channel dropout is
    also randomly applied, leaving a random number of channels and
    removing the others. A better way to augment the CHM data would have
    been to apply random displacements and deletions of points in the
    point cloud, before computing the CHM layers. However, these
    operations are too costly to be integrated in the training pipeline
    without consequently increasing the training time, so this idea was
    discarded.</p>
      </sec>
    </sec>
    <sec id="model-and-training">
      <title>Model and training</title>
      <p>The deep learning model that is used is based on AMF GD YOLOv8, the
  model proposed in this paper
  (<xref alt="Zhong et al. 2024" rid="ref-amf_gd_yolov8" ref-type="bibr">Zhong
  et al. 2024</xref>).</p>
      <sec id="model-architecture">
        <title>Model architecture</title>
        <p>The architecture of the model is conceptually simple. The model
    takes two inputs in the form of two rasters with the same height and
    width. The two inputs are processed using the backbone of the YOLOv8
    model
    (<xref alt="Redmon et al. 2016" rid="ref-yolo" ref-type="bibr">Redmon
    et al. 2016</xref>) to extract features at different scales. Then
    Attention Multi-level Fusion (AMF) layers are used to fuse the
    features of the two inputs at each scale level. Then, a
    Gather-and-Distribute (GD) mechanism is used to propagate
    information between the different scales. This mechanism fuses the
    features from all scales before redistributing them to the features,
    two times in a row. Finally, the features of the three smallest
    scales are fed into detection layers responsible for extracting
    bounding boxes and assigning confidence scores and class
    probabilities to them.</p>
        <p>In practical terms, the input rasters have a shape of
    <inline-formula><alternatives><tex-math><![CDATA[640 \times 640 \times c_{\text{RGB}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">RGB</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives><tex-math><![CDATA[640 \times 640 \times c_{\text{CHM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">CHM</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
    where <inline-formula><alternatives><tex-math><![CDATA[c_{\text{RGB}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">RGB</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
    is equal to 6 when using RGB and CIR images, and 3 when using only
    one of them, and <inline-formula><alternatives><tex-math><![CDATA[c_{\text{CHM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">CHM</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
    is the number of CHM layers that we decide to use for the model.
    Since the resolution that is used is 0.08 m, this means that each
    image spans over 51.2 m.</p>
        <p>The only real modification that I made to the architecture
    compared to the initial paper is adding any number of channels in
    the CHM input, while we had <inline-formula><alternatives><tex-math><![CDATA[c_{\text{CHM}} = 1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">CHM</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    originally. Using CIR images in addition to RGB images is also new,
    but this is a less important modification.</p>
      </sec>
      <sec id="training-pipeline">
        <title>Training pipeline</title>
        <p>The training pipeline consists of three steps. First, the data is
    pre-processed to create the inputs to feed into the model. Then, the
    training loop runs until the end condition is reached. Finally, the
    final model is evaluated on all the datasets.</p>
        <sec id="data-preprocessing">
          <title>Data preprocessing</title>
          <p>Data pre-processing is quite straightforward. The first step is
      to divide the dataset into a grid of
      <inline-formula><alternatives><tex-math><![CDATA[640 \times 640]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      tiles. Then, all these tiles are placed into one of the training,
      validation and test sets.</p>
          <p>As for RGB and CIR images, preprocessing only contains two
      simple steps: tiling the large images into small
      <inline-formula><alternatives><tex-math><![CDATA[640 \times 640]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      images, and normalizing all images along each channel. When both
      data sources are used, RGB and CIR images are also merged into
      images with 6 channels, which will be the input of the model.</p>
          <p>As for CHM layers, there are more steps. The first step is to
      compute a sort of flattened point cloud, by computing the DTM,
      which represents the height of the ground, and removing this
      height to the point cloud. Then, for each CHM layer, if the height
      interval is <inline-formula><alternatives><tex-math><![CDATA[[z_\text{bot}, z_\text{top}]]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">bot</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">top</mml:mtext></mml:msub><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
      we first extract all the points which have a height
      <inline-formula><alternatives><tex-math><![CDATA[h]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>
      such that <inline-formula><alternatives><tex-math><![CDATA[z_\text{bot} \leq h \leq z_\text{top}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">bot</mml:mtext></mml:msub><mml:mo>≤</mml:mo><mml:mi>h</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">top</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
      and we then compute the DSM for this smaller point cloud. Since
      the ground height was already removed from the point cloud, this
      DSM is the CHM. Then, all the layers are merged into one raster
      with multiple channels and we normalize the whole raster with the
      average and the standard deviation over all channels. Finally, we
      can simply tile these rasters exactly like RGB and CIR images,
      which gives us the inputs of the model.</p>
          <p>All these operations are conceptually simple, but they can be
      computationally expensive. Therefore, I had to put a certain
      effort into accelerating with different methods. First, I made
      sure to save the most important and generic elements to avoid
      useless computations every time the model is trained again,
      without saturating the memory. Then, I also implemented
      multi-threading for every possible step to improve the raw speed
      of preprocessing. Finally, performance is also the reason why
      normalization if performed during preprocessing instead of during
      the initialization of the data in the training loop.</p>
        </sec>
        <sec id="training-loop">
          <title>Training loop</title>
          <p>The training loop is very generic, so I will only mention the
      most interesting parts. First, we use an Adam optimizer and a
      simple learning rate scheduler with a multiplier at each epoch i
      which is <inline-formula><alternatives><tex-math><![CDATA[1/\sqrt{i+2}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>.</p>
          <p>Then, since the batch size cannot be very large because of the
      space required by all the images, there is the possibility to
      perform gradient accumulation, which means that backward
      propagation won’t be performed with each batch, but instead every
      two or more batches. The idea behind this is to add more stability
      to the training, since back-propagating on only a few images is
      prone to overfitting on a set of examples which are not
      representative of the whole dataset.</p>
          <p>As for the criterion to stop the training session, we use the
      loss on the validation set. Once this loss didn’t improve during
      50 iterations over the whole dataset, we stop and keep the model
      that had the best validation loss.</p>
          <p>Besides these details, the training loop is very generic. We
      loop over the entire training set with batches to compute the loss
      and perform gradient back-propagation,. Then we compute the loss
      on the validation set and store this loss as the metric that
      decides when to stop.</p>
        </sec>
        <sec id="output-postprocessing">
          <title>Output postprocessing</title>
          <p>Regarding postprocessing of the output of the model, there a
      few things to mention. First, the model outputs a lot of bounding
      boxes, which have to be cleaned using two criteria. The first
      criterion is the confidence score. We can just set a threshold
      below which bounding boxes are discarded. The second criterion is
      the intersection over union (IoU) with other bounding boxes. IoU
      is a metrics used to quantify how similar two bounding boxes are.
      It is a value between 0 and 1, which formula is:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
      \text{IoU}(A, B) = \frac{\text{area}(A \cap B)}{\text{area}(A \cup B)}
      ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mrow>
                    <mml:mtext mathvariant="normal">IoU</mml:mtext>
                    <mml:mrow>
                      <mml:mo stretchy="true" form="prefix">(</mml:mo>
                      <mml:mi>A</mml:mi>
                      <mml:mo>,</mml:mo>
                      <mml:mi>B</mml:mi>
                      <mml:mo stretchy="true" form="postfix">)</mml:mo>
                    </mml:mrow>
                    <mml:mo>=</mml:mo>
                    <mml:mfrac>
                      <mml:mrow>
                        <mml:mtext mathvariant="normal">area</mml:mtext>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>A</mml:mi>
                          <mml:mo>∩</mml:mo>
                          <mml:mi>B</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                      <mml:mrow>
                        <mml:mtext mathvariant="normal">area</mml:mtext>
                        <mml:mrow>
                          <mml:mo stretchy="true" form="prefix">(</mml:mo>
                          <mml:mi>A</mml:mi>
                          <mml:mo>∪</mml:mo>
                          <mml:mi>B</mml:mi>
                          <mml:mo stretchy="true" form="postfix">)</mml:mo>
                        </mml:mrow>
                      </mml:mrow>
                    </mml:mfrac>
                  </mml:mrow>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>Using this metrics, we can detect bounding boxes which are too
      similar to each other, and simply keep the bounding box with the
      highest confidence score when two bounding boxes have an IoU
      larger than a certain threshold.</p>
          <p>For the evaluation, the process is a little different, because
      we only perform the clean up relying on IoU, and keep all other
      bounding boxes. The main metric that we compute is called sortedAP
      (<xref alt="Chen et al. 2023" rid="ref-sortedAP" ref-type="bibr">Chen
      et al. 2023</xref>), which is an evolution of the mean (point)
      average precision (mAP). mAP is defined as follows:</p>
          <p>
            <disp-formula>
              <alternatives>
                <tex-math><![CDATA[
      \begin{array}{rcl}
      \text{mAP} & = & \frac{1}{N} \sum\limits_{t\in T} \text{AP}_t \\
      \text{AP}_t & = & \frac{{TP}_t}{{TP}_t + {FP}_t + {FN}_t}
      \end{array}
      ]]></tex-math>
                <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                  <mml:mtable>
                    <mml:mtr>
                      <mml:mtd columnalign="right" style="text-align: right">
                        <mml:mtext mathvariant="normal">mAP</mml:mtext>
                      </mml:mtd>
                      <mml:mtd columnalign="center" style="text-align: center">
                        <mml:mo>=</mml:mo>
                      </mml:mtd>
                      <mml:mtd columnalign="left" style="text-align: left">
                        <mml:mfrac>
                          <mml:mn>1</mml:mn>
                          <mml:mi>N</mml:mi>
                        </mml:mfrac>
                        <mml:munder>
                          <mml:mo>∑</mml:mo>
                          <mml:mrow>
                            <mml:mi>t</mml:mi>
                            <mml:mo>∈</mml:mo>
                            <mml:mi>T</mml:mi>
                          </mml:mrow>
                        </mml:munder>
                        <mml:msub>
                          <mml:mtext mathvariant="normal">AP</mml:mtext>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                    </mml:mtr>
                    <mml:mtr>
                      <mml:mtd columnalign="right" style="text-align: right">
                        <mml:msub>
                          <mml:mtext mathvariant="normal">AP</mml:mtext>
                          <mml:mi>t</mml:mi>
                        </mml:msub>
                      </mml:mtd>
                      <mml:mtd columnalign="center" style="text-align: center">
                        <mml:mo>=</mml:mo>
                      </mml:mtd>
                      <mml:mtd columnalign="left" style="text-align: left">
                        <mml:mfrac>
                          <mml:msub>
                            <mml:mrow>
                              <mml:mi>T</mml:mi>
                              <mml:mi>P</mml:mi>
                            </mml:mrow>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                          <mml:mrow>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>T</mml:mi>
                                <mml:mi>P</mml:mi>
                              </mml:mrow>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>F</mml:mi>
                                <mml:mi>P</mml:mi>
                              </mml:mrow>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mo>+</mml:mo>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>F</mml:mi>
                                <mml:mi>N</mml:mi>
                              </mml:mrow>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                          </mml:mrow>
                        </mml:mfrac>
                      </mml:mtd>
                    </mml:mtr>
                  </mml:mtable>
                </mml:math>
              </alternatives>
            </disp-formula>
          </p>
          <p>where <inline-formula><alternatives><tex-math><![CDATA[T=\{t_1, t_2, \dots, t_N\}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      is a list of IoU threshold values, <inline-formula><alternatives><tex-math><![CDATA[{TP}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      are the true positives when the the IoU threshold is
      <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives><tex-math><![CDATA[{FP}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      are false positives and <inline-formula><alternatives><tex-math><![CDATA[{FN}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      are false negatives. The reason why <inline-formula><alternatives><tex-math><![CDATA[TP]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives><tex-math><![CDATA[FP]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[FN]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      depend on <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
      is that a bounding box is considered to be true if its IoU with
      one of the ground-truth bounding boxes is larger than
      <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>.</p>
          <p>sortedAP is an improvement over this method because there is no
      need to select a list of IoU threshold values. Predicted bounding
      boxes are sorted according to their confidence score which allows
      to compute <inline-formula><alternatives><tex-math><![CDATA[\text{AP}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mtext mathvariant="normal">AP</mml:mtext></mml:math></alternatives></inline-formula>
      incrementally for any value of <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>.
      Then, the area of the curve of the AP with respect to the IoU
      threshold is used as a metric, between 0 and 1, 1 being the best
      possible value.</p>
        </sec>
      </sec>
    </sec>
    <sec id="results">
      <title>Results</title>
      <p>In this section are the results of the experiments performed with
  the model and the dataset presented before.</p>
      <sec id="training-parameters">
        <title>Training parameters</title>
        <p>The first experiment was a simple test over the different
    parameters regarding the training loop. There were two goals to this
    experiment. The first one was to find the best training parameters
    for the next experiments. The second one was to see if randomly
    dropping one of the inputs of the model (either RGB/CIR or CHM)
    could help the model by pushing it to learn to make the best out of
    the two types of data.</p>
        <p>The different parameters that are tested here are:</p>
        <list list-type="bullet">
          <list-item>
            <p>“Learn. rate”: the initial learning rate.</p>
          </list-item>
          <list-item>
            <p>“Prob. drop”: the probability to drop either RGB/CIR or CHM.
        The probability is the same for the two types, which means that
        if the displayed value is 0.1, then all data will be used 80% of
        the time, while only RGB/CIR and only CHM both happen 10% of the
        time.</p>
          </list-item>
          <list-item>
            <p>“Accum. count”: the accumulation count, which means the
        amount of training data to process and compute the loss on
        before performing gradient back-propagation.</p>
          </list-item>
        </list>
        <p>As you can see on
    <xref alt="Figure 1" rid="fig-training-parameters-experiments">Figure 1</xref>,
    sortedAP reaches at best values just above 0.3. The reason why the
    column name is “Best sortedAP” is due to the dataset being too
    small. Since the dataset is small, the training process overfits
    quickly, and the model doesn’t have enough training steps to have
    confidence scores which reach very high values. As a consequence, it
    is difficult to know beforehand which confidence threshold to
    choose. Therefore, the sortedAP metric is computed over several
    different confidence thresholds, and the one that gives the best
    value of sortedAP is kept.</p>
        <p>With this experiment, we can see that a learning rate of 0.01
    seems to make the training too much unstable, while 0.001 doesn’t
    give very high score. Then, we can also see how unstable the
    training process is in general, which comes mostly from the dataset
    being too small. However, a learning rate between 0.0025 and 0.006
    seems to give the most stable results, when the drop probability is
    0. This seems to show that the idea of randomly dropping one of the
    two inputs doesn’t really help the model to learn.</p>
        <fig id="fig-training-parameters-experiments">
          <caption>
            <p>Figure 1: Results with different training parameters
      for all experiments</p>
          </caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-training-parameters-experiments-output-1.png"/>
        </fig>
        <p>In the next graph
    (<xref alt="Figure 2" rid="fig-training-parameters-data">Figure 2</xref>),
    we can see more results for the same experiments. Here, the results
    are colored according to the data that we use to evaluate the model.
    In blue, we see the value of sortedAP when we evaluate the model
    with the CHM layers data and dummy zero arrays as RGB/CIR data.
    These dummy arrays are also those that are used as input when one of
    the channel is dropped during training, when we have a drop
    probability larger than 0. Some interesting patterns appear in some
    of the cells in this plot. Firstly, it looks like randomly dropping
    one of the two inputs with the same probability has a much larger
    influence over the results using RGB/CIR than CHM. While CHM gives
    better results than RGB/CIR when always training using everything,
    RGB/CIR seems to perform better alone when also trained alone, even
    outperforming the combination of both inputs in certain cases.</p>
        <fig id="fig-training-parameters-data">
          <caption>
            <p>Figure 2: Results with different training parameters
      for all evaluation data setups</p>
          </caption>
          <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-training-parameters-data-output-2.png"/>
        </fig>
        <p>From the results of this experiment, I decided to pick the
    following parameters for the next experiments:</p>
        <list list-type="bullet">
          <list-item>
            <p>Initial learning rate: 0.004</p>
          </list-item>
          <list-item>
            <p>Drop probability: 0</p>
          </list-item>
          <list-item>
            <p>Accumulation count: 10</p>
          </list-item>
        </list>
      </sec>
      <sec id="data-used">
        <title>Data used</title>
      </sec>
      <sec id="chm-layers">
        <title>CHM layers</title>
      </sec>
      <sec id="hard-trees">
        <title>Hard trees</title>
      </sec>
    </sec>
    <sec id="discussion">
      <title>Discussion</title>
      <sec id="dataset">
        <title>Dataset</title>
        <list list-type="bullet">
          <list-item>
            <p>DeepForest: A Python package for RGB deep learning tree crown
        delineation
        (<xref alt="B. G. Weinstein et al. 2020" rid="ref-DeepForest" ref-type="bibr">B.
        G. Weinstein et al. 2020</xref>): uses only RGB data to detect
        trees, but uses LiDAR to create millions of annotations of
        moderate quality to pre-train the model, before using around
        10,000 hand-annotations to finalize and specialize the training
        on a certain area.</p>
          </list-item>
        </list>
      </sec>
      <sec id="combination-of-data-types">
        <title>Combination of data types</title>
      </sec>
    </sec>
    <sec id="conclusion">
      <title>Conclusion</title>
      <p>Blablabla</p>
    </sec>
  </body>
  <back>
    <ref-list>
      <title/>
      <ref id="ref-FoMo-Bench">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Bountos</surname>
              <given-names>Nikolaos Ioannis</given-names>
            </name>
            <name>
              <surname>Ouaknine</surname>
              <given-names>Arthur</given-names>
            </name>
            <name>
              <surname>Rolnick</surname>
              <given-names>David</given-names>
            </name>
          </person-group>
          <article-title>FoMo-bench: A multi-modal, multi-scale and multi-task forest monitoring benchmark for remote sensing foundation models</article-title>
          <source>arXiv preprint arXiv:2312.10114</source>
          <year iso-8601-date="2023">2023</year>
          <uri>https://arxiv.org/abs/2312.10114</uri>
        </element-citation>
      </ref>
      <ref id="ref-OpenForest">
        <element-citation>
          <person-group person-group-type="author">
            <name>
              <surname>Ouaknine</surname>
              <given-names>Arthur</given-names>
            </name>
            <name>
              <surname>Kattenborn</surname>
              <given-names>Teja</given-names>
            </name>
            <name>
              <surname>Laliberté</surname>
              <given-names>Etienne</given-names>
            </name>
            <name>
              <surname>Rolnick</surname>
              <given-names>David</given-names>
            </name>
          </person-group>
          <article-title>OpenForest: A data catalogue for machine learning in forest monitoring</article-title>
          <year iso-8601-date="2023">2023</year>
          <uri>https://arxiv.org/abs/2311.00277</uri>
        </element-citation>
      </ref>
      <ref id="ref-DeepForestBefore">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Weinstein</surname>
              <given-names>Ben G.</given-names>
            </name>
            <name>
              <surname>Marconi</surname>
              <given-names>Sergio</given-names>
            </name>
            <name>
              <surname>Bohlman</surname>
              <given-names>Stephanie</given-names>
            </name>
            <name>
              <surname>Zare</surname>
              <given-names>Alina</given-names>
            </name>
            <name>
              <surname>White</surname>
              <given-names>Ethan</given-names>
            </name>
          </person-group>
          <article-title>Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks</article-title>
          <source>Remote Sensing</source>
          <year iso-8601-date="2019">2019</year>
          <volume>11</volume>
          <issue>11</issue>
          <issn>2072-4292</issn>
          <uri>https://www.mdpi.com/2072-4292/11/11/1309</uri>
          <pub-id pub-id-type="doi">10.3390/rs11111309</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-ReforesTree">
        <element-citation>
          <person-group person-group-type="author">
            <name>
              <surname>Reiersen</surname>
              <given-names>Gyri</given-names>
            </name>
            <name>
              <surname>Dao</surname>
              <given-names>David</given-names>
            </name>
            <name>
              <surname>Lütjens</surname>
              <given-names>Björn</given-names>
            </name>
            <name>
              <surname>Klemmer</surname>
              <given-names>Konstantin</given-names>
            </name>
            <name>
              <surname>Amara</surname>
              <given-names>Kenza</given-names>
            </name>
            <name>
              <surname>Steinegger</surname>
              <given-names>Attila</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Ce</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Xiaoxiang</given-names>
            </name>
          </person-group>
          <article-title>ReforesTree: A dataset for estimating tropical forest carbon stock with deep learning and aerial imagery</article-title>
          <year iso-8601-date="2022">2022</year>
          <uri>https://arxiv.org/abs/2201.11192</uri>
        </element-citation>
      </ref>
      <ref id="ref-FOR-instance">
        <element-citation>
          <person-group person-group-type="author">
            <name>
              <surname>Puliti</surname>
              <given-names>Stefano</given-names>
            </name>
            <name>
              <surname>Pearse</surname>
              <given-names>Grant</given-names>
            </name>
            <name>
              <surname>Surový</surname>
              <given-names>Peter</given-names>
            </name>
            <name>
              <surname>Wallace</surname>
              <given-names>Luke</given-names>
            </name>
            <name>
              <surname>Hollaus</surname>
              <given-names>Markus</given-names>
            </name>
            <name>
              <surname>Wielgosz</surname>
              <given-names>Maciej</given-names>
            </name>
            <name>
              <surname>Astrup</surname>
              <given-names>Rasmus</given-names>
            </name>
          </person-group>
          <article-title>FOR-instance: A UAV laser scanning benchmark dataset for semantic and instance segmentation of individual trees</article-title>
          <year iso-8601-date="2023">2023</year>
          <uri>https://arxiv.org/abs/2309.01279</uri>
        </element-citation>
      </ref>
      <ref id="ref-MDAS">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Hu</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>R.</given-names>
            </name>
            <name>
              <surname>Hong</surname>
              <given-names>D.</given-names>
            </name>
            <name>
              <surname>Camero</surname>
              <given-names>A.</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>J.</given-names>
            </name>
            <name>
              <surname>Schneider</surname>
              <given-names>M.</given-names>
            </name>
            <name>
              <surname>Kurz</surname>
              <given-names>F.</given-names>
            </name>
            <name>
              <surname>Segl</surname>
              <given-names>K.</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>X. X.</given-names>
            </name>
          </person-group>
          <article-title>MDAS: A new multimodal benchmark dataset for remote sensing</article-title>
          <source>Earth System Science Data</source>
          <year iso-8601-date="2023">2023</year>
          <volume>15</volume>
          <issue>1</issue>
          <uri>https://essd.copernicus.org/articles/15/113/2023/</uri>
          <pub-id pub-id-type="doi">10.5194/essd-15-113-2023</pub-id>
          <fpage>113</fpage>
          <lpage>131</lpage>
        </element-citation>
      </ref>
      <ref id="ref-TALLO">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Jucker</surname>
              <given-names>Tommaso</given-names>
            </name>
            <name>
              <surname>Fischer</surname>
              <given-names>Fabian Jörg</given-names>
            </name>
            <name>
              <surname>Chave</surname>
              <given-names>Jérôme</given-names>
            </name>
            <name>
              <surname>Coomes</surname>
              <given-names>David A.</given-names>
            </name>
            <name>
              <surname>Caspersen</surname>
              <given-names>John</given-names>
            </name>
            <name>
              <surname>Ali</surname>
              <given-names>Arshad</given-names>
            </name>
            <name>
              <surname>Loubota Panzou</surname>
              <given-names>Grace Jopaul</given-names>
            </name>
            <name>
              <surname>Feldpausch</surname>
              <given-names>Ted R.</given-names>
            </name>
            <name>
              <surname>Falster</surname>
              <given-names>Daniel</given-names>
            </name>
            <name>
              <surname>Usoltsev</surname>
              <given-names>Vladimir A.</given-names>
            </name>
            <name>
              <surname>Adu-Bredu</surname>
              <given-names>Stephen</given-names>
            </name>
            <name>
              <surname>Alves</surname>
              <given-names>Luciana F.</given-names>
            </name>
            <name>
              <surname>Aminpour</surname>
              <given-names>Mohammad</given-names>
            </name>
            <name>
              <surname>Angoboy</surname>
              <given-names>Ilondea B.</given-names>
            </name>
            <name>
              <surname>Anten</surname>
              <given-names>Niels P. R.</given-names>
            </name>
            <name>
              <surname>Antin</surname>
              <given-names>Cécile</given-names>
            </name>
            <name>
              <surname>Askari</surname>
              <given-names>Yousef</given-names>
            </name>
            <name>
              <surname>Muñoz</surname>
              <given-names>Rodrigo</given-names>
            </name>
            <name>
              <surname>Ayyappan</surname>
              <given-names>Narayanan</given-names>
            </name>
            <name>
              <surname>Balvanera</surname>
              <given-names>Patricia</given-names>
            </name>
            <name>
              <surname>Banin</surname>
              <given-names>Lindsay</given-names>
            </name>
            <name>
              <surname>Barbier</surname>
              <given-names>Nicolas</given-names>
            </name>
            <name>
              <surname>Battles</surname>
              <given-names>John J.</given-names>
            </name>
            <name>
              <surname>Beeckman</surname>
              <given-names>Hans</given-names>
            </name>
            <name>
              <surname>Bocko</surname>
              <given-names>Yannick E.</given-names>
            </name>
            <name>
              <surname>Bond-Lamberty</surname>
              <given-names>Ben</given-names>
            </name>
            <name>
              <surname>Bongers</surname>
              <given-names>Frans</given-names>
            </name>
            <name>
              <surname>Bowers</surname>
              <given-names>Samuel</given-names>
            </name>
            <name>
              <surname>Brade</surname>
              <given-names>Thomas</given-names>
            </name>
            <name>
              <surname>Breugel</surname>
              <given-names>Michiel van</given-names>
            </name>
            <name>
              <surname>Chantrain</surname>
              <given-names>Arthur</given-names>
            </name>
            <name>
              <surname>Chaudhary</surname>
              <given-names>Rajeev</given-names>
            </name>
            <name>
              <surname>Dai</surname>
              <given-names>Jingyu</given-names>
            </name>
            <name>
              <surname>Dalponte</surname>
              <given-names>Michele</given-names>
            </name>
            <name>
              <surname>Dimobe</surname>
              <given-names>Kangbéni</given-names>
            </name>
            <name>
              <surname>Domec</surname>
              <given-names>Jean-Christophe</given-names>
            </name>
            <name>
              <surname>Doucet</surname>
              <given-names>Jean-Louis</given-names>
            </name>
            <name>
              <surname>Duursma</surname>
              <given-names>Remko A.</given-names>
            </name>
            <name>
              <surname>Enríquez</surname>
              <given-names>Moisés</given-names>
            </name>
            <name>
              <surname>Ewijk</surname>
              <given-names>Karin Y. van</given-names>
            </name>
            <name>
              <surname>Farfán-Rios</surname>
              <given-names>William</given-names>
            </name>
            <name>
              <surname>Fayolle</surname>
              <given-names>Adeline</given-names>
            </name>
            <name>
              <surname>Forni</surname>
              <given-names>Eric</given-names>
            </name>
            <name>
              <surname>Forrester</surname>
              <given-names>David I.</given-names>
            </name>
            <name>
              <surname>Gilani</surname>
              <given-names>Hammad</given-names>
            </name>
            <name>
              <surname>Godlee</surname>
              <given-names>John L.</given-names>
            </name>
            <name>
              <surname>Gourlet-Fleury</surname>
              <given-names>Sylvie</given-names>
            </name>
            <name>
              <surname>Haeni</surname>
              <given-names>Matthias</given-names>
            </name>
            <name>
              <surname>Hall</surname>
              <given-names>Jefferson S.</given-names>
            </name>
            <name>
              <surname>He</surname>
              <given-names>Jie-Kun</given-names>
            </name>
            <name>
              <surname>Hemp</surname>
              <given-names>Andreas</given-names>
            </name>
            <name>
              <surname>Hernández-Stefanoni</surname>
              <given-names>José L.</given-names>
            </name>
            <name>
              <surname>Higgins</surname>
              <given-names>Steven I.</given-names>
            </name>
            <name>
              <surname>Holdaway</surname>
              <given-names>Robert J.</given-names>
            </name>
            <name>
              <surname>Hussain</surname>
              <given-names>Kiramat</given-names>
            </name>
            <name>
              <surname>Hutley</surname>
              <given-names>Lindsay B.</given-names>
            </name>
            <name>
              <surname>Ichie</surname>
              <given-names>Tomoaki</given-names>
            </name>
            <name>
              <surname>Iida</surname>
              <given-names>Yoshiko</given-names>
            </name>
            <name>
              <surname>Jiang</surname>
              <given-names>Hai-sheng</given-names>
            </name>
            <name>
              <surname>Joshi</surname>
              <given-names>Puspa Raj</given-names>
            </name>
            <name>
              <surname>Kaboli</surname>
              <given-names>Hasan</given-names>
            </name>
            <name>
              <surname>Larsary</surname>
              <given-names>Maryam Kazempour</given-names>
            </name>
            <name>
              <surname>Kenzo</surname>
              <given-names>Tanaka</given-names>
            </name>
            <name>
              <surname>Kloeppel</surname>
              <given-names>Brian D.</given-names>
            </name>
            <name>
              <surname>Kohyama</surname>
              <given-names>Takashi</given-names>
            </name>
            <name>
              <surname>Kunwar</surname>
              <given-names>Suwash</given-names>
            </name>
            <name>
              <surname>Kuyah</surname>
              <given-names>Shem</given-names>
            </name>
            <name>
              <surname>Kvasnica</surname>
              <given-names>Jakub</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Siliang</given-names>
            </name>
            <name>
              <surname>Lines</surname>
              <given-names>Emily R.</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Hongyan</given-names>
            </name>
            <name>
              <surname>Lorimer</surname>
              <given-names>Craig</given-names>
            </name>
            <name>
              <surname>Loumeto</surname>
              <given-names>Jean-Joël</given-names>
            </name>
            <name>
              <surname>Malhi</surname>
              <given-names>Yadvinder</given-names>
            </name>
            <name>
              <surname>Marshall</surname>
              <given-names>Peter L.</given-names>
            </name>
            <name>
              <surname>Mattsson</surname>
              <given-names>Eskil</given-names>
            </name>
            <name>
              <surname>Matula</surname>
              <given-names>Radim</given-names>
            </name>
            <name>
              <surname>Meave</surname>
              <given-names>Jorge A.</given-names>
            </name>
            <name>
              <surname>Mensah</surname>
              <given-names>Sylvanus</given-names>
            </name>
            <name>
              <surname>Mi</surname>
              <given-names>Xiangcheng</given-names>
            </name>
            <name>
              <surname>Momo</surname>
              <given-names>Stéphane</given-names>
            </name>
            <name>
              <surname>Moncrieff</surname>
              <given-names>Glenn R.</given-names>
            </name>
            <name>
              <surname>Mora</surname>
              <given-names>Francisco</given-names>
            </name>
            <name>
              <surname>Nissanka</surname>
              <given-names>Sarath P.</given-names>
            </name>
            <name>
              <surname>O’Hara</surname>
              <given-names>Kevin L.</given-names>
            </name>
            <name>
              <surname>Pearce</surname>
              <given-names>Steven</given-names>
            </name>
            <name>
              <surname>Pelissier</surname>
              <given-names>Raphaël</given-names>
            </name>
            <name>
              <surname>Peri</surname>
              <given-names>Pablo L.</given-names>
            </name>
            <name>
              <surname>Ploton</surname>
              <given-names>Pierre</given-names>
            </name>
            <name>
              <surname>Poorter</surname>
              <given-names>Lourens</given-names>
            </name>
            <name>
              <surname>Pour</surname>
              <given-names>Mohsen Javanmiri</given-names>
            </name>
            <name>
              <surname>Pourbabaei</surname>
              <given-names>Hassan</given-names>
            </name>
            <name>
              <surname>Dupuy-Rada</surname>
              <given-names>Juan Manuel</given-names>
            </name>
            <name>
              <surname>Ribeiro</surname>
              <given-names>Sabina C.</given-names>
            </name>
            <name>
              <surname>Ryan</surname>
              <given-names>Casey</given-names>
            </name>
            <name>
              <surname>Sanaei</surname>
              <given-names>Anvar</given-names>
            </name>
            <name>
              <surname>Sanger</surname>
              <given-names>Jennifer</given-names>
            </name>
            <name>
              <surname>Schlund</surname>
              <given-names>Michael</given-names>
            </name>
            <name>
              <surname>Sellan</surname>
              <given-names>Giacomo</given-names>
            </name>
            <name>
              <surname>Shenkin</surname>
              <given-names>Alexander</given-names>
            </name>
            <name>
              <surname>Sonké</surname>
              <given-names>Bonaventure</given-names>
            </name>
            <name>
              <surname>Sterck</surname>
              <given-names>Frank J.</given-names>
            </name>
            <name>
              <surname>Svátek</surname>
              <given-names>Martin</given-names>
            </name>
            <name>
              <surname>Takagi</surname>
              <given-names>Kentaro</given-names>
            </name>
            <name>
              <surname>Trugman</surname>
              <given-names>Anna T.</given-names>
            </name>
            <name>
              <surname>Ullah</surname>
              <given-names>Farman</given-names>
            </name>
            <name>
              <surname>Vadeboncoeur</surname>
              <given-names>Matthew A.</given-names>
            </name>
            <name>
              <surname>Valipour</surname>
              <given-names>Ahmad</given-names>
            </name>
            <name>
              <surname>Vanderwel</surname>
              <given-names>Mark C.</given-names>
            </name>
            <name>
              <surname>Vovides</surname>
              <given-names>Alejandra G.</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Weiwei</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Li-Qiu</given-names>
            </name>
            <name>
              <surname>Wirth</surname>
              <given-names>Christian</given-names>
            </name>
            <name>
              <surname>Woods</surname>
              <given-names>Murray</given-names>
            </name>
            <name>
              <surname>Xiang</surname>
              <given-names>Wenhua</given-names>
            </name>
            <name>
              <surname>Ximenes</surname>
              <given-names>Fabiano de Aquino</given-names>
            </name>
            <name>
              <surname>Xu</surname>
              <given-names>Yaozhan</given-names>
            </name>
            <name>
              <surname>Yamada</surname>
              <given-names>Toshihiro</given-names>
            </name>
            <name>
              <surname>Zavala</surname>
              <given-names>Miguel A.</given-names>
            </name>
          </person-group>
          <article-title>Tallo: A global tree allometry and crown architecture database</article-title>
          <source>Global Change Biology</source>
          <year iso-8601-date="2022">2022</year>
          <volume>28</volume>
          <issue>17</issue>
          <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.16302</uri>
          <pub-id pub-id-type="doi">10.1111/gcb.16302</pub-id>
          <fpage>5254</fpage>
          <lpage>5268</lpage>
        </element-citation>
      </ref>
      <ref id="ref-MillionTrees">
        <element-citation publication-type="webpage">
          <person-group person-group-type="author">
            <name>
              <surname>Weinstein</surname>
              <given-names>Ben</given-names>
            </name>
          </person-group>
          <article-title>MillionTrees</article-title>
          <year iso-8601-date="2023">2023</year>
          <date-in-citation content-type="access-date">
            <year iso-8601-date="2024-07-08">2024</year>
            <month>07</month>
            <day>08</day>
          </date-in-citation>
          <uri>https://milliontrees.idtrees.org/</uri>
        </element-citation>
      </ref>
      <ref id="ref-WildForest3D">
        <element-citation>
          <person-group person-group-type="author">
            <name>
              <surname>Kalinicheva</surname>
              <given-names>Ekaterina</given-names>
            </name>
            <name>
              <surname>Landrieu</surname>
              <given-names>Loic</given-names>
            </name>
            <name>
              <surname>Mallet</surname>
              <given-names>Clément</given-names>
            </name>
            <name>
              <surname>Chehata</surname>
              <given-names>Nesrine</given-names>
            </name>
          </person-group>
          <article-title>Multi-layer modeling of dense vegetation from aerial LiDAR scans</article-title>
          <year iso-8601-date="2022">2022</year>
          <uri>https://arxiv.org/abs/2204.11620</uri>
        </element-citation>
      </ref>
      <ref id="ref-sortedAP">
        <element-citation publication-type="paper-conference">
          <person-group person-group-type="author">
            <name>
              <surname>Chen</surname>
              <given-names>Long</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Yuli</given-names>
            </name>
            <name>
              <surname>Stegmaier</surname>
              <given-names>Johannes</given-names>
            </name>
            <name>
              <surname>Merhof</surname>
              <given-names>Dorit</given-names>
            </name>
          </person-group>
          <article-title>SortedAP: Rethinking evaluation metrics for instance segmentation</article-title>
          <source>Proceedings of the IEEE/CVF international conference on computer vision (ICCV) workshops</source>
          <year iso-8601-date="2023-10">2023</year>
          <month>10</month>
          <uri>https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html</uri>
          <fpage>3923</fpage>
          <lpage>3929</lpage>
        </element-citation>
      </ref>
      <ref id="ref-AHN4">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Actueel Hoogtebestand Nederland</string-name>
          </person-group>
          <article-title>AHN4 - Actual Height Model of the Netherlands</article-title>
          <year iso-8601-date="2020">2020</year>
          <uri>https://www.ahn.nl/</uri>
        </element-citation>
      </ref>
      <ref id="ref-Luchtfotos">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Beeldmateriaal Nederland</string-name>
          </person-group>
          <article-title>Luchtfoto’s (Aerial Photographs)</article-title>
          <year iso-8601-date="2024">2024</year>
          <uri>https://www.beeldmateriaal.nl/luchtfotos</uri>
        </element-citation>
      </ref>
      <ref id="ref-IGN_LiDAR_HD">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Institut national de l’information géographique et forestière (IGN)</string-name>
          </person-group>
          <article-title>LiDAR HD</article-title>
          <year iso-8601-date="2020">2020</year>
          <uri>https://geoservices.ign.fr/lidarhd</uri>
        </element-citation>
      </ref>
      <ref id="ref-IGN_BD_ORTHO">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Institut national de l’information géographique et forestière (IGN)</string-name>
          </person-group>
          <article-title>BD ORTHO</article-title>
          <year iso-8601-date="2021">2021</year>
          <uri>https://geoservices.ign.fr/bdortho</uri>
        </element-citation>
      </ref>
      <ref id="ref-amsterdam_trees">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Gemeente Amsterdam</string-name>
          </person-group>
          <article-title>Bomenbestand Amsterdam (Amsterdam Tree Dataset)</article-title>
          <year iso-8601-date="2024">2024</year>
          <uri>https://maps.amsterdam.nl/open_geodata/?k=505</uri>
        </element-citation>
      </ref>
      <ref id="ref-bordeaux_trees">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Bordeaux Métropole</string-name>
          </person-group>
          <article-title>Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole)</article-title>
          <year iso-8601-date="2024">2024</year>
          <uri>https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee</uri>
        </element-citation>
      </ref>
      <ref id="ref-boomregister">
        <element-citation>
          <person-group person-group-type="author">
            <string-name>Coöperatief Boomregister U.A.</string-name>
          </person-group>
          <article-title>Boom Register (Tree Register)</article-title>
          <year iso-8601-date="2014">2014</year>
          <uri>https://boomregister.nl/</uri>
        </element-citation>
      </ref>
      <ref id="ref-urban-trees">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Arevalo-Ramirez</surname>
              <given-names>Tito</given-names>
            </name>
            <name>
              <surname>Alfaro</surname>
              <given-names>Anali</given-names>
            </name>
            <name>
              <surname>Figueroa</surname>
              <given-names>José</given-names>
            </name>
            <name>
              <surname>Ponce-Donoso</surname>
              <given-names>Mauricio</given-names>
            </name>
            <name>
              <surname>Saavedra</surname>
              <given-names>Jose M.</given-names>
            </name>
            <name>
              <surname>Recabarren</surname>
              <given-names>Matías</given-names>
            </name>
            <name>
              <surname>Delpiano</surname>
              <given-names>José</given-names>
            </name>
          </person-group>
          <article-title>Challenges for computer vision as a tool for screening urban trees through street-view images</article-title>
          <source>Urban Forestry &amp; Urban Greening</source>
          <year iso-8601-date="2024">2024</year>
          <volume>95</volume>
          <issn>1618-8667</issn>
          <uri>https://www.sciencedirect.com/science/article/pii/S1618866724001146</uri>
          <pub-id pub-id-type="doi">10.1016/j.ufug.2024.128316</pub-id>
          <fpage>128316</fpage>
          <lpage/>
        </element-citation>
      </ref>
      <ref id="ref-olive-tree">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Safonova</surname>
              <given-names>Anastasiia</given-names>
            </name>
            <name>
              <surname>Guirado</surname>
              <given-names>Emilio</given-names>
            </name>
            <name>
              <surname>Maglinets</surname>
              <given-names>Yuriy</given-names>
            </name>
            <name>
              <surname>Alcaraz-Segura</surname>
              <given-names>Domingo</given-names>
            </name>
            <name>
              <surname>Tabik</surname>
              <given-names>Siham</given-names>
            </name>
          </person-group>
          <article-title>Olive tree biovolume from UAV multi-resolution image segmentation with mask r-CNN</article-title>
          <source>Sensors</source>
          <year iso-8601-date="2021">2021</year>
          <volume>21</volume>
          <issue>5</issue>
          <issn>1424-8220</issn>
          <uri>https://www.mdpi.com/1424-8220/21/5/1617</uri>
          <pub-id pub-id-type="doi">10.3390/s21051617</pub-id>
          <pub-id pub-id-type="pmid">33668984</pub-id>
          <fpage>1617</fpage>
          <lpage/>
        </element-citation>
      </ref>
      <ref id="ref-amf_gd_yolov8">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Zhong</surname>
              <given-names>Hao</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Zheyu</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Haoran</given-names>
            </name>
            <name>
              <surname>Wu</surname>
              <given-names>Jinzhuo</given-names>
            </name>
            <name>
              <surname>Lin</surname>
              <given-names>Wenshu</given-names>
            </name>
          </person-group>
          <article-title>Individual tree species identification for complex coniferous and broad-leaved mixed forests based on deep learning combined with UAV LiDAR data and RGB images</article-title>
          <source>Forests</source>
          <year iso-8601-date="2024">2024</year>
          <volume>15</volume>
          <issue>2</issue>
          <issn>1999-4907</issn>
          <uri>https://www.mdpi.com/1999-4907/15/2/293</uri>
          <pub-id pub-id-type="doi">10.3390/f15020293</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-lidar_benchmark">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Eysn</surname>
              <given-names>Lothar</given-names>
            </name>
            <name>
              <surname>Hollaus</surname>
              <given-names>Markus</given-names>
            </name>
            <name>
              <surname>Lindberg</surname>
              <given-names>Eva</given-names>
            </name>
            <name>
              <surname>Berger</surname>
              <given-names>Frédéric</given-names>
            </name>
            <name>
              <surname>Monnet</surname>
              <given-names>Jean-Matthieu</given-names>
            </name>
            <name>
              <surname>Dalponte</surname>
              <given-names>Michele</given-names>
            </name>
            <name>
              <surname>Kobal</surname>
              <given-names>Milan</given-names>
            </name>
            <name>
              <surname>Pellegrini</surname>
              <given-names>Marco</given-names>
            </name>
            <name>
              <surname>Lingua</surname>
              <given-names>Emanuele</given-names>
            </name>
            <name>
              <surname>Mongus</surname>
              <given-names>Domen</given-names>
            </name>
            <name>
              <surname>Pfeifer</surname>
              <given-names>Norbert</given-names>
            </name>
          </person-group>
          <article-title>A benchmark of lidar-based single tree detection methods using heterogeneous forest data from the alpine space</article-title>
          <source>Forests</source>
          <year iso-8601-date="2015">2015</year>
          <volume>6</volume>
          <issue>5</issue>
          <issn>1999-4907</issn>
          <uri>https://www.mdpi.com/1999-4907/6/5/1721</uri>
          <pub-id pub-id-type="doi">10.3390/f6051721</pub-id>
          <fpage>1721</fpage>
          <lpage>1747</lpage>
        </element-citation>
      </ref>
      <ref id="ref-rgb-dl-watershed">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Freudenberg</surname>
              <given-names>Maximilian</given-names>
            </name>
            <name>
              <surname>Magdon</surname>
              <given-names>Paul</given-names>
            </name>
            <name>
              <surname>Nölke</surname>
              <given-names>Nils</given-names>
            </name>
          </person-group>
          <article-title>Individual tree crown delineation in high-resolution remote sensing images based on u-net</article-title>
          <source>Neural Computing and Applications</source>
          <year iso-8601-date="2022">2022</year>
          <volume>34</volume>
          <issue>24</issue>
          <issn>1433-3058</issn>
          <pub-id pub-id-type="doi">10.1007/s00521-022-07640-4</pub-id>
          <fpage>22197</fpage>
          <lpage>22207</lpage>
        </element-citation>
      </ref>
      <ref id="ref-DeepForest">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Weinstein</surname>
              <given-names>Ben G.</given-names>
            </name>
            <name>
              <surname>Marconi</surname>
              <given-names>Sergio</given-names>
            </name>
            <name>
              <surname>Aubry-Kientz</surname>
              <given-names>Mélaine</given-names>
            </name>
            <name>
              <surname>Vincent</surname>
              <given-names>Gregoire</given-names>
            </name>
            <name>
              <surname>Senyondo</surname>
              <given-names>Henry</given-names>
            </name>
            <name>
              <surname>White</surname>
              <given-names>Ethan P.</given-names>
            </name>
          </person-group>
          <article-title>DeepForest: A python package for RGB deep learning tree crown delineation</article-title>
          <source>Methods in Ecology and Evolution</source>
          <year iso-8601-date="2020">2020</year>
          <volume>11</volume>
          <issue>12</issue>
          <uri>https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13472</uri>
          <pub-id pub-id-type="doi">10.1111/2041-210X.13472</pub-id>
          <fpage>1743</fpage>
          <lpage>1751</lpage>
        </element-citation>
      </ref>
      <ref id="ref-NEONdata">
        <element-citation>
          <person-group person-group-type="author">
            <name>
              <surname>Weinstein</surname>
              <given-names>Ben</given-names>
            </name>
            <name>
              <surname>Marconi</surname>
              <given-names>Sergio</given-names>
            </name>
            <name>
              <surname>White</surname>
              <given-names>Ethan</given-names>
            </name>
          </person-group>
          <article-title>Data for the NeonTreeEvaluation benchmark (0.2.2)</article-title>
          <publisher-name>Zenodo</publisher-name>
          <year iso-8601-date="2022">2022</year>
          <pub-id pub-id-type="doi">10.5281/zenodo.5914554</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-lidar_benchmark_2">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wang</surname>
              <given-names>Yunsheng</given-names>
            </name>
            <name>
              <surname>Hyyppä</surname>
              <given-names>Juha</given-names>
            </name>
            <name>
              <surname>Liang</surname>
              <given-names>Xinlian</given-names>
            </name>
            <name>
              <surname>Kaartinen</surname>
              <given-names>Harri</given-names>
            </name>
            <name>
              <surname>Yu</surname>
              <given-names>Xiaowei</given-names>
            </name>
            <name>
              <surname>Lindberg</surname>
              <given-names>Eva</given-names>
            </name>
            <name>
              <surname>Holmgren</surname>
              <given-names>Johan</given-names>
            </name>
            <name>
              <surname>Qin</surname>
              <given-names>Yuchu</given-names>
            </name>
            <name>
              <surname>Mallet</surname>
              <given-names>Clément</given-names>
            </name>
            <name>
              <surname>Ferraz</surname>
              <given-names>António</given-names>
            </name>
            <name>
              <surname>Torabzadeh</surname>
              <given-names>Hossein</given-names>
            </name>
            <name>
              <surname>Morsdorf</surname>
              <given-names>Felix</given-names>
            </name>
            <name>
              <surname>Zhu</surname>
              <given-names>Lingli</given-names>
            </name>
            <name>
              <surname>Liu</surname>
              <given-names>Jingbin</given-names>
            </name>
            <name>
              <surname>Alho</surname>
              <given-names>Petteri</given-names>
            </name>
          </person-group>
          <article-title>International benchmarking of the individual tree detection methods for modeling 3-d canopy structure for silviculture and forest ecology using airborne laser scanning</article-title>
          <source>IEEE Transactions on Geoscience and Remote Sensing</source>
          <year iso-8601-date="2016">2016</year>
          <volume>54</volume>
          <issue>9</issue>
          <pub-id pub-id-type="doi">10.1109/TGRS.2016.2543225</pub-id>
          <fpage>5011</fpage>
          <lpage>5027</lpage>
        </element-citation>
      </ref>
      <ref id="ref-gan_data_augment">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Sun</surname>
              <given-names>Chenxin</given-names>
            </name>
            <name>
              <surname>Huang</surname>
              <given-names>Chengwei</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Huaiqing</given-names>
            </name>
            <name>
              <surname>Chen</surname>
              <given-names>Bangqian</given-names>
            </name>
            <name>
              <surname>An</surname>
              <given-names>Feng</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Liwen</given-names>
            </name>
            <name>
              <surname>Yun</surname>
              <given-names>Ting</given-names>
            </name>
          </person-group>
          <article-title>Individual tree crown segmentation and crown width extraction from a heightmap derived from aerial laser scanning data using a deep learning framework</article-title>
          <source>Frontiers in Plant Science</source>
          <year iso-8601-date="2022">2022</year>
          <volume>13</volume>
          <issn>1664-462X</issn>
          <uri>https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2022.914974</uri>
          <pub-id pub-id-type="doi">10.3389/fpls.2022.914974</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-albumentations">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Buslaev</surname>
              <given-names>Alexander</given-names>
            </name>
            <name>
              <surname>Iglovikov</surname>
              <given-names>Vladimir I.</given-names>
            </name>
            <name>
              <surname>Khvedchenya</surname>
              <given-names>Eugene</given-names>
            </name>
            <name>
              <surname>Parinov</surname>
              <given-names>Alex</given-names>
            </name>
            <name>
              <surname>Druzhinin</surname>
              <given-names>Mikhail</given-names>
            </name>
            <name>
              <surname>Kalinin</surname>
              <given-names>Alexandr A.</given-names>
            </name>
          </person-group>
          <article-title>Albumentations: Fast and flexible image augmentations</article-title>
          <source>Information</source>
          <year iso-8601-date="2020">2020</year>
          <volume>11</volume>
          <issue>2</issue>
          <issn>2078-2489</issn>
          <uri>https://www.mdpi.com/2078-2489/11/2/125</uri>
          <pub-id pub-id-type="doi">10.3390/info11020125</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-lidar_classification">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Diab</surname>
              <given-names>Ahmed</given-names>
            </name>
            <name>
              <surname>Kashef</surname>
              <given-names>Rasha</given-names>
            </name>
            <name>
              <surname>Shaker</surname>
              <given-names>Ahmed</given-names>
            </name>
          </person-group>
          <article-title>Deep learning for LiDAR point cloud classification in remote sensing</article-title>
          <source>Sensors (Basel)</source>
          <year iso-8601-date="2022-10">2022</year>
          <month>10</month>
          <volume>22</volume>
          <issue>20</issue>
          <pub-id pub-id-type="doi">10.3390/s22207868</pub-id>
          <pub-id pub-id-type="pmid">36298220</pub-id>
          <fpage>7868</fpage>
          <lpage/>
        </element-citation>
      </ref>
      <ref id="ref-lidar_rgb_wst">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Qin</surname>
              <given-names>Haiming</given-names>
            </name>
            <name>
              <surname>Zhou</surname>
              <given-names>Weiqi</given-names>
            </name>
            <name>
              <surname>Yao</surname>
              <given-names>Yang</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Weimin</given-names>
            </name>
          </person-group>
          <article-title>Individual tree segmentation and tree species classification in subtropical broadleaf forests using UAV-based LiDAR, hyperspectral, and ultrahigh-resolution RGB data</article-title>
          <source>Remote Sensing of Environment</source>
          <year iso-8601-date="2022">2022</year>
          <volume>280</volume>
          <issn>0034-4257</issn>
          <uri>https://www.sciencedirect.com/science/article/pii/S0034425722002577</uri>
          <pub-id pub-id-type="doi">10.1016/j.rse.2022.113143</pub-id>
          <fpage>113143</fpage>
          <lpage/>
        </element-citation>
      </ref>
      <ref id="ref-lidar_rgb_acnet">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Li</surname>
              <given-names>Yingbo</given-names>
            </name>
            <name>
              <surname>Chai</surname>
              <given-names>Guoqi</given-names>
            </name>
            <name>
              <surname>Wang</surname>
              <given-names>Yueting</given-names>
            </name>
            <name>
              <surname>Lei</surname>
              <given-names>Lingting</given-names>
            </name>
            <name>
              <surname>Zhang</surname>
              <given-names>Xiaoli</given-names>
            </name>
          </person-group>
          <article-title>ACE r-CNN: An attention complementary and edge detection-based instance segmentation algorithm for individual tree species identification using UAV RGB images and LiDAR data</article-title>
          <source>Remote Sensing</source>
          <year iso-8601-date="2022">2022</year>
          <volume>14</volume>
          <issue>13</issue>
          <issn>2072-4292</issn>
          <uri>https://www.mdpi.com/2072-4292/14/13/3035</uri>
          <pub-id pub-id-type="doi">10.3390/rs14133035</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-watershed">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Vincent</surname>
              <given-names>L.</given-names>
            </name>
            <name>
              <surname>Soille</surname>
              <given-names>P.</given-names>
            </name>
          </person-group>
          <article-title>Watersheds in digital spaces: An efficient algorithm based on immersion simulations</article-title>
          <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
          <year iso-8601-date="1991">1991</year>
          <volume>13</volume>
          <issue>6</issue>
          <pub-id pub-id-type="doi">10.1109/34.87344</pub-id>
          <fpage>583</fpage>
          <lpage>598</lpage>
        </element-citation>
      </ref>
      <ref id="ref-lidar_watershed">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Kwak</surname>
              <given-names>Doo-Ahn</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Woo-Kyun</given-names>
            </name>
            <name>
              <surname>Lee</surname>
              <given-names>Jun-Hak</given-names>
            </name>
            <name>
              <surname>Biging</surname>
              <given-names>Greg S.</given-names>
            </name>
            <name>
              <surname>Gong</surname>
              <given-names>Peng</given-names>
            </name>
          </person-group>
          <article-title>Detection of individual trees and estimation of tree height using LiDAR data</article-title>
          <source>Journal of Forest Research</source>
          <year iso-8601-date="2007">2007</year>
          <volume>12</volume>
          <issue>6</issue>
          <issn>1610-7403</issn>
          <pub-id pub-id-type="doi">10.1007/s10310-007-0041-9</pub-id>
          <fpage>425</fpage>
          <lpage>434</lpage>
        </element-citation>
      </ref>
      <ref id="ref-rgb_analytical">
        <element-citation publication-type="chapter">
          <person-group person-group-type="author">
            <name>
              <surname>Gomes</surname>
              <given-names>Marilia Ferreira</given-names>
            </name>
            <name>
              <surname>Maillard</surname>
              <given-names>Philippe</given-names>
            </name>
          </person-group>
          <article-title>Detection of tree crowns in very high spatial resolution images</article-title>
          <source>Environmental applications of remote sensing</source>
          <person-group person-group-type="editor">
            <name>
              <surname>Marghany</surname>
              <given-names>Maged</given-names>
            </name>
          </person-group>
          <publisher-name>IntechOpen</publisher-name>
          <publisher-loc>Rijeka</publisher-loc>
          <year iso-8601-date="2016">2016</year>
          <pub-id pub-id-type="doi">10.5772/62122</pub-id>
        </element-citation>
      </ref>
      <ref id="ref-local-maximum">
        <element-citation publication-type="article-journal">
          <person-group person-group-type="author">
            <name>
              <surname>Wulder</surname>
              <given-names>Mike</given-names>
            </name>
            <name>
              <surname>Niemann</surname>
              <given-names>K.Olaf</given-names>
            </name>
            <name>
              <surname>Goodenough</surname>
              <given-names>David G.</given-names>
            </name>
          </person-group>
          <article-title>Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery</article-title>
          <source>Remote Sensing of Environment</source>
          <year iso-8601-date="2000">2000</year>
          <volume>73</volume>
          <issue>1</issue>
          <issn>0034-4257</issn>
          <uri>https://www.sciencedirect.com/science/article/pii/S0034425700001012</uri>
          <pub-id pub-id-type="doi">10.1016/S0034-4257(00)00101-2</pub-id>
          <fpage>103</fpage>
          <lpage>114</lpage>
        </element-citation>
      </ref>
      <ref id="ref-valley-following">
        <element-citation publication-type="paper-conference">
          <person-group person-group-type="author">
            <name>
              <surname>Gougeon</surname>
              <given-names>François A</given-names>
            </name>
            <etal/>
          </person-group>
          <article-title>Automatic individual tree crown delineation using a valley-following algorithm and rule-based system</article-title>
          <source>Proc. International forum on automated interpretation of high spatial resolution digital imagery for forestry, victoria, british columbia, canada</source>
          <publisher-name>Citeseer</publisher-name>
          <year iso-8601-date="1998">1998</year>
          <uri>https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf</uri>
          <fpage>11</fpage>
          <lpage>23</lpage>
        </element-citation>
      </ref>
      <ref id="ref-template-matching">
        <element-citation publication-type="thesis">
          <person-group person-group-type="author">
            <name>
              <surname>Pollock</surname>
              <given-names>Richard James</given-names>
            </name>
          </person-group>
          <article-title>The automatic recognition of individual trees in aerial images of forests based on a synthetic tree crown image model</article-title>
          <publisher-name>The University of British Columbia (Canada)</publisher-name>
          <year iso-8601-date="1996">1996</year>
          <isbn>0612148157</isbn>
          <uri>https://dx.doi.org/10.14288/1.0051597</uri>
        </element-citation>
      </ref>
      <ref id="ref-yolo">
        <element-citation publication-type="paper-conference">
          <person-group person-group-type="author">
            <name>
              <surname>Redmon</surname>
              <given-names>Joseph</given-names>
            </name>
            <name>
              <surname>Divvala</surname>
              <given-names>Santosh</given-names>
            </name>
            <name>
              <surname>Girshick</surname>
              <given-names>Ross</given-names>
            </name>
            <name>
              <surname>Farhadi</surname>
              <given-names>Ali</given-names>
            </name>
          </person-group>
          <article-title>You only look once: Unified, real-time object detection</article-title>
          <source>2016 IEEE conference on computer vision and pattern recognition (CVPR)</source>
          <year iso-8601-date="2016">2016</year>
          <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>
          <fpage>779</fpage>
          <lpage>788</lpage>
        </element-citation>
      </ref>
    </ref-list>
  </back>
  <sub-article article-type="notebook" id="nb-4-nb-article">
    <front-stub>
      <title-group>
        <article-title>Tree object detection using airborne images and LiDAR
point clouds</article-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" corresp="yes">
          <name>
            <surname>Bry</surname>
            <given-names>Alexandre</given-names>
          </name>
          <string-name>Alexandre Bry</string-name>
          <email>alexandre.bry.21@polytechnique.org</email>
          <role vocab="https://credit.niso.org" vocab-term="writing – original draft" vocab-term-identifier="https://credit.niso.org/contributor-roles/writing-original-draft/">writing</role>
          <xref ref-type="aff" rid="aff-1-nb-article">a</xref>
          <xref ref-type="aff" rid="aff-2-nb-article">b</xref>
          <xref ref-type="corresp" rid="cor-1-nb-article">*</xref>
        </contrib>
      </contrib-group>
      <aff id="aff-1-nb-article">
        <institution content-type="dept">Département
d’informatique</institution>
        <institution-wrap>
          <institution>École polytechnique</institution>
        </institution-wrap>
        <city>Palaiseau</city>
        <country>France</country>
        <ext-link ext-link-type="uri" xlink:href="https://portail.polytechnique.edu/informatique/fr/page-daccueil">https://portail.polytechnique.edu/informatique/fr/page-daccueil</ext-link>
      </aff>
      <aff id="aff-2-nb-article">
        <institution content-type="dept">Research</institution>
        <institution-wrap>
          <institution>Geodan B.V.</institution>
        </institution-wrap>
        <city>Amsterdam</city>
        <country>Netherlands</country>
        <ext-link ext-link-type="uri" xlink:href="https://research.geodan.nl/">https://research.geodan.nl/</ext-link>
      </aff>
      <author-notes>
        <corresp id="cor-1-nb-article">alexandre.bry.21@polytechnique.org</corresp>
      </author-notes>
      <abstract>
        <p>This is the abstract. It can be on multiple lines and contain
<bold>Markdown</bold>.</p>
      </abstract>
    </front-stub>
    <body>
      <sec id="introduction-nb-article">
        <title>Introduction</title>
        <p>The goal of the internship was to study the possibility of
  combining LiDAR point clouds and aerial images in a deep learning
  model to identify individual trees. The two types of data are indeed
  complementary, as point clouds capture geometric shapes, while images
  capture colors. However, combining them into a format that allows a
  model to handle them simultaneously is not a straightforward task
  because they inherently have a very different spatial repartition and
  encoding.</p>
        <p>In this work, I focused on one specific deep learning model, and
  tried to improve it by using more information from the LiDAR point
  cloud. To do this, I had to create my own tree annotations dataset,
  with which I also tried to study the ability of this new model to
  detect trees that are covered by other trees.</p>
      </sec>
      <sec id="state-of-the-art-nb-article">
        <title>State-of-the-art</title>
        <sec id="computer-vision-tasks-related-to-trees-nb-article">
          <title>Computer vision tasks related to trees</title>
          <p>Before talking about models and datasets, let’s define properly
    the task that this project focused on, in the midst of all the
    various computer vision tasks, and specifically those related to
    tree detection.</p>
          <p>The first main differentiation between tree recognition tasks
    comes from the acquisition of the data. There are some very
    different tasks and methods using either ground data or
    aerial/satellite data. This is especially true when focusing on
    urban trees, since a lot of street view data is available
    (<xref alt="Arevalo-Ramirez et al. 2024" rid="ref-urban-trees-nb-article" ref-type="bibr">Arevalo-Ramirez
    et al. 2024</xref>).</p>
          <p>This leads to the second variation, which is related to the kind
    of environment that we are interested in. There are mainly three
    types of environments, which among other things, influence the
    organization of the trees in space: urban areas, tree plantations
    and forests. This is important, because the tasks and the difficulty
    depends on the type of environment. Tree plantations are much easier
    to work with than completely wild forests, while urban areas contain
    various levels of difficulty ranging from alignment trees to private
    and disorganized gardens and parks. For this project, we mainly
    focused on urban areas, but everything should still be applicable to
    tree plantations and forests.</p>
          <p>Then, the four fundamental computer vision tasks have their
    application when dealing with trees
    (<xref alt="Safonova et al. 2021" rid="ref-olive-tree-nb-article" ref-type="bibr">Safonova
    et al. 2021</xref>):</p>
          <list list-type="bullet">
            <list-item>
              <p>Classification, although this is quite rare for airborne tree
        applications since there are multiple trees on each image most
        of the time</p>
            </list-item>
            <list-item>
              <p>Detection, which consists in detecting objects and placing
        boxes around them</p>
            </list-item>
            <list-item>
              <p>Semantic segmentation, which consists in associating a label
        to every pixel of an image</p>
            </list-item>
            <list-item>
              <p>Instance segmentation, which consists in adding a layer of
        complexity to semantic segmentation by also differentiating
        between the different instances of each class</p>
            </list-item>
          </list>
          <p>These generic tasks can be extended by trying to get more
    information about the trees. The most common information are the
    species and the height, but some models also try to predict the
    health of the trees, or their carbon stock.</p>
          <p>In this work, the task that is tackled is the detection of trees,
    with a special classification between several labels related to the
    discrepancies between the different kinds of data. The kind of model
    that is used would also have allowed to focus on some more advanced
    tasks, by replacing detection with instance segmentation and asking
    the model to also predict the species. But due to the difficulties
    regarding the dataset, a simpler task with a simpler dataset was
    used, without compromising the ability to experiment with different
    possible improvements of the model. The difficulties and the
    experiments are developed below.</p>
        </sec>
        <sec id="datasets-nb-article">
          <title>Datasets</title>
          <sec id="sec-sota-datasets-requirements-nb-article">
            <title>Requirements</title>
            <p>Before presenting the different promising datasets and the
      reasons why they were not fully usable for the project, let’s
      enumerate the different conditions and requirements for the tree
      instance segmentation task:</p>
            <list list-type="bullet">
              <list-item>
                <p>Multiple types of data:</p>
                <list list-type="bullet">
                  <list-item>
                    <p>Aerial RGB images</p>
                  </list-item>
                  <list-item>
                    <p>LiDAR point clouds (preferably aerial)</p>
                  </list-item>
                  <list-item>
                    <p>(Optional) Aerial infrared (CIR) images</p>
                  </list-item>
                </list>
              </list-item>
              <list-item>
                <p>Tree crown annotations or bounding boxes</p>
              </list-item>
              <list-item>
                <p>High-enough resolution:</p>
                <list list-type="bullet">
                  <list-item>
                    <p>For images, about 25 cm</p>
                  </list-item>
                  <list-item>
                    <p>For point clouds, about 10 cm</p>
                  </list-item>
                </list>
              </list-item>
            </list>
            <p>Here are the explanations for these requirements. As for the
      types of data, RGB images and point clouds are required to
      experiment on the ability of the model to combine the two very
      different kinds of information they hold. Having infrared data as
      well could be beneficial, but it was not necessary. Regarding tree
      annotations, it was necessary to have a way to spatially identify
      them individually, using crown contours or simply bounding boxes.
      Since the model outputs bounding boxes, any kind of other format
      could easily be transformed to bounding boxes. Finally, the
      resolution had to be high enough to identify individual trees and
      be able to really use the data. For the point clouds especially,
      the whole idea was to see if and how the topology of the trees
      could be learnt, using at least the trunks and even the biggest
      branches if possible. Therefore, even if they are not really
      comparable, this is the reason why the required resolution is more
      precise for the point clouds.</p>
            <p>Unfortunately, none of the datasets that I found matched all
      these criteria. Furthermore, I didn’t find any overlapping
      datasets that I could merge to create a dataset with all the
      required types of data. In the next parts, I will go through the
      different kinds of datasets that exist, the reasons why they did
      not really fit for the project and the ideas I got when searching
      for a way to use them.</p>
          </sec>
          <sec id="existing-tree-datasets-nb-article">
            <title>Existing tree datasets</title>
            <p>As explained above, there were quite a lot of requirements to
      fulfill to have a complete dataset usable for the task. This means
      that almost all the available datasets were missing something, as
      they were mainly focusing on using one kind of data and trying to
      make the most out of it, instead of trying to use all the types of
      data together.</p>
            <p>The most comprehensive list of tree annotations datasets was
      published in OpenForest
      (<xref alt="Ouaknine et al. 2023" rid="ref-OpenForest-nb-article" ref-type="bibr">Ouaknine
      et al. 2023</xref>). FoMo-Bench
      (<xref alt="Bountos, Ouaknine, and Rolnick 2023" rid="ref-FoMo-Bench-nb-article" ref-type="bibr">Bountos,
      Ouaknine, and Rolnick 2023</xref>) also lists several interesting
      datasets, even though most of them can also be found in
      OpenForest. Without enumerating all of them, there were multiple
      kinds of datasets that all have their own flaws regarding the
      requirements I was looking for.</p>
            <p>Firstly, there are the forest inventories. TALLO
      (<xref alt="Jucker et al. 2022" rid="ref-TALLO-nb-article" ref-type="bibr">Jucker
      et al. 2022</xref>) is probably the most interesting one in this
      category, because it contains a lot of spatial information about
      almost 500K trees, with their locations, their crown radii and
      their heights. Therefore, everything needed to localize trees is
      in the dataset. However, I didn’t manage to find RGB images or
      LiDAR point clouds of the areas where the trees are located,
      making it impossible to use these annotations to train tree
      detection.</p>
            <p>Secondly, there are the RGB datasets. ReforesTree
      (<xref alt="Reiersen et al. 2022" rid="ref-ReforesTree-nb-article" ref-type="bibr">Reiersen
      et al. 2022</xref>) and MillionTrees
      (<xref alt="B. Weinstein 2023" rid="ref-MillionTrees-nb-article" ref-type="bibr">B.
      Weinstein 2023</xref>) are two of them and the quality of their
      images are high. The only drawback of these datasets is obviously
      that they don’t provide any kind of point cloud, which make them
      unsuitable for the task.</p>
            <p>Thirdly, there are the LiDAR datasets, such as
      (<xref alt="Kalinicheva et al. 2022" rid="ref-WildForest3D-nb-article" ref-type="bibr">Kalinicheva
      et al. 2022</xref>) and
      (<xref alt="Puliti et al. 2023" rid="ref-FOR-instance-nb-article" ref-type="bibr">Puliti
      et al. 2023</xref>). Similarly to RGB datasets, they lack one of
      the data source for the task I worked on. But unlike them, they
      have the advantage that the missing data could be much easier to
      acquire from another source, since RGB aerial or satellite images
      are much more common than LiDAR point clouds. However, this
      solution was abandoned for two main reasons. First it is quite
      challenging to find the exact locations where the point clouds
      were acquired. Then, even when the location is known, it is often
      in the middle of a forest where the quality of satellite imagery
      is very low.</p>
            <p>Finally, I also found two datasets that had RGB and LiDAR
      components. The first one is MDAS
      (<xref alt="Hu et al. 2023" rid="ref-MDAS-nb-article" ref-type="bibr">Hu et
      al. 2023</xref>). This benchmark dataset encompasses RGB images,
      hyperspectral images and Digital Surface Models (DSM). There were
      however two major flaws. The obvious one was that this dataset was
      created with land semantic segmentation tasks in mind, so there
      was no tree annotations. The less obvious one was that a DSM is
      not a point cloud, even though it is some kind of 3D information
      and was often created using a LiDAR point cloud. As a consequence,
      I would have been very limited in my ability to use the point
      cloud.</p>
            <p>The only real dataset with RGB and LiDAR came from NEON
      (<xref alt="B. Weinstein, Marconi, and White 2022" rid="ref-NEONdata-nb-article" ref-type="bibr">B.
      Weinstein, Marconi, and White 2022</xref>). This dataset contains
      exactly all the data I was looking for, with RGB images,
      hyperspectral images and LiDAR point clouds. With 30975 tree
      annotations, it is also a quite large dataset, spanning across
      multiple various forests. The reason why I decided not to use it
      despite all this is that at the beginning of the project, I
      thought that the quality of the images and the point clouds was
      too low. Looking back on this decision, I think that I probably
      could have worked with this dataset and gotten great results. This
      would have saved me the time spent annotating the trees for my own
      dataset, which I will talk more about later. My decision was also
      influenced by the quality of the images and the point clouds
      available in the Netherlands, which I will talk about in the next
      section.</p>
          </sec>
          <sec id="public-data-nb-article">
            <title>Public data</title>
            <p>After rejecting all the available datasets I had found, the
      only solution I had left was to create my own dataset. I won’t
      dive too much in this process that I will explain in
      <xref alt="Section 3" rid="sec-dataset-nb-article">Section 3</xref>. I just
      want to mention all the publicly available datasets that I used or
      could have used to create this custom dataset.</p>
            <p>For practical reasons, the two countries where I mostly
      searched for available data are France and the Netherlands. I was
      looking for three different data types independently:</p>
            <list list-type="bullet">
              <list-item>
                <p>RGB (and if possible CIR) images</p>
              </list-item>
              <list-item>
                <p>LiDAR point clouds</p>
              </list-item>
              <list-item>
                <p>Tree annotations</p>
              </list-item>
            </list>
            <p>These three types of data are available in similar ways in both
      countries, although the Netherlands have a small edge over France.
      RGB images are really easy to find in France with the BD ORTHO
      (<xref alt="Institut national de l’information géographique et forestière (IGN) 2021" rid="ref-IGN_BD_ORTHO-nb-article" ref-type="bibr">Institut
      national de l’information géographique et forestière (IGN)
      2021</xref>) and in the Netherlands with the Luchtfotos
      (<xref alt="Beeldmateriaal Nederland 2024" rid="ref-Luchtfotos-nb-article" ref-type="bibr">Beeldmateriaal
      Nederland 2024</xref>), but the resolution is better in the
      Netherlands (8 cm vs 20 cm). Hyperspectral images are also
      available in both countries, although for those the resolution is
      only 25 cm in the Netherlands.</p>
            <p>As for LiDAR point clouds, the Netherlands have a small edge
      over France, because they have already completed their forth
      version covering the whole country with AHN4
      (<xref alt="Actueel Hoogtebestand Nederland 2020" rid="ref-AHN4-nb-article" ref-type="bibr">Actueel
      Hoogtebestand Nederland 2020</xref>), and are working on the fifth
      version. In France, data acquisition for the first LiDAR point
      cloud covering the whole country started a few years ago
      (<xref alt="Institut national de l’information géographique et forestière (IGN) 2020" rid="ref-IGN_LiDAR_HD-nb-article" ref-type="bibr">Institut
      national de l’information géographique et forestière (IGN)
      2020</xref>). It is not yet finished, even though data is already
      available for half of the country. The other advantage of the data
      from Netherlands regarding LiDAR point clouds is that all flights
      are performed during winter, which allows light beams to penetrate
      more deeply in trees and reach trunks and branches. This is not
      the case in France.</p>
            <p>The part that is missing in both countries is related to tree
      annotations. Many municipalities have datasets containing
      information about all the public trees they handle. This is for
      example the case for Amsterdam
      (<xref alt="Gemeente Amsterdam 2024" rid="ref-amsterdam_trees-nb-article" ref-type="bibr">Gemeente
      Amsterdam 2024</xref>) and Bordeaux
      (<xref alt="Bordeaux Métropole 2024" rid="ref-bordeaux_trees-nb-article" ref-type="bibr">Bordeaux
      Métropole 2024</xref>). However, these datasets cannot really be
      used as ground truth for a custom dataset for several reasons.
      First, many of them do not contain coordinates indicating the
      position of each tree in the city. Then, even those that contain
      coordinates are most of the time missing any kind of information
      allowing to deduce a bounding box for the tree crowns. Finally,
      even if they did contain everything, they only focus on public
      trees, and are missing every single tree located in a private
      area. Since public and private areas are obviously imbricated in
      all cities, it means that any area we try to train the model on
      would be missing all the private trees, making the training
      process impossible because we cannot have only a partial
      annotation of images.</p>
            <p>The other tree annotation source that we could have used is
      Boomregister
      (<xref alt="Coöperatief Boomregister U.A. 2014" rid="ref-boomregister-nb-article" ref-type="bibr">Coöperatief
      Boomregister U.A. 2014</xref>). This work covers the whole of the
      Netherlands, including public and private trees. However, the
      precision of the masks is far from perfect, and many trees are
      missing or incorrectly segmented, especially when they are less
      than 9 m heigh or have a crown diameter smaller than 4 m.
      Therefore, even it is a very impressive piece of work, we thought
      that it could not be used as training data for a deep learning
      models due to its biases and imperfections.</p>
          </sec>
          <sec id="sec-sota-dataset-augment-nb-article">
            <title>Dataset augmentation techniques</title>
            <p>When a dataset is too small to train a model, there are several
      ways of artificially enlarging it.</p>
            <p>The most common way to do it is to randomly apply deterministic
      or random transformations to the data, during the training
      process, to be able to generate several unique and different
      realistic data instances from one real data instance. There are a
      lot of different transformations that can be applied to images,
      divided into two categories: pixel-level and spatial-level
      (<xref alt="Buslaev et al. 2020" rid="ref-albumentations-nb-article" ref-type="bibr">Buslaev
      et al. 2020</xref>). Pixel-level transformations modify the value
      of individual pixels, by applying different filters, such as
      random noise, color shifts and more complex effects like fog and
      sun flare. Spatial-level transformations modify the spatial
      arrangement of the image, without changing the pixel values. In
      other words, these transformations move the pixels in the image.
      The transformations range from simple rotations and croppings to
      complex spatial distortions. In the end, all these transformations
      are simply producing one artificial image out of one real
      image.</p>
            <p>Another way to enlarge a dataset is to instead generate
      completely new input data sharing the same properties as the
      initial dataset. This can be done using Generative Adversarial
      Networks (GAN). These models usually have two parts, a generator
      and a discriminator, which are trained in parallel. The generator
      learns to produce realistic artificial data, while the
      discriminator learns to identify real data and artificial data
      produced by the generator. If the training is successful, we can
      then use the generator and random seeds to generate random but
      realistic artificial data similar to the dataset. This method has
      for example been successfully used to generate artificial tree
      height maps
      (<xref alt="Sun et al. 2022" rid="ref-gan_data_augment-nb-article" ref-type="bibr">Sun
      et al. 2022</xref>).</p>
          </sec>
        </sec>
        <sec id="algorithms-and-models-nb-article">
          <title>Algorithms and models</title>
          <p>In this section, the different algorithms and methods are grouped
    according to the type of data they use as input.</p>
          <sec id="images-only-nb-article">
            <title>Images only</title>
            <p>Then, there are methods that perform tree detection using only
      visible or hyperspectral images or both. Several different
      algorithms have been developed to analytically delineate tree
      crowns from RGB images, by using the particular shape of the trees
      and its effect on images
      (<xref alt="Gomes and Maillard 2016" rid="ref-rgb_analytical-nb-article" ref-type="bibr">Gomes
      and Maillard 2016</xref>). Without diving into the details, here
      are a few of them. The watershed algorithm identifies trees to
      inverted watersheds in the grey-scale image and tree crowns
      frontiers are found by incrementally flooding the watersheds
      (<xref alt="Vincent and Soille 1991" rid="ref-watershed-nb-article" ref-type="bibr">Vincent
      and Soille 1991</xref>). The local maxima filtering uses the
      intensity of the pixels in the grey-scale image to identify the
      brightest points locally and use them as treetops
      (<xref alt="Wulder, Niemann, and Goodenough 2000" rid="ref-local-maximum-nb-article" ref-type="bibr">Wulder,
      Niemann, and Goodenough 2000</xref>). Reversely, the
      valley-following algorithm uses the darkest pixels which are
      considered as the junctions between the trees since shaded areas
      are the lower part of the tree crowns
      (<xref alt="Gougeon et al. 1998" rid="ref-valley-following-nb-article" ref-type="bibr">Gougeon
      et al. 1998</xref>). Another interesting algorithm is template
      matching. This algorithm simulates the appearance of simple tree
      templates with the light effects, and tries to identify similar
      patterns in the grey-scale image
      (<xref alt="Pollock 1996" rid="ref-template-matching-nb-article" ref-type="bibr">Pollock
      1996</xref>). Combinations of these techniques and others have
      also been proposed.</p>
            <p>But with the recent developments of deep learning in image
      analysis, deep learning models are increasingly used to detect
      trees using RGB images. In some cases, deep learning is used to
      extract features that can then be the input of one of the
      algorithms described above. One example is the use of two neural
      networks to predict masks, outlines and distance transforms which
      can then be the input of a watershed algorithm
      (<xref alt="Freudenberg, Magdon, and Nölke 2022" rid="ref-rgb-dl-watershed-nb-article" ref-type="bibr">Freudenberg,
      Magdon, and Nölke 2022</xref>). In other cases, a deep learning
      model is responsible of directly detecting tree masks or bounding
      boxes, often using CNNs, given the images
      (<xref alt="B. G. Weinstein et al. 2020" rid="ref-DeepForest-nb-article" ref-type="bibr">B.
      G. Weinstein et al. 2020</xref>).</p>
          </sec>
          <sec id="lidar-only-nb-article">
            <title>LiDAR only</title>
            <p>Some of the methods to identify individual trees use LiDAR data
      only. There are a lot of different ways to use and analyze point
      clouds, but the one that is mostly used for trees is based on
      height maps, or Canopy Height Models (CHM).</p>
            <p>A CHM is a raster computed as the subtraction of the Digital
      Terrain Model (DTM) to the Digital Surface Model (DSM). What it
      means is that a CHM contains the height above ground of the
      highest point in the area corresponding to each pixel. This CHM
      can for example be used as the input raster for the watershed
      algorithm, as it contains the height values that can be used to
      determine local maxima
      (<xref alt="Kwak et al. 2007" rid="ref-lidar_watershed-nb-article" ref-type="bibr">Kwak
      et al. 2007</xref>). A lot of different analytical methods and
      variations of the simple CHM were proposed to perform individual
      tree detection, but in the end, most of them still the concept of
      local maxima
      (<xref alt="Eysn et al. 2015" rid="ref-lidar_benchmark-nb-article" ref-type="bibr">Eysn
      et al. 2015</xref>;
      <xref alt="Wang et al. 2016" rid="ref-lidar_benchmark_2-nb-article" ref-type="bibr">Wang
      et al. 2016</xref>). A CHM can also be used as the input of any
      kind of convolutional neural network (CNN) because it is shaped
      exactly like any image. This allows to use a lot of different
      techniques usually applied to object detection in images.</p>
            <p>Then, even though I finally used an approach similar to the
      CHM, I want to mention other kinds of deep learning techniques
      that exist and could potentially leverage all the information
      contained in a point cloud. These techniques can be divided in two
      categories: projection-based and point-based methods
      (<xref alt="Diab, Kashef, and Shaker 2022" rid="ref-lidar_classification-nb-article" ref-type="bibr">Diab,
      Kashef, and Shaker 2022</xref>). The main difference between the
      two is that projection-based techniques are based on grids while
      point-based methods take unstructured point clouds as input. Among
      projection-based methods, the most basic method is 2D CNN, which
      is how CHM can be processed. Then, multiview representation tries
      to tackle the 3D aspect by projecting the point cloud in multiple
      directions before merging them together. To really deal with 3D
      data, volumetric grid representation consists in using 3D
      occupancy grids, which are processed using 3D CNNs. Among
      point-based methods, there are methods based on PointNet, which
      are able to extract features and perform the classical computer
      vision tasks by taking point clouds as input. Finally,
      Convolutional Point Networks use a continuous generalization of
      convolutions to apply convolution kernels to arbitrarily
      distributed point clouds.</p>
          </sec>
          <sec id="lidar-and-images-nb-article">
            <title>LiDAR and images</title>
            <p>Let’s now talk about the models of interest for this work,
      which are machine learning pipelines using both LiDAR point cloud
      data and RGB images.</p>
            <p>The first pipeline
      (<xref alt="Qin et al. 2022" rid="ref-lidar_rgb_wst-nb-article" ref-type="bibr">Qin
      et al. 2022</xref>) uses a watershed algorithm to extract crown
      boundaries, before extracting individual tree features from the
      LiDAR point cloud, hyperspectral and RGB images. These features
      are then used by a random forest classifier to identify which
      species the tree belongs to. This pipeline therefore makes the
      most out of all data to identify species, but sticks to an
      improved variant of the watershed for individual tree
      segmentation, which only uses a CHM raster.</p>
            <p>Other works focused on using only one model that is able to
      take both the CHM and the RGB data as input and combine them to
      make the most out of all the available data. Among other models,
      there are for example ACE R-CNN
      (<xref alt="Li et al. 2022" rid="ref-lidar_rgb_acnet-nb-article" ref-type="bibr">Li
      et al. 2022</xref>), an evolution of Mask region-based convolution
      neural network (Mask R-CNN) and AMF GD YOLOv8
      (<xref alt="Zhong et al. 2024" rid="ref-amf_gd_yolov8-nb-article" ref-type="bibr">Zhong
      et al. 2024</xref>), an evolution of YOLOv8. These two models have
      proven to give much better results when using both the images and
      the LiDAR data as a CHM thant when using only one of them.</p>
          </sec>
        </sec>
      </sec>
      <sec id="objectives-and-motivations-nb-article">
        <title>Objectives and motivations</title>
        <p>In this section, I will explain the objectives that I set for this
  internship and the motivations that led to them.</p>
        <sec id="data-and-model-nb-article">
          <title>Data and model</title>
          <p>The basis for this internship was to look at deep learning models
    to detect trees using LiDAR and aerial images. In fourth months, it
    would have been difficult to dive into the literature, think about a
    completely new approach and develop it. Therefore, I wanted to find
    an interesting and not too complicated deep learning model, and try
    a few changes that would hopefully improve the results.</p>
          <p>This idea was also reinforced by the decision to create my own
    dataset for two reasons. The first reason was the small number of
    openly available tree annotation datasets which contained both LiDAR
    and RGB data. I therefore thought that creating a new dataset and
    making it available could be a great contribution. The second reason
    was to have more control over the definition and the characteristics
    of the dataset, to be able to experiment on the detection of
    specific trees.</p>
        </sec>
        <sec id="sec-obj-covered_trees-nb-article">
          <title>Covered trees</title>
          <p>The main thing that I wanted to experiment on was the possibility
    to make a better use of the LiDAR point cloud to be able to detect
    covered trees. Covered trees are the trees which are located
    partially or completely under another tree’s crown. This makes them
    impossible to completely delineate when using only data that is
    visible from above. These trees are not meaningless or negligible,
    because as demonstrated in this paper
    (<xref alt="Wang et al. 2016" rid="ref-lidar_benchmark_2-nb-article" ref-type="bibr">Wang
    et al. 2016</xref>), they can represent up to 50% of the trees in a
    forest.</p>
          <p>However, doing this implied being able to process them on the
    whole pipeline. In practice, covered trees are never annotated in
    all the datasets that are created using only RGB images, because
    they are simply not visible. This means that creating my own dataset
    was the only solution to have a dataset containing really all trees
    including covered trees and be able to easily identify them.</p>
        </sec>
        <sec id="multiple-layers-of-chm-nb-article">
          <title>Multiple layers of CHM</title>
          <p>Being able to find covered trees meant finding a way to find more
    information out of the LiDAR point cloud than what is contained by
    the CHM. In fact, the CHM only contains a very small part of the
    point cloud and doesn’t really benefit from the 3D shape that is
    contained in the point cloud. This is particularly true when the
    point cloud is acquired in a season where trees don’t have their
    leaves, because the LiDAR then goes deep into the tree more easily,
    and can find the trunk and many of the largest branches.</p>
          <p>Therefore, getting information below the tree crown surface was
    mandatory to find covered trees. But it could also be helpful for
    the model to find better separations between each tree, thanks to
    having access to the branches and the trunks. Even though I didn’t
    end up asking the model to also identify the species, this is
    another task that could have been improved a lot if the model could
    use the architecture of the branches.</p>
          <p>To do this, I wanted to stick with a simple solution that would
    integrate well with the initial model and wouldn’t require too many
    changes. The idea I implemented is therefore very simple. Instead of
    having only one CHM raster, I would have multiple layers, each
    focusing on a different height interval. There are many ways to do
    this, but due to a lack of time, I only really tried what seemed to
    me the easiest and most straightforward way to do it, which consists
    in removing all the points above a certain height threshold, and
    compute the CHM with the points that are left. When doing this for
    multiple height thresholds, we get an interesting view of what the
    point cloud looks like at multiple levels, which gives a lot more
    information about the organization of the point cloud. Another way
    to do this, which is used in the third method of this paper
    (<xref alt="Eysn et al. 2015" rid="ref-lidar_benchmark-nb-article" ref-type="bibr">Eysn
    et al. 2015</xref>), would be instead to use the previous CHM by
    removing all the points that are in the interval between the CHM
    height and 0.5 m below, before computing an additional layer. It
    could be interesting to see if this method works better than
    dropping the points at pre-determined heights.</p>
        </sec>
      </sec>
      <sec id="sec-dataset-nb-article">
        <title>Dataset creation</title>
        <p>The highest resolution of the CHM which keeps a high enough quality
  depends entirely on the density of the point cloud. Also, depending on
  the season when the point cloud is acquired, using a CHM might imply
  throwing away the majority of the information contained in the point
  cloud.</p>
        <sec id="definition-and-content-nb-article">
          <title>Definition and content</title>
          <p>As explained in the section
    <xref alt="Section 1.2.1" rid="sec-sota-datasets-requirements-nb-article">Section 1.2.1</xref>,
    the main requirements of the dataset that I wanted to create were to
    contain at the same time LiDAR data, RGB data and CIR data, with
    simple bounding box annotations for all trees. And as explained in
    <xref alt="Section 2.2" rid="sec-obj-covered_trees-nb-article">Section 2.2</xref>,
    all trees means also annotating trees that are partially or
    completely covered by other trees.</p>
          <p>Then, to make the most out of the point cloud resolution and the
    RGB images resolution, I decided to use a CHM resolution of 8 cm,
    which is also the resolution of the RGB images. However, the
    resolution of CIR images is 25 cm, which made it less optimal, but
    still usable.</p>
          <p>To be able to get results even with a small dataset, I decided to
    focus on one specific area, to limit the diversity of trees and
    environments to something that could hopefully still be learnt with
    a small dataset. Therefore, the whole dataset is currently inside of
    a 1 km × 1 km square around Geodan office, in Amsterdam. It contains
    2726 annotated trees spread over 241 images of size 640 px × 640 px
    i.e. 51.2 m × 51.2 m. All tree annotations have at least a bounding
    box, and some of them have a more accurate polygon representing the
    shape of the crown. There are four classes, which I will detail in
    the next section
    <xref alt="Section 3.2" rid="sec-dataset-challenges-nb-article">Section 3.2</xref>,
    and each tree belongs to one class.</p>
          <p>Annotating all these trees took me about 100 hours, with a very
    high variation of the time spent on each tree depending on the
    complexity of the area.</p>
        </sec>
        <sec id="sec-dataset-challenges-nb-article">
          <title>Challenges and solutions</title>
          <p>The creation of this dataset raised a number of challenges. The
    first one was the interval of time between the acquisition of the
    different types of data. While the point cloud data dated from 2020,
    the RGB images were acquired in 2023. It would have been possible to
    use images from 2021 or 2022 with the same resolution, but the
    quality of the 2023 images was much better. Consequently, there were
    a certain amount of changes regarding trees between these two
    periods of acquisition. Some large trees were cut off, while small
    trees were planted, sometimes even at the position of old trees that
    were previously cut off in the same time frame. For this reason, a
    non negligible number of trees were either present only in the point
    cloud, or only in the images. To try to handle this situation, I
    created two new class labels corresponding to these situation. This
    amounted up to 4 class labels:</p>
          <list list-type="bullet">
            <list-item>
              <p>“Tree”: trees which are visible in the point cloud and the
        images</p>
            </list-item>
            <list-item>
              <p>“Tree_LiDAR”: trees which are visible in the point cloud only
        but would be visible in the images if they had been there during
        the acquisition</p>
            </list-item>
            <list-item>
              <p>“Tree_RGB”: trees which are visible in the images only but
        would be visible in the point cloud if they had been there
        during the acquisition</p>
            </list-item>
            <list-item>
              <p>“Tree_covered”: trees that are visible in the point cloud
        only because they are covered by other trees.</p>
            </list-item>
          </list>
          <p>All these labels theoretically correspond to situations that have
    no intersection, even though it is more complicated in practice.</p>
          <p>The next challenge was the misalignment of images and point
    cloud. This misalignment comes from the images not being perfectly
    orthonormal. Point clouds don’t have this problem, because the data
    is acquired and represented in 3D, but images have to be projected
    to a 2D plane after being acquired with an angle that is not
    perfectly orthogonal to the plane. Despite the post-processing that
    was surely performed on the images, they are therefore not perfect,
    and there is a shift between the positions of each object in the
    point cloud and in the images. This shift cannot really be solved,
    because it depends on the position. Because of this misalignment, a
    choice had to be made as to where tree annotations should be placed,
    using either the point clouds or the RGB images. I chose to the RGB
    images as it is simpler to visualize and annotate, but there was not
    really a perfect choice.</p>
          <p>Finally, the last challenge comes from the definition of what we
    consider as a tree and what we don’t. There are two main
    sub-problems. The first one comes from the threshold to set between
    bushes and trees. Large bushes can be much larger than small trees,
    and sometimes have a similar shape. Therefore, it is hard to keep
    coherent rules when annotating them. The second sub-problem comes
    from multi-stemmed and close trees. It can be very difficult to see,
    even with the point cloud, if a there is only one tree with two or
    more trunks dividing at the bottom, or multiple trees which are
    simply close to one another. (Un)fortunately I know that I was not
    the only one to face this problem because it was also mentioned in
    another paper
    (<xref alt="B. G. Weinstein et al. 2019" rid="ref-DeepForestBefore-nb-article" ref-type="bibr">B.
    G. Weinstein et al. 2019</xref>). In the end, it was just an
    unsolvable problem for which the most important was to remain
    consistent in the whole dataset.</p>
        </sec>
        <sec id="augmentation-methods-nb-article">
          <title>Augmentation methods</title>
          <p>Dataset augmentation methods are in the middle between dataset
    creation and deep learning model training, because they are a way to
    enhance the dataset but depend on the objective for which the model
    is trained. Their importance is inversely proportional with the size
    of the dataset, which made them very important for my small
    dataset.</p>
          <p>As it was already explained in
    <xref alt="Section 1.2.4" rid="sec-sota-dataset-augment-nb-article">Section 1.2.4</xref>,
    I used Albumentations
    (<xref alt="Buslaev et al. 2020" rid="ref-albumentations-nb-article" ref-type="bibr">Buslaev
    et al. 2020</xref>) to apply two types of augmentations: pixel-level
    and spatial-level.</p>
          <p>Spatial-level augmentations had to be in the exact same way to
    the whole dataset, to maintain the spatial coherence between RGB
    images, CIR images and the CHM layers. I used three different
    spatial transformations, applied with random parameters. The first
    one chooses one of the eight possible images we can get when
    flipping and rotating the image by angles that are multiples of 90°.
    The second one adds a perspective effect to the images. The third
    one adds a small distortion to the image.</p>
          <p>On the contrary, Pixel-level augmentations must be applied
    differently to RGB images and CHM layers because they represent
    different kinds of data, so the values of the pixels do not have the
    same meaning. In practice, a lot of transformations were conceived
    to reproduce camera effects on RGB images or to shift the color
    spectrum. Among others, I used random modifications of the
    brightness, the gamma value and added noise and a blurring effect
    randomly to RGB images. For both types of data, a channel dropout is
    also randomly applied, leaving a random number of channels and
    removing the others. A better way to augment the CHM data would have
    been to apply random displacements and deletions of points in the
    point cloud, before computing the CHM layers. However, these
    operations are too costly to be integrated in the training pipeline
    without consequently increasing the training time, so this idea was
    discarded.</p>
        </sec>
      </sec>
      <sec id="model-and-training-nb-article">
        <title>Model and training</title>
        <p>The deep learning model that is used is based on AMF GD YOLOv8, the
  model proposed in this paper
  (<xref alt="Zhong et al. 2024" rid="ref-amf_gd_yolov8-nb-article" ref-type="bibr">Zhong
  et al. 2024</xref>).</p>
        <sec id="model-architecture-nb-article">
          <title>Model architecture</title>
          <p>The architecture of the model is conceptually simple. The model
    takes two inputs in the form of two rasters with the same height and
    width. The two inputs are processed using the backbone of the YOLOv8
    model
    (<xref alt="Redmon et al. 2016" rid="ref-yolo-nb-article" ref-type="bibr">Redmon
    et al. 2016</xref>) to extract features at different scales. Then
    Attention Multi-level Fusion (AMF) layers are used to fuse the
    features of the two inputs at each scale level. Then, a
    Gather-and-Distribute (GD) mechanism is used to propagate
    information between the different scales. This mechanism fuses the
    features from all scales before redistributing them to the features,
    two times in a row. Finally, the features of the three smallest
    scales are fed into detection layers responsible for extracting
    bounding boxes and assigning confidence scores and class
    probabilities to them.</p>
          <p>In practical terms, the input rasters have a shape of
    <inline-formula><alternatives><tex-math><![CDATA[640 \times 640 \times c_{\text{RGB}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">RGB</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>
    and <inline-formula><alternatives><tex-math><![CDATA[640 \times 640 \times c_{\text{CHM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">CHM</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
    where <inline-formula><alternatives><tex-math><![CDATA[c_{\text{RGB}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">RGB</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
    is equal to 6 when using RGB and CIR images, and 3 when using only
    one of them, and <inline-formula><alternatives><tex-math><![CDATA[c_{\text{CHM}}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">CHM</mml:mtext></mml:msub></mml:math></alternatives></inline-formula>
    is the number of CHM layers that we decide to use for the model.
    Since the resolution that is used is 0.08 m, this means that each
    image spans over 51.2 m.</p>
          <p>The only real modification that I made to the architecture
    compared to the initial paper is adding any number of channels in
    the CHM input, while we had <inline-formula><alternatives><tex-math><![CDATA[c_{\text{CHM}} = 1]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>c</mml:mi><mml:mtext mathvariant="normal">CHM</mml:mtext></mml:msub><mml:mo>=</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
    originally. Using CIR images in addition to RGB images is also new,
    but this is a less important modification.</p>
        </sec>
        <sec id="training-pipeline-nb-article">
          <title>Training pipeline</title>
          <p>The training pipeline consists of three steps. First, the data is
    pre-processed to create the inputs to feed into the model. Then, the
    training loop runs until the end condition is reached. Finally, the
    final model is evaluated on all the datasets.</p>
          <sec id="data-preprocessing-nb-article">
            <title>Data preprocessing</title>
            <p>Data pre-processing is quite straightforward. The first step is
      to divide the dataset into a grid of
      <inline-formula><alternatives><tex-math><![CDATA[640 \times 640]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      tiles. Then, all these tiles are placed into one of the training,
      validation and test sets.</p>
            <p>As for RGB and CIR images, preprocessing only contains two
      simple steps: tiling the large images into small
      <inline-formula><alternatives><tex-math><![CDATA[640 \times 640]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>640</mml:mn><mml:mo>×</mml:mo><mml:mn>640</mml:mn></mml:mrow></mml:math></alternatives></inline-formula>
      images, and normalizing all images along each channel. When both
      data sources are used, RGB and CIR images are also merged into
      images with 6 channels, which will be the input of the model.</p>
            <p>As for CHM layers, there are more steps. The first step is to
      compute a sort of flattened point cloud, by computing the DTM,
      which represents the height of the ground, and removing this
      height to the point cloud. Then, for each CHM layer, if the height
      interval is <inline-formula><alternatives><tex-math><![CDATA[[z_\text{bot}, z_\text{top}]]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mo stretchy="true" form="prefix">[</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">bot</mml:mtext></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">top</mml:mtext></mml:msub><mml:mo stretchy="true" form="postfix">]</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>,
      we first extract all the points which have a height
      <inline-formula><alternatives><tex-math><![CDATA[h]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>h</mml:mi></mml:math></alternatives></inline-formula>
      such that <inline-formula><alternatives><tex-math><![CDATA[z_\text{bot} \leq h \leq z_\text{top}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">bot</mml:mtext></mml:msub><mml:mo>≤</mml:mo><mml:mi>h</mml:mi><mml:mo>≤</mml:mo><mml:msub><mml:mi>z</mml:mi><mml:mtext mathvariant="normal">top</mml:mtext></mml:msub></mml:mrow></mml:math></alternatives></inline-formula>,
      and we then compute the DSM for this smaller point cloud. Since
      the ground height was already removed from the point cloud, this
      DSM is the CHM. Then, all the layers are merged into one raster
      with multiple channels and we normalize the whole raster with the
      average and the standard deviation over all channels. Finally, we
      can simply tile these rasters exactly like RGB and CIR images,
      which gives us the inputs of the model.</p>
            <p>All these operations are conceptually simple, but they can be
      computationally expensive. Therefore, I had to put a certain
      effort into accelerating with different methods. First, I made
      sure to save the most important and generic elements to avoid
      useless computations every time the model is trained again,
      without saturating the memory. Then, I also implemented
      multi-threading for every possible step to improve the raw speed
      of preprocessing. Finally, performance is also the reason why
      normalization if performed during preprocessing instead of during
      the initialization of the data in the training loop.</p>
          </sec>
          <sec id="training-loop-nb-article">
            <title>Training loop</title>
            <p>The training loop is very generic, so I will only mention the
      most interesting parts. First, we use an Adam optimizer and a
      simple learning rate scheduler with a multiplier at each epoch i
      which is <inline-formula><alternatives><tex-math><![CDATA[1/\sqrt{i+2}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mn>1</mml:mn><mml:mi>/</mml:mi><mml:msqrt><mml:mrow><mml:mi>i</mml:mi><mml:mo>+</mml:mo><mml:mn>2</mml:mn></mml:mrow></mml:msqrt></mml:mrow></mml:math></alternatives></inline-formula>.</p>
            <p>Then, since the batch size cannot be very large because of the
      space required by all the images, there is the possibility to
      perform gradient accumulation, which means that backward
      propagation won’t be performed with each batch, but instead every
      two or more batches. The idea behind this is to add more stability
      to the training, since back-propagating on only a few images is
      prone to overfitting on a set of examples which are not
      representative of the whole dataset.</p>
            <p>As for the criterion to stop the training session, we use the
      loss on the validation set. Once this loss didn’t improve during
      50 iterations over the whole dataset, we stop and keep the model
      that had the best validation loss.</p>
            <p>Besides these details, the training loop is very generic. We
      loop over the entire training set with batches to compute the loss
      and perform gradient back-propagation,. Then we compute the loss
      on the validation set and store this loss as the metric that
      decides when to stop.</p>
          </sec>
          <sec id="output-postprocessing-nb-article">
            <title>Output postprocessing</title>
            <p>Regarding postprocessing of the output of the model, there a
      few things to mention. First, the model outputs a lot of bounding
      boxes, which have to be cleaned using two criteria. The first
      criterion is the confidence score. We can just set a threshold
      below which bounding boxes are discarded. The second criterion is
      the intersection over union (IoU) with other bounding boxes. IoU
      is a metrics used to quantify how similar two bounding boxes are.
      It is a value between 0 and 1, which formula is:</p>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
      \text{IoU}(A, B) = \frac{\text{area}(A \cap B)}{\text{area}(A \cup B)}
      ]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mrow>
                      <mml:mtext mathvariant="normal">IoU</mml:mtext>
                      <mml:mrow>
                        <mml:mo stretchy="true" form="prefix">(</mml:mo>
                        <mml:mi>A</mml:mi>
                        <mml:mo>,</mml:mo>
                        <mml:mi>B</mml:mi>
                        <mml:mo stretchy="true" form="postfix">)</mml:mo>
                      </mml:mrow>
                      <mml:mo>=</mml:mo>
                      <mml:mfrac>
                        <mml:mrow>
                          <mml:mtext mathvariant="normal">area</mml:mtext>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>A</mml:mi>
                            <mml:mo>∩</mml:mo>
                            <mml:mi>B</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                        <mml:mrow>
                          <mml:mtext mathvariant="normal">area</mml:mtext>
                          <mml:mrow>
                            <mml:mo stretchy="true" form="prefix">(</mml:mo>
                            <mml:mi>A</mml:mi>
                            <mml:mo>∪</mml:mo>
                            <mml:mi>B</mml:mi>
                            <mml:mo stretchy="true" form="postfix">)</mml:mo>
                          </mml:mrow>
                        </mml:mrow>
                      </mml:mfrac>
                    </mml:mrow>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <p>Using this metrics, we can detect bounding boxes which are too
      similar to each other, and simply keep the bounding box with the
      highest confidence score when two bounding boxes have an IoU
      larger than a certain threshold.</p>
            <p>For the evaluation, the process is a little different, because
      we only perform the clean up relying on IoU, and keep all other
      bounding boxes. The main metric that we compute is called sortedAP
      (<xref alt="Chen et al. 2023" rid="ref-sortedAP-nb-article" ref-type="bibr">Chen
      et al. 2023</xref>), which is an evolution of the mean (point)
      average precision (mAP). mAP is defined as follows:</p>
            <p>
              <disp-formula>
                <alternatives>
                  <tex-math><![CDATA[
      \begin{array}{rcl}
      \text{mAP} & = & \frac{1}{N} \sum\limits_{t\in T} \text{AP}_t \\
      \text{AP}_t & = & \frac{{TP}_t}{{TP}_t + {FP}_t + {FN}_t}
      \end{array}
      ]]></tex-math>
                  <mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="block">
                    <mml:mtable>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:mtext mathvariant="normal">mAP</mml:mtext>
                        </mml:mtd>
                        <mml:mtd columnalign="center" style="text-align: center">
                          <mml:mo>=</mml:mo>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mfrac>
                            <mml:mn>1</mml:mn>
                            <mml:mi>N</mml:mi>
                          </mml:mfrac>
                          <mml:munder>
                            <mml:mo>∑</mml:mo>
                            <mml:mrow>
                              <mml:mi>t</mml:mi>
                              <mml:mo>∈</mml:mo>
                              <mml:mi>T</mml:mi>
                            </mml:mrow>
                          </mml:munder>
                          <mml:msub>
                            <mml:mtext mathvariant="normal">AP</mml:mtext>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                      </mml:mtr>
                      <mml:mtr>
                        <mml:mtd columnalign="right" style="text-align: right">
                          <mml:msub>
                            <mml:mtext mathvariant="normal">AP</mml:mtext>
                            <mml:mi>t</mml:mi>
                          </mml:msub>
                        </mml:mtd>
                        <mml:mtd columnalign="center" style="text-align: center">
                          <mml:mo>=</mml:mo>
                        </mml:mtd>
                        <mml:mtd columnalign="left" style="text-align: left">
                          <mml:mfrac>
                            <mml:msub>
                              <mml:mrow>
                                <mml:mi>T</mml:mi>
                                <mml:mi>P</mml:mi>
                              </mml:mrow>
                              <mml:mi>t</mml:mi>
                            </mml:msub>
                            <mml:mrow>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>T</mml:mi>
                                  <mml:mi>P</mml:mi>
                                </mml:mrow>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>F</mml:mi>
                                  <mml:mi>P</mml:mi>
                                </mml:mrow>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                              <mml:mo>+</mml:mo>
                              <mml:msub>
                                <mml:mrow>
                                  <mml:mi>F</mml:mi>
                                  <mml:mi>N</mml:mi>
                                </mml:mrow>
                                <mml:mi>t</mml:mi>
                              </mml:msub>
                            </mml:mrow>
                          </mml:mfrac>
                        </mml:mtd>
                      </mml:mtr>
                    </mml:mtable>
                  </mml:math>
                </alternatives>
              </disp-formula>
            </p>
            <p>where <inline-formula><alternatives><tex-math><![CDATA[T=\{t_1, t_2, \dots, t_N\}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mo>=</mml:mo><mml:mo stretchy="false" form="prefix">{</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>1</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mn>2</mml:mn></mml:msub><mml:mo>,</mml:mo><mml:mi>…</mml:mi><mml:mo>,</mml:mo><mml:msub><mml:mi>t</mml:mi><mml:mi>N</mml:mi></mml:msub><mml:mo stretchy="false" form="postfix">}</mml:mo></mml:mrow></mml:math></alternatives></inline-formula>
      is a list of IoU threshold values, <inline-formula><alternatives><tex-math><![CDATA[{TP}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      are the true positives when the the IoU threshold is
      <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives><tex-math><![CDATA[{FP}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      are false positives and <inline-formula><alternatives><tex-math><![CDATA[{FN}_t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:msub><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow><mml:mi>t</mml:mi></mml:msub></mml:math></alternatives></inline-formula>
      are false negatives. The reason why <inline-formula><alternatives><tex-math><![CDATA[TP]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>T</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>,
      <inline-formula><alternatives><tex-math><![CDATA[FP]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>P</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      and <inline-formula><alternatives><tex-math><![CDATA[FN]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mrow><mml:mi>F</mml:mi><mml:mi>N</mml:mi></mml:mrow></mml:math></alternatives></inline-formula>
      depend on <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>
      is that a bounding box is considered to be true if its IoU with
      one of the ground-truth bounding boxes is larger than
      <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>.</p>
            <p>sortedAP is an improvement over this method because there is no
      need to select a list of IoU threshold values. Predicted bounding
      boxes are sorted according to their confidence score which allows
      to compute <inline-formula><alternatives><tex-math><![CDATA[\text{AP}]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mtext mathvariant="normal">AP</mml:mtext></mml:math></alternatives></inline-formula>
      incrementally for any value of <inline-formula><alternatives><tex-math><![CDATA[t]]></tex-math><mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" display="inline"><mml:mi>t</mml:mi></mml:math></alternatives></inline-formula>.
      Then, the area of the curve of the AP with respect to the IoU
      threshold is used as a metric, between 0 and 1, 1 being the best
      possible value.</p>
          </sec>
        </sec>
      </sec>
      <sec id="results-nb-article">
        <title>Results</title>
        <p>In this section are the results of the experiments performed with
  the model and the dataset presented before.</p>
        <sec id="training-parameters-nb-article">
          <title>Training parameters</title>
          <p>The first experiment was a simple test over the different
    parameters regarding the training loop. There were two goals to this
    experiment. The first one was to find the best training parameters
    for the next experiments. The second one was to see if randomly
    dropping one of the inputs of the model (either RGB/CIR or CHM)
    could help the model by pushing it to learn to make the best out of
    the two types of data.</p>
          <p>The different parameters that are tested here are:</p>
          <list list-type="bullet">
            <list-item>
              <p>“Learn. rate”: the initial learning rate.</p>
            </list-item>
            <list-item>
              <p>“Prob. drop”: the probability to drop either RGB/CIR or CHM.
        The probability is the same for the two types, which means that
        if the displayed value is 0.1, then all data will be used 80% of
        the time, while only RGB/CIR and only CHM both happen 10% of the
        time.</p>
            </list-item>
            <list-item>
              <p>“Accum. count”: the accumulation count, which means the
        amount of training data to process and compute the loss on
        before performing gradient back-propagation.</p>
            </list-item>
          </list>
          <p>As you can see on
    <xref alt="Figure 1" rid="fig-training-parameters-experiments-nb-article">Figure 1</xref>,
    sortedAP reaches at best values just above 0.3. The reason why the
    column name is “Best sortedAP” is due to the dataset being too
    small. Since the dataset is small, the training process overfits
    quickly, and the model doesn’t have enough training steps to have
    confidence scores which reach very high values. As a consequence, it
    is difficult to know beforehand which confidence threshold to
    choose. Therefore, the sortedAP metric is computed over several
    different confidence thresholds, and the one that gives the best
    value of sortedAP is kept.</p>
          <p>With this experiment, we can see that a learning rate of 0.01
    seems to make the training too much unstable, while 0.001 doesn’t
    give very high score. Then, we can also see how unstable the
    training process is in general, which comes mostly from the dataset
    being too small. However, a learning rate between 0.0025 and 0.006
    seems to give the most stable results, when the drop probability is
    0. This seems to show that the idea of randomly dropping one of the
    two inputs doesn’t really help the model to learn.</p>
          <sec id="cell-fig-training-parameters-experiments-nb-article" specific-use="notebook-content">
            <code language="python">import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("data/training_params_experiment_all.csv")
df = df[df["Data used for evaluation"] == "RGB, CIR and CHM"]
df["Prob. drop"] = df["proba_drop_chm"]
df.rename(columns = {
    "accumulate": "Accum. count",
    "Data used for evaluation": "Evaluation data",
    "lr": "Learn. rate",
    "repartition_name": "Exp. name"
  },
  inplace=True)
sns.set_style("ticks", {"axes.grid": True})
sns.catplot(
    data=df,
    kind="swarm",
    x="Accum. count",
    y="Best sortedAP",
    hue="Exp. name",
    hue_order=["exp0", "exp1", "exp2", "exp3", "exp4"],
    col="Learn. rate",
    row="Prob. drop",
    margin_titles=True,
    height=2,
    aspect=1,
    palette="colorblind"
)
plt.show()</code>
            <boxed-text>
              <fig id="fig-training-parameters-experiments-nb-article">
                <caption>
                  <p>Figure 1: Results with different training parameters
        for all experiments</p>
                </caption>
                <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-training-parameters-experiments-output-1.png"/>
              </fig>
            </boxed-text>
          </sec>
          <sec id="cell-6128a698-nb-article" specific-use="notebook-content">
            <p>In the next graph
    (<xref alt="Figure 2" rid="fig-training-parameters-data-nb-article">Figure 2</xref>),
    we can see more results for the same experiments. Here, the results
    are colored according to the data that we use to evaluate the model.
    In blue, we see the value of sortedAP when we evaluate the model
    with the CHM layers data and dummy zero arrays as RGB/CIR data.
    These dummy arrays are also those that are used as input when one of
    the channel is dropped during training, when we have a drop
    probability larger than 0. Some interesting patterns appear in some
    of the cells in this plot. Firstly, it looks like randomly dropping
    one of the two inputs with the same probability has a much larger
    influence over the results using RGB/CIR than CHM. While CHM gives
    better results than RGB/CIR when always training using everything,
    RGB/CIR seems to perform better alone when also trained alone, even
    outperforming the combination of both inputs in certain cases.</p>
          </sec>
          <sec id="cell-fig-training-parameters-data-nb-article" specific-use="notebook-content">
            <code language="python">import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("data/training_params_experiment_all.csv")
df.sort_values(
    by=["proba_drop_chm", "Data used for evaluation"],
    inplace=True,
)
df["Prob. drop"] = df["proba_drop_chm"]
df.rename(columns = {
    "accumulate": "Accum. count",
    "Data used for evaluation": "Evaluation data",
    "lr": "Learn. rate"
  },
  inplace=True)
sns.set_style("ticks", {"axes.grid": True})
sns.catplot(
    data=df,
    kind="swarm",
    x="Accum. count",
    y="Best sortedAP",
    hue="Evaluation data",
    col="Learn. rate",
    row="Prob. drop",
    margin_titles=True,
    height=1.7,
    aspect=1.2,
    palette="colorblind"
)
plt.show()</code>
            <boxed-text>
              <preformat>/home/alexandre/anaconda3/envs/tree-segment/lib/python3.11/site-packages/seaborn/categorical.py:3399: UserWarning:

8.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.

/home/alexandre/anaconda3/envs/tree-segment/lib/python3.11/site-packages/seaborn/categorical.py:3399: UserWarning:

6.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.

/home/alexandre/anaconda3/envs/tree-segment/lib/python3.11/site-packages/seaborn/categorical.py:3399: UserWarning:

20.0% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.

/home/alexandre/anaconda3/envs/tree-segment/lib/python3.11/site-packages/seaborn/categorical.py:3399: UserWarning:

33.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.

/home/alexandre/anaconda3/envs/tree-segment/lib/python3.11/site-packages/seaborn/categorical.py:3399: UserWarning:

13.3% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.

/home/alexandre/anaconda3/envs/tree-segment/lib/python3.11/site-packages/seaborn/categorical.py:3399: UserWarning:

16.7% of the points cannot be placed; you may want to decrease the size of the markers or use stripplot.
      </preformat>
            </boxed-text>
            <boxed-text>
              <fig id="fig-training-parameters-data-nb-article">
                <caption>
                  <p>Figure 2: Results with different training parameters
        for all evaluation data setups</p>
                </caption>
                <graphic mimetype="image" mime-subtype="png" xlink:href="index_files/figure-jats/fig-training-parameters-data-output-2.png"/>
              </fig>
            </boxed-text>
          </sec>
          <sec id="cell-12bc24d4-nb-article" specific-use="notebook-content">
            <p>From the results of this experiment, I decided to pick the
    following parameters for the next experiments:</p>
            <list list-type="bullet">
              <list-item>
                <p>Initial learning rate: 0.004</p>
              </list-item>
              <list-item>
                <p>Drop probability: 0</p>
              </list-item>
              <list-item>
                <p>Accumulation count: 10</p>
              </list-item>
            </list>
          </sec>
          <sec id="data-used-nb-article">
            <title>Data used</title>
          </sec>
          <sec id="chm-layers-nb-article">
            <title>CHM layers</title>
          </sec>
          <sec id="hard-trees-nb-article">
            <title>Hard trees</title>
          </sec>
        </sec>
        <sec id="discussion-nb-article">
          <title>Discussion</title>
          <sec id="dataset-nb-article">
            <title>Dataset</title>
            <list list-type="bullet">
              <list-item>
                <p>DeepForest: A Python package for RGB deep learning tree crown
        delineation
        (<xref alt="B. G. Weinstein et al. 2020" rid="ref-DeepForest-nb-article" ref-type="bibr">B.
        G. Weinstein et al. 2020</xref>): uses only RGB data to detect
        trees, but uses LiDAR to create millions of annotations of
        moderate quality to pre-train the model, before using around
        10,000 hand-annotations to finalize and specialize the training
        on a certain area.</p>
              </list-item>
            </list>
          </sec>
          <sec id="combination-of-data-types-nb-article">
            <title>Combination of data types</title>
          </sec>
        </sec>
        <sec id="conclusion-nb-article">
          <title>Conclusion</title>
          <p>Blablabla</p>
        </sec>
      </sec>
    </body>
    <back>
      <ref-list>
        <title/>
        <ref id="ref-FoMo-Bench-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Bountos</surname>
                <given-names>Nikolaos Ioannis</given-names>
              </name>
              <name>
                <surname>Ouaknine</surname>
                <given-names>Arthur</given-names>
              </name>
              <name>
                <surname>Rolnick</surname>
                <given-names>David</given-names>
              </name>
            </person-group>
            <article-title>FoMo-bench: A multi-modal, multi-scale and multi-task forest monitoring benchmark for remote sensing foundation models</article-title>
            <source>arXiv preprint arXiv:2312.10114</source>
            <year iso-8601-date="2023">2023</year>
            <uri>https://arxiv.org/abs/2312.10114</uri>
          </element-citation>
        </ref>
        <ref id="ref-OpenForest-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <name>
                <surname>Ouaknine</surname>
                <given-names>Arthur</given-names>
              </name>
              <name>
                <surname>Kattenborn</surname>
                <given-names>Teja</given-names>
              </name>
              <name>
                <surname>Laliberté</surname>
                <given-names>Etienne</given-names>
              </name>
              <name>
                <surname>Rolnick</surname>
                <given-names>David</given-names>
              </name>
            </person-group>
            <article-title>OpenForest: A data catalogue for machine learning in forest monitoring</article-title>
            <year iso-8601-date="2023">2023</year>
            <uri>https://arxiv.org/abs/2311.00277</uri>
          </element-citation>
        </ref>
        <ref id="ref-DeepForestBefore-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Weinstein</surname>
                <given-names>Ben G.</given-names>
              </name>
              <name>
                <surname>Marconi</surname>
                <given-names>Sergio</given-names>
              </name>
              <name>
                <surname>Bohlman</surname>
                <given-names>Stephanie</given-names>
              </name>
              <name>
                <surname>Zare</surname>
                <given-names>Alina</given-names>
              </name>
              <name>
                <surname>White</surname>
                <given-names>Ethan</given-names>
              </name>
            </person-group>
            <article-title>Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks</article-title>
            <source>Remote Sensing</source>
            <year iso-8601-date="2019">2019</year>
            <volume>11</volume>
            <issue>11</issue>
            <issn>2072-4292</issn>
            <uri>https://www.mdpi.com/2072-4292/11/11/1309</uri>
            <pub-id pub-id-type="doi">10.3390/rs11111309</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-ReforesTree-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <name>
                <surname>Reiersen</surname>
                <given-names>Gyri</given-names>
              </name>
              <name>
                <surname>Dao</surname>
                <given-names>David</given-names>
              </name>
              <name>
                <surname>Lütjens</surname>
                <given-names>Björn</given-names>
              </name>
              <name>
                <surname>Klemmer</surname>
                <given-names>Konstantin</given-names>
              </name>
              <name>
                <surname>Amara</surname>
                <given-names>Kenza</given-names>
              </name>
              <name>
                <surname>Steinegger</surname>
                <given-names>Attila</given-names>
              </name>
              <name>
                <surname>Zhang</surname>
                <given-names>Ce</given-names>
              </name>
              <name>
                <surname>Zhu</surname>
                <given-names>Xiaoxiang</given-names>
              </name>
            </person-group>
            <article-title>ReforesTree: A dataset for estimating tropical forest carbon stock with deep learning and aerial imagery</article-title>
            <year iso-8601-date="2022">2022</year>
            <uri>https://arxiv.org/abs/2201.11192</uri>
          </element-citation>
        </ref>
        <ref id="ref-FOR-instance-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <name>
                <surname>Puliti</surname>
                <given-names>Stefano</given-names>
              </name>
              <name>
                <surname>Pearse</surname>
                <given-names>Grant</given-names>
              </name>
              <name>
                <surname>Surový</surname>
                <given-names>Peter</given-names>
              </name>
              <name>
                <surname>Wallace</surname>
                <given-names>Luke</given-names>
              </name>
              <name>
                <surname>Hollaus</surname>
                <given-names>Markus</given-names>
              </name>
              <name>
                <surname>Wielgosz</surname>
                <given-names>Maciej</given-names>
              </name>
              <name>
                <surname>Astrup</surname>
                <given-names>Rasmus</given-names>
              </name>
            </person-group>
            <article-title>FOR-instance: A UAV laser scanning benchmark dataset for semantic and instance segmentation of individual trees</article-title>
            <year iso-8601-date="2023">2023</year>
            <uri>https://arxiv.org/abs/2309.01279</uri>
          </element-citation>
        </ref>
        <ref id="ref-MDAS-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Hu</surname>
                <given-names>J.</given-names>
              </name>
              <name>
                <surname>Liu</surname>
                <given-names>R.</given-names>
              </name>
              <name>
                <surname>Hong</surname>
                <given-names>D.</given-names>
              </name>
              <name>
                <surname>Camero</surname>
                <given-names>A.</given-names>
              </name>
              <name>
                <surname>Yao</surname>
                <given-names>J.</given-names>
              </name>
              <name>
                <surname>Schneider</surname>
                <given-names>M.</given-names>
              </name>
              <name>
                <surname>Kurz</surname>
                <given-names>F.</given-names>
              </name>
              <name>
                <surname>Segl</surname>
                <given-names>K.</given-names>
              </name>
              <name>
                <surname>Zhu</surname>
                <given-names>X. X.</given-names>
              </name>
            </person-group>
            <article-title>MDAS: A new multimodal benchmark dataset for remote sensing</article-title>
            <source>Earth System Science Data</source>
            <year iso-8601-date="2023">2023</year>
            <volume>15</volume>
            <issue>1</issue>
            <uri>https://essd.copernicus.org/articles/15/113/2023/</uri>
            <pub-id pub-id-type="doi">10.5194/essd-15-113-2023</pub-id>
            <fpage>113</fpage>
            <lpage>131</lpage>
          </element-citation>
        </ref>
        <ref id="ref-TALLO-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Jucker</surname>
                <given-names>Tommaso</given-names>
              </name>
              <name>
                <surname>Fischer</surname>
                <given-names>Fabian Jörg</given-names>
              </name>
              <name>
                <surname>Chave</surname>
                <given-names>Jérôme</given-names>
              </name>
              <name>
                <surname>Coomes</surname>
                <given-names>David A.</given-names>
              </name>
              <name>
                <surname>Caspersen</surname>
                <given-names>John</given-names>
              </name>
              <name>
                <surname>Ali</surname>
                <given-names>Arshad</given-names>
              </name>
              <name>
                <surname>Loubota Panzou</surname>
                <given-names>Grace Jopaul</given-names>
              </name>
              <name>
                <surname>Feldpausch</surname>
                <given-names>Ted R.</given-names>
              </name>
              <name>
                <surname>Falster</surname>
                <given-names>Daniel</given-names>
              </name>
              <name>
                <surname>Usoltsev</surname>
                <given-names>Vladimir A.</given-names>
              </name>
              <name>
                <surname>Adu-Bredu</surname>
                <given-names>Stephen</given-names>
              </name>
              <name>
                <surname>Alves</surname>
                <given-names>Luciana F.</given-names>
              </name>
              <name>
                <surname>Aminpour</surname>
                <given-names>Mohammad</given-names>
              </name>
              <name>
                <surname>Angoboy</surname>
                <given-names>Ilondea B.</given-names>
              </name>
              <name>
                <surname>Anten</surname>
                <given-names>Niels P. R.</given-names>
              </name>
              <name>
                <surname>Antin</surname>
                <given-names>Cécile</given-names>
              </name>
              <name>
                <surname>Askari</surname>
                <given-names>Yousef</given-names>
              </name>
              <name>
                <surname>Muñoz</surname>
                <given-names>Rodrigo</given-names>
              </name>
              <name>
                <surname>Ayyappan</surname>
                <given-names>Narayanan</given-names>
              </name>
              <name>
                <surname>Balvanera</surname>
                <given-names>Patricia</given-names>
              </name>
              <name>
                <surname>Banin</surname>
                <given-names>Lindsay</given-names>
              </name>
              <name>
                <surname>Barbier</surname>
                <given-names>Nicolas</given-names>
              </name>
              <name>
                <surname>Battles</surname>
                <given-names>John J.</given-names>
              </name>
              <name>
                <surname>Beeckman</surname>
                <given-names>Hans</given-names>
              </name>
              <name>
                <surname>Bocko</surname>
                <given-names>Yannick E.</given-names>
              </name>
              <name>
                <surname>Bond-Lamberty</surname>
                <given-names>Ben</given-names>
              </name>
              <name>
                <surname>Bongers</surname>
                <given-names>Frans</given-names>
              </name>
              <name>
                <surname>Bowers</surname>
                <given-names>Samuel</given-names>
              </name>
              <name>
                <surname>Brade</surname>
                <given-names>Thomas</given-names>
              </name>
              <name>
                <surname>Breugel</surname>
                <given-names>Michiel van</given-names>
              </name>
              <name>
                <surname>Chantrain</surname>
                <given-names>Arthur</given-names>
              </name>
              <name>
                <surname>Chaudhary</surname>
                <given-names>Rajeev</given-names>
              </name>
              <name>
                <surname>Dai</surname>
                <given-names>Jingyu</given-names>
              </name>
              <name>
                <surname>Dalponte</surname>
                <given-names>Michele</given-names>
              </name>
              <name>
                <surname>Dimobe</surname>
                <given-names>Kangbéni</given-names>
              </name>
              <name>
                <surname>Domec</surname>
                <given-names>Jean-Christophe</given-names>
              </name>
              <name>
                <surname>Doucet</surname>
                <given-names>Jean-Louis</given-names>
              </name>
              <name>
                <surname>Duursma</surname>
                <given-names>Remko A.</given-names>
              </name>
              <name>
                <surname>Enríquez</surname>
                <given-names>Moisés</given-names>
              </name>
              <name>
                <surname>Ewijk</surname>
                <given-names>Karin Y. van</given-names>
              </name>
              <name>
                <surname>Farfán-Rios</surname>
                <given-names>William</given-names>
              </name>
              <name>
                <surname>Fayolle</surname>
                <given-names>Adeline</given-names>
              </name>
              <name>
                <surname>Forni</surname>
                <given-names>Eric</given-names>
              </name>
              <name>
                <surname>Forrester</surname>
                <given-names>David I.</given-names>
              </name>
              <name>
                <surname>Gilani</surname>
                <given-names>Hammad</given-names>
              </name>
              <name>
                <surname>Godlee</surname>
                <given-names>John L.</given-names>
              </name>
              <name>
                <surname>Gourlet-Fleury</surname>
                <given-names>Sylvie</given-names>
              </name>
              <name>
                <surname>Haeni</surname>
                <given-names>Matthias</given-names>
              </name>
              <name>
                <surname>Hall</surname>
                <given-names>Jefferson S.</given-names>
              </name>
              <name>
                <surname>He</surname>
                <given-names>Jie-Kun</given-names>
              </name>
              <name>
                <surname>Hemp</surname>
                <given-names>Andreas</given-names>
              </name>
              <name>
                <surname>Hernández-Stefanoni</surname>
                <given-names>José L.</given-names>
              </name>
              <name>
                <surname>Higgins</surname>
                <given-names>Steven I.</given-names>
              </name>
              <name>
                <surname>Holdaway</surname>
                <given-names>Robert J.</given-names>
              </name>
              <name>
                <surname>Hussain</surname>
                <given-names>Kiramat</given-names>
              </name>
              <name>
                <surname>Hutley</surname>
                <given-names>Lindsay B.</given-names>
              </name>
              <name>
                <surname>Ichie</surname>
                <given-names>Tomoaki</given-names>
              </name>
              <name>
                <surname>Iida</surname>
                <given-names>Yoshiko</given-names>
              </name>
              <name>
                <surname>Jiang</surname>
                <given-names>Hai-sheng</given-names>
              </name>
              <name>
                <surname>Joshi</surname>
                <given-names>Puspa Raj</given-names>
              </name>
              <name>
                <surname>Kaboli</surname>
                <given-names>Hasan</given-names>
              </name>
              <name>
                <surname>Larsary</surname>
                <given-names>Maryam Kazempour</given-names>
              </name>
              <name>
                <surname>Kenzo</surname>
                <given-names>Tanaka</given-names>
              </name>
              <name>
                <surname>Kloeppel</surname>
                <given-names>Brian D.</given-names>
              </name>
              <name>
                <surname>Kohyama</surname>
                <given-names>Takashi</given-names>
              </name>
              <name>
                <surname>Kunwar</surname>
                <given-names>Suwash</given-names>
              </name>
              <name>
                <surname>Kuyah</surname>
                <given-names>Shem</given-names>
              </name>
              <name>
                <surname>Kvasnica</surname>
                <given-names>Jakub</given-names>
              </name>
              <name>
                <surname>Lin</surname>
                <given-names>Siliang</given-names>
              </name>
              <name>
                <surname>Lines</surname>
                <given-names>Emily R.</given-names>
              </name>
              <name>
                <surname>Liu</surname>
                <given-names>Hongyan</given-names>
              </name>
              <name>
                <surname>Lorimer</surname>
                <given-names>Craig</given-names>
              </name>
              <name>
                <surname>Loumeto</surname>
                <given-names>Jean-Joël</given-names>
              </name>
              <name>
                <surname>Malhi</surname>
                <given-names>Yadvinder</given-names>
              </name>
              <name>
                <surname>Marshall</surname>
                <given-names>Peter L.</given-names>
              </name>
              <name>
                <surname>Mattsson</surname>
                <given-names>Eskil</given-names>
              </name>
              <name>
                <surname>Matula</surname>
                <given-names>Radim</given-names>
              </name>
              <name>
                <surname>Meave</surname>
                <given-names>Jorge A.</given-names>
              </name>
              <name>
                <surname>Mensah</surname>
                <given-names>Sylvanus</given-names>
              </name>
              <name>
                <surname>Mi</surname>
                <given-names>Xiangcheng</given-names>
              </name>
              <name>
                <surname>Momo</surname>
                <given-names>Stéphane</given-names>
              </name>
              <name>
                <surname>Moncrieff</surname>
                <given-names>Glenn R.</given-names>
              </name>
              <name>
                <surname>Mora</surname>
                <given-names>Francisco</given-names>
              </name>
              <name>
                <surname>Nissanka</surname>
                <given-names>Sarath P.</given-names>
              </name>
              <name>
                <surname>O’Hara</surname>
                <given-names>Kevin L.</given-names>
              </name>
              <name>
                <surname>Pearce</surname>
                <given-names>Steven</given-names>
              </name>
              <name>
                <surname>Pelissier</surname>
                <given-names>Raphaël</given-names>
              </name>
              <name>
                <surname>Peri</surname>
                <given-names>Pablo L.</given-names>
              </name>
              <name>
                <surname>Ploton</surname>
                <given-names>Pierre</given-names>
              </name>
              <name>
                <surname>Poorter</surname>
                <given-names>Lourens</given-names>
              </name>
              <name>
                <surname>Pour</surname>
                <given-names>Mohsen Javanmiri</given-names>
              </name>
              <name>
                <surname>Pourbabaei</surname>
                <given-names>Hassan</given-names>
              </name>
              <name>
                <surname>Dupuy-Rada</surname>
                <given-names>Juan Manuel</given-names>
              </name>
              <name>
                <surname>Ribeiro</surname>
                <given-names>Sabina C.</given-names>
              </name>
              <name>
                <surname>Ryan</surname>
                <given-names>Casey</given-names>
              </name>
              <name>
                <surname>Sanaei</surname>
                <given-names>Anvar</given-names>
              </name>
              <name>
                <surname>Sanger</surname>
                <given-names>Jennifer</given-names>
              </name>
              <name>
                <surname>Schlund</surname>
                <given-names>Michael</given-names>
              </name>
              <name>
                <surname>Sellan</surname>
                <given-names>Giacomo</given-names>
              </name>
              <name>
                <surname>Shenkin</surname>
                <given-names>Alexander</given-names>
              </name>
              <name>
                <surname>Sonké</surname>
                <given-names>Bonaventure</given-names>
              </name>
              <name>
                <surname>Sterck</surname>
                <given-names>Frank J.</given-names>
              </name>
              <name>
                <surname>Svátek</surname>
                <given-names>Martin</given-names>
              </name>
              <name>
                <surname>Takagi</surname>
                <given-names>Kentaro</given-names>
              </name>
              <name>
                <surname>Trugman</surname>
                <given-names>Anna T.</given-names>
              </name>
              <name>
                <surname>Ullah</surname>
                <given-names>Farman</given-names>
              </name>
              <name>
                <surname>Vadeboncoeur</surname>
                <given-names>Matthew A.</given-names>
              </name>
              <name>
                <surname>Valipour</surname>
                <given-names>Ahmad</given-names>
              </name>
              <name>
                <surname>Vanderwel</surname>
                <given-names>Mark C.</given-names>
              </name>
              <name>
                <surname>Vovides</surname>
                <given-names>Alejandra G.</given-names>
              </name>
              <name>
                <surname>Wang</surname>
                <given-names>Weiwei</given-names>
              </name>
              <name>
                <surname>Wang</surname>
                <given-names>Li-Qiu</given-names>
              </name>
              <name>
                <surname>Wirth</surname>
                <given-names>Christian</given-names>
              </name>
              <name>
                <surname>Woods</surname>
                <given-names>Murray</given-names>
              </name>
              <name>
                <surname>Xiang</surname>
                <given-names>Wenhua</given-names>
              </name>
              <name>
                <surname>Ximenes</surname>
                <given-names>Fabiano de Aquino</given-names>
              </name>
              <name>
                <surname>Xu</surname>
                <given-names>Yaozhan</given-names>
              </name>
              <name>
                <surname>Yamada</surname>
                <given-names>Toshihiro</given-names>
              </name>
              <name>
                <surname>Zavala</surname>
                <given-names>Miguel A.</given-names>
              </name>
            </person-group>
            <article-title>Tallo: A global tree allometry and crown architecture database</article-title>
            <source>Global Change Biology</source>
            <year iso-8601-date="2022">2022</year>
            <volume>28</volume>
            <issue>17</issue>
            <uri>https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.16302</uri>
            <pub-id pub-id-type="doi">10.1111/gcb.16302</pub-id>
            <fpage>5254</fpage>
            <lpage>5268</lpage>
          </element-citation>
        </ref>
        <ref id="ref-MillionTrees-nb-article">
          <element-citation publication-type="webpage">
            <person-group person-group-type="author">
              <name>
                <surname>Weinstein</surname>
                <given-names>Ben</given-names>
              </name>
            </person-group>
            <article-title>MillionTrees</article-title>
            <year iso-8601-date="2023">2023</year>
            <date-in-citation content-type="access-date">
              <year iso-8601-date="2024-07-08">2024</year>
              <month>07</month>
              <day>08</day>
            </date-in-citation>
            <uri>https://milliontrees.idtrees.org/</uri>
          </element-citation>
        </ref>
        <ref id="ref-WildForest3D-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <name>
                <surname>Kalinicheva</surname>
                <given-names>Ekaterina</given-names>
              </name>
              <name>
                <surname>Landrieu</surname>
                <given-names>Loic</given-names>
              </name>
              <name>
                <surname>Mallet</surname>
                <given-names>Clément</given-names>
              </name>
              <name>
                <surname>Chehata</surname>
                <given-names>Nesrine</given-names>
              </name>
            </person-group>
            <article-title>Multi-layer modeling of dense vegetation from aerial LiDAR scans</article-title>
            <year iso-8601-date="2022">2022</year>
            <uri>https://arxiv.org/abs/2204.11620</uri>
          </element-citation>
        </ref>
        <ref id="ref-sortedAP-nb-article">
          <element-citation publication-type="paper-conference">
            <person-group person-group-type="author">
              <name>
                <surname>Chen</surname>
                <given-names>Long</given-names>
              </name>
              <name>
                <surname>Wu</surname>
                <given-names>Yuli</given-names>
              </name>
              <name>
                <surname>Stegmaier</surname>
                <given-names>Johannes</given-names>
              </name>
              <name>
                <surname>Merhof</surname>
                <given-names>Dorit</given-names>
              </name>
            </person-group>
            <article-title>SortedAP: Rethinking evaluation metrics for instance segmentation</article-title>
            <source>Proceedings of the IEEE/CVF international conference on computer vision (ICCV) workshops</source>
            <year iso-8601-date="2023-10">2023</year>
            <month>10</month>
            <uri>https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html</uri>
            <fpage>3923</fpage>
            <lpage>3929</lpage>
          </element-citation>
        </ref>
        <ref id="ref-AHN4-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Actueel Hoogtebestand Nederland</string-name>
            </person-group>
            <article-title>AHN4 - Actual Height Model of the Netherlands</article-title>
            <year iso-8601-date="2020">2020</year>
            <uri>https://www.ahn.nl/</uri>
          </element-citation>
        </ref>
        <ref id="ref-Luchtfotos-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Beeldmateriaal Nederland</string-name>
            </person-group>
            <article-title>Luchtfoto’s (Aerial Photographs)</article-title>
            <year iso-8601-date="2024">2024</year>
            <uri>https://www.beeldmateriaal.nl/luchtfotos</uri>
          </element-citation>
        </ref>
        <ref id="ref-IGN_LiDAR_HD-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Institut national de l’information géographique et forestière (IGN)</string-name>
            </person-group>
            <article-title>LiDAR HD</article-title>
            <year iso-8601-date="2020">2020</year>
            <uri>https://geoservices.ign.fr/lidarhd</uri>
          </element-citation>
        </ref>
        <ref id="ref-IGN_BD_ORTHO-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Institut national de l’information géographique et forestière (IGN)</string-name>
            </person-group>
            <article-title>BD ORTHO</article-title>
            <year iso-8601-date="2021">2021</year>
            <uri>https://geoservices.ign.fr/bdortho</uri>
          </element-citation>
        </ref>
        <ref id="ref-amsterdam_trees-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Gemeente Amsterdam</string-name>
            </person-group>
            <article-title>Bomenbestand Amsterdam (Amsterdam Tree Dataset)</article-title>
            <year iso-8601-date="2024">2024</year>
            <uri>https://maps.amsterdam.nl/open_geodata/?k=505</uri>
          </element-citation>
        </ref>
        <ref id="ref-bordeaux_trees-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Bordeaux Métropole</string-name>
            </person-group>
            <article-title>Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole)</article-title>
            <year iso-8601-date="2024">2024</year>
            <uri>https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee</uri>
          </element-citation>
        </ref>
        <ref id="ref-boomregister-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <string-name>Coöperatief Boomregister U.A.</string-name>
            </person-group>
            <article-title>Boom Register (Tree Register)</article-title>
            <year iso-8601-date="2014">2014</year>
            <uri>https://boomregister.nl/</uri>
          </element-citation>
        </ref>
        <ref id="ref-urban-trees-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Arevalo-Ramirez</surname>
                <given-names>Tito</given-names>
              </name>
              <name>
                <surname>Alfaro</surname>
                <given-names>Anali</given-names>
              </name>
              <name>
                <surname>Figueroa</surname>
                <given-names>José</given-names>
              </name>
              <name>
                <surname>Ponce-Donoso</surname>
                <given-names>Mauricio</given-names>
              </name>
              <name>
                <surname>Saavedra</surname>
                <given-names>Jose M.</given-names>
              </name>
              <name>
                <surname>Recabarren</surname>
                <given-names>Matías</given-names>
              </name>
              <name>
                <surname>Delpiano</surname>
                <given-names>José</given-names>
              </name>
            </person-group>
            <article-title>Challenges for computer vision as a tool for screening urban trees through street-view images</article-title>
            <source>Urban Forestry &amp; Urban Greening</source>
            <year iso-8601-date="2024">2024</year>
            <volume>95</volume>
            <issn>1618-8667</issn>
            <uri>https://www.sciencedirect.com/science/article/pii/S1618866724001146</uri>
            <pub-id pub-id-type="doi">10.1016/j.ufug.2024.128316</pub-id>
            <fpage>128316</fpage>
            <lpage/>
          </element-citation>
        </ref>
        <ref id="ref-olive-tree-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Safonova</surname>
                <given-names>Anastasiia</given-names>
              </name>
              <name>
                <surname>Guirado</surname>
                <given-names>Emilio</given-names>
              </name>
              <name>
                <surname>Maglinets</surname>
                <given-names>Yuriy</given-names>
              </name>
              <name>
                <surname>Alcaraz-Segura</surname>
                <given-names>Domingo</given-names>
              </name>
              <name>
                <surname>Tabik</surname>
                <given-names>Siham</given-names>
              </name>
            </person-group>
            <article-title>Olive tree biovolume from UAV multi-resolution image segmentation with mask r-CNN</article-title>
            <source>Sensors</source>
            <year iso-8601-date="2021">2021</year>
            <volume>21</volume>
            <issue>5</issue>
            <issn>1424-8220</issn>
            <uri>https://www.mdpi.com/1424-8220/21/5/1617</uri>
            <pub-id pub-id-type="doi">10.3390/s21051617</pub-id>
            <pub-id pub-id-type="pmid">33668984</pub-id>
            <fpage>1617</fpage>
            <lpage/>
          </element-citation>
        </ref>
        <ref id="ref-amf_gd_yolov8-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Zhong</surname>
                <given-names>Hao</given-names>
              </name>
              <name>
                <surname>Zhang</surname>
                <given-names>Zheyu</given-names>
              </name>
              <name>
                <surname>Liu</surname>
                <given-names>Haoran</given-names>
              </name>
              <name>
                <surname>Wu</surname>
                <given-names>Jinzhuo</given-names>
              </name>
              <name>
                <surname>Lin</surname>
                <given-names>Wenshu</given-names>
              </name>
            </person-group>
            <article-title>Individual tree species identification for complex coniferous and broad-leaved mixed forests based on deep learning combined with UAV LiDAR data and RGB images</article-title>
            <source>Forests</source>
            <year iso-8601-date="2024">2024</year>
            <volume>15</volume>
            <issue>2</issue>
            <issn>1999-4907</issn>
            <uri>https://www.mdpi.com/1999-4907/15/2/293</uri>
            <pub-id pub-id-type="doi">10.3390/f15020293</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-lidar_benchmark-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Eysn</surname>
                <given-names>Lothar</given-names>
              </name>
              <name>
                <surname>Hollaus</surname>
                <given-names>Markus</given-names>
              </name>
              <name>
                <surname>Lindberg</surname>
                <given-names>Eva</given-names>
              </name>
              <name>
                <surname>Berger</surname>
                <given-names>Frédéric</given-names>
              </name>
              <name>
                <surname>Monnet</surname>
                <given-names>Jean-Matthieu</given-names>
              </name>
              <name>
                <surname>Dalponte</surname>
                <given-names>Michele</given-names>
              </name>
              <name>
                <surname>Kobal</surname>
                <given-names>Milan</given-names>
              </name>
              <name>
                <surname>Pellegrini</surname>
                <given-names>Marco</given-names>
              </name>
              <name>
                <surname>Lingua</surname>
                <given-names>Emanuele</given-names>
              </name>
              <name>
                <surname>Mongus</surname>
                <given-names>Domen</given-names>
              </name>
              <name>
                <surname>Pfeifer</surname>
                <given-names>Norbert</given-names>
              </name>
            </person-group>
            <article-title>A benchmark of lidar-based single tree detection methods using heterogeneous forest data from the alpine space</article-title>
            <source>Forests</source>
            <year iso-8601-date="2015">2015</year>
            <volume>6</volume>
            <issue>5</issue>
            <issn>1999-4907</issn>
            <uri>https://www.mdpi.com/1999-4907/6/5/1721</uri>
            <pub-id pub-id-type="doi">10.3390/f6051721</pub-id>
            <fpage>1721</fpage>
            <lpage>1747</lpage>
          </element-citation>
        </ref>
        <ref id="ref-rgb-dl-watershed-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Freudenberg</surname>
                <given-names>Maximilian</given-names>
              </name>
              <name>
                <surname>Magdon</surname>
                <given-names>Paul</given-names>
              </name>
              <name>
                <surname>Nölke</surname>
                <given-names>Nils</given-names>
              </name>
            </person-group>
            <article-title>Individual tree crown delineation in high-resolution remote sensing images based on u-net</article-title>
            <source>Neural Computing and Applications</source>
            <year iso-8601-date="2022">2022</year>
            <volume>34</volume>
            <issue>24</issue>
            <issn>1433-3058</issn>
            <pub-id pub-id-type="doi">10.1007/s00521-022-07640-4</pub-id>
            <fpage>22197</fpage>
            <lpage>22207</lpage>
          </element-citation>
        </ref>
        <ref id="ref-DeepForest-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Weinstein</surname>
                <given-names>Ben G.</given-names>
              </name>
              <name>
                <surname>Marconi</surname>
                <given-names>Sergio</given-names>
              </name>
              <name>
                <surname>Aubry-Kientz</surname>
                <given-names>Mélaine</given-names>
              </name>
              <name>
                <surname>Vincent</surname>
                <given-names>Gregoire</given-names>
              </name>
              <name>
                <surname>Senyondo</surname>
                <given-names>Henry</given-names>
              </name>
              <name>
                <surname>White</surname>
                <given-names>Ethan P.</given-names>
              </name>
            </person-group>
            <article-title>DeepForest: A python package for RGB deep learning tree crown delineation</article-title>
            <source>Methods in Ecology and Evolution</source>
            <year iso-8601-date="2020">2020</year>
            <volume>11</volume>
            <issue>12</issue>
            <uri>https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13472</uri>
            <pub-id pub-id-type="doi">10.1111/2041-210X.13472</pub-id>
            <fpage>1743</fpage>
            <lpage>1751</lpage>
          </element-citation>
        </ref>
        <ref id="ref-NEONdata-nb-article">
          <element-citation>
            <person-group person-group-type="author">
              <name>
                <surname>Weinstein</surname>
                <given-names>Ben</given-names>
              </name>
              <name>
                <surname>Marconi</surname>
                <given-names>Sergio</given-names>
              </name>
              <name>
                <surname>White</surname>
                <given-names>Ethan</given-names>
              </name>
            </person-group>
            <article-title>Data for the NeonTreeEvaluation benchmark (0.2.2)</article-title>
            <publisher-name>Zenodo</publisher-name>
            <year iso-8601-date="2022">2022</year>
            <pub-id pub-id-type="doi">10.5281/zenodo.5914554</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-lidar_benchmark_2-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Wang</surname>
                <given-names>Yunsheng</given-names>
              </name>
              <name>
                <surname>Hyyppä</surname>
                <given-names>Juha</given-names>
              </name>
              <name>
                <surname>Liang</surname>
                <given-names>Xinlian</given-names>
              </name>
              <name>
                <surname>Kaartinen</surname>
                <given-names>Harri</given-names>
              </name>
              <name>
                <surname>Yu</surname>
                <given-names>Xiaowei</given-names>
              </name>
              <name>
                <surname>Lindberg</surname>
                <given-names>Eva</given-names>
              </name>
              <name>
                <surname>Holmgren</surname>
                <given-names>Johan</given-names>
              </name>
              <name>
                <surname>Qin</surname>
                <given-names>Yuchu</given-names>
              </name>
              <name>
                <surname>Mallet</surname>
                <given-names>Clément</given-names>
              </name>
              <name>
                <surname>Ferraz</surname>
                <given-names>António</given-names>
              </name>
              <name>
                <surname>Torabzadeh</surname>
                <given-names>Hossein</given-names>
              </name>
              <name>
                <surname>Morsdorf</surname>
                <given-names>Felix</given-names>
              </name>
              <name>
                <surname>Zhu</surname>
                <given-names>Lingli</given-names>
              </name>
              <name>
                <surname>Liu</surname>
                <given-names>Jingbin</given-names>
              </name>
              <name>
                <surname>Alho</surname>
                <given-names>Petteri</given-names>
              </name>
            </person-group>
            <article-title>International benchmarking of the individual tree detection methods for modeling 3-d canopy structure for silviculture and forest ecology using airborne laser scanning</article-title>
            <source>IEEE Transactions on Geoscience and Remote Sensing</source>
            <year iso-8601-date="2016">2016</year>
            <volume>54</volume>
            <issue>9</issue>
            <pub-id pub-id-type="doi">10.1109/TGRS.2016.2543225</pub-id>
            <fpage>5011</fpage>
            <lpage>5027</lpage>
          </element-citation>
        </ref>
        <ref id="ref-gan_data_augment-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Sun</surname>
                <given-names>Chenxin</given-names>
              </name>
              <name>
                <surname>Huang</surname>
                <given-names>Chengwei</given-names>
              </name>
              <name>
                <surname>Zhang</surname>
                <given-names>Huaiqing</given-names>
              </name>
              <name>
                <surname>Chen</surname>
                <given-names>Bangqian</given-names>
              </name>
              <name>
                <surname>An</surname>
                <given-names>Feng</given-names>
              </name>
              <name>
                <surname>Wang</surname>
                <given-names>Liwen</given-names>
              </name>
              <name>
                <surname>Yun</surname>
                <given-names>Ting</given-names>
              </name>
            </person-group>
            <article-title>Individual tree crown segmentation and crown width extraction from a heightmap derived from aerial laser scanning data using a deep learning framework</article-title>
            <source>Frontiers in Plant Science</source>
            <year iso-8601-date="2022">2022</year>
            <volume>13</volume>
            <issn>1664-462X</issn>
            <uri>https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2022.914974</uri>
            <pub-id pub-id-type="doi">10.3389/fpls.2022.914974</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-albumentations-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Buslaev</surname>
                <given-names>Alexander</given-names>
              </name>
              <name>
                <surname>Iglovikov</surname>
                <given-names>Vladimir I.</given-names>
              </name>
              <name>
                <surname>Khvedchenya</surname>
                <given-names>Eugene</given-names>
              </name>
              <name>
                <surname>Parinov</surname>
                <given-names>Alex</given-names>
              </name>
              <name>
                <surname>Druzhinin</surname>
                <given-names>Mikhail</given-names>
              </name>
              <name>
                <surname>Kalinin</surname>
                <given-names>Alexandr A.</given-names>
              </name>
            </person-group>
            <article-title>Albumentations: Fast and flexible image augmentations</article-title>
            <source>Information</source>
            <year iso-8601-date="2020">2020</year>
            <volume>11</volume>
            <issue>2</issue>
            <issn>2078-2489</issn>
            <uri>https://www.mdpi.com/2078-2489/11/2/125</uri>
            <pub-id pub-id-type="doi">10.3390/info11020125</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-lidar_classification-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Diab</surname>
                <given-names>Ahmed</given-names>
              </name>
              <name>
                <surname>Kashef</surname>
                <given-names>Rasha</given-names>
              </name>
              <name>
                <surname>Shaker</surname>
                <given-names>Ahmed</given-names>
              </name>
            </person-group>
            <article-title>Deep learning for LiDAR point cloud classification in remote sensing</article-title>
            <source>Sensors (Basel)</source>
            <year iso-8601-date="2022-10">2022</year>
            <month>10</month>
            <volume>22</volume>
            <issue>20</issue>
            <pub-id pub-id-type="doi">10.3390/s22207868</pub-id>
            <pub-id pub-id-type="pmid">36298220</pub-id>
            <fpage>7868</fpage>
            <lpage/>
          </element-citation>
        </ref>
        <ref id="ref-lidar_rgb_wst-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Qin</surname>
                <given-names>Haiming</given-names>
              </name>
              <name>
                <surname>Zhou</surname>
                <given-names>Weiqi</given-names>
              </name>
              <name>
                <surname>Yao</surname>
                <given-names>Yang</given-names>
              </name>
              <name>
                <surname>Wang</surname>
                <given-names>Weimin</given-names>
              </name>
            </person-group>
            <article-title>Individual tree segmentation and tree species classification in subtropical broadleaf forests using UAV-based LiDAR, hyperspectral, and ultrahigh-resolution RGB data</article-title>
            <source>Remote Sensing of Environment</source>
            <year iso-8601-date="2022">2022</year>
            <volume>280</volume>
            <issn>0034-4257</issn>
            <uri>https://www.sciencedirect.com/science/article/pii/S0034425722002577</uri>
            <pub-id pub-id-type="doi">10.1016/j.rse.2022.113143</pub-id>
            <fpage>113143</fpage>
            <lpage/>
          </element-citation>
        </ref>
        <ref id="ref-lidar_rgb_acnet-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Li</surname>
                <given-names>Yingbo</given-names>
              </name>
              <name>
                <surname>Chai</surname>
                <given-names>Guoqi</given-names>
              </name>
              <name>
                <surname>Wang</surname>
                <given-names>Yueting</given-names>
              </name>
              <name>
                <surname>Lei</surname>
                <given-names>Lingting</given-names>
              </name>
              <name>
                <surname>Zhang</surname>
                <given-names>Xiaoli</given-names>
              </name>
            </person-group>
            <article-title>ACE r-CNN: An attention complementary and edge detection-based instance segmentation algorithm for individual tree species identification using UAV RGB images and LiDAR data</article-title>
            <source>Remote Sensing</source>
            <year iso-8601-date="2022">2022</year>
            <volume>14</volume>
            <issue>13</issue>
            <issn>2072-4292</issn>
            <uri>https://www.mdpi.com/2072-4292/14/13/3035</uri>
            <pub-id pub-id-type="doi">10.3390/rs14133035</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-watershed-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Vincent</surname>
                <given-names>L.</given-names>
              </name>
              <name>
                <surname>Soille</surname>
                <given-names>P.</given-names>
              </name>
            </person-group>
            <article-title>Watersheds in digital spaces: An efficient algorithm based on immersion simulations</article-title>
            <source>IEEE Transactions on Pattern Analysis and Machine Intelligence</source>
            <year iso-8601-date="1991">1991</year>
            <volume>13</volume>
            <issue>6</issue>
            <pub-id pub-id-type="doi">10.1109/34.87344</pub-id>
            <fpage>583</fpage>
            <lpage>598</lpage>
          </element-citation>
        </ref>
        <ref id="ref-lidar_watershed-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Kwak</surname>
                <given-names>Doo-Ahn</given-names>
              </name>
              <name>
                <surname>Lee</surname>
                <given-names>Woo-Kyun</given-names>
              </name>
              <name>
                <surname>Lee</surname>
                <given-names>Jun-Hak</given-names>
              </name>
              <name>
                <surname>Biging</surname>
                <given-names>Greg S.</given-names>
              </name>
              <name>
                <surname>Gong</surname>
                <given-names>Peng</given-names>
              </name>
            </person-group>
            <article-title>Detection of individual trees and estimation of tree height using LiDAR data</article-title>
            <source>Journal of Forest Research</source>
            <year iso-8601-date="2007">2007</year>
            <volume>12</volume>
            <issue>6</issue>
            <issn>1610-7403</issn>
            <pub-id pub-id-type="doi">10.1007/s10310-007-0041-9</pub-id>
            <fpage>425</fpage>
            <lpage>434</lpage>
          </element-citation>
        </ref>
        <ref id="ref-rgb_analytical-nb-article">
          <element-citation publication-type="chapter">
            <person-group person-group-type="author">
              <name>
                <surname>Gomes</surname>
                <given-names>Marilia Ferreira</given-names>
              </name>
              <name>
                <surname>Maillard</surname>
                <given-names>Philippe</given-names>
              </name>
            </person-group>
            <article-title>Detection of tree crowns in very high spatial resolution images</article-title>
            <source>Environmental applications of remote sensing</source>
            <person-group person-group-type="editor">
              <name>
                <surname>Marghany</surname>
                <given-names>Maged</given-names>
              </name>
            </person-group>
            <publisher-name>IntechOpen</publisher-name>
            <publisher-loc>Rijeka</publisher-loc>
            <year iso-8601-date="2016">2016</year>
            <pub-id pub-id-type="doi">10.5772/62122</pub-id>
          </element-citation>
        </ref>
        <ref id="ref-local-maximum-nb-article">
          <element-citation publication-type="article-journal">
            <person-group person-group-type="author">
              <name>
                <surname>Wulder</surname>
                <given-names>Mike</given-names>
              </name>
              <name>
                <surname>Niemann</surname>
                <given-names>K.Olaf</given-names>
              </name>
              <name>
                <surname>Goodenough</surname>
                <given-names>David G.</given-names>
              </name>
            </person-group>
            <article-title>Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery</article-title>
            <source>Remote Sensing of Environment</source>
            <year iso-8601-date="2000">2000</year>
            <volume>73</volume>
            <issue>1</issue>
            <issn>0034-4257</issn>
            <uri>https://www.sciencedirect.com/science/article/pii/S0034425700001012</uri>
            <pub-id pub-id-type="doi">10.1016/S0034-4257(00)00101-2</pub-id>
            <fpage>103</fpage>
            <lpage>114</lpage>
          </element-citation>
        </ref>
        <ref id="ref-valley-following-nb-article">
          <element-citation publication-type="paper-conference">
            <person-group person-group-type="author">
              <name>
                <surname>Gougeon</surname>
                <given-names>François A</given-names>
              </name>
              <etal/>
            </person-group>
            <article-title>Automatic individual tree crown delineation using a valley-following algorithm and rule-based system</article-title>
            <source>Proc. International forum on automated interpretation of high spatial resolution digital imagery for forestry, victoria, british columbia, canada</source>
            <publisher-name>Citeseer</publisher-name>
            <year iso-8601-date="1998">1998</year>
            <uri>https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf</uri>
            <fpage>11</fpage>
            <lpage>23</lpage>
          </element-citation>
        </ref>
        <ref id="ref-template-matching-nb-article">
          <element-citation publication-type="thesis">
            <person-group person-group-type="author">
              <name>
                <surname>Pollock</surname>
                <given-names>Richard James</given-names>
              </name>
            </person-group>
            <article-title>The automatic recognition of individual trees in aerial images of forests based on a synthetic tree crown image model</article-title>
            <publisher-name>The University of British Columbia (Canada)</publisher-name>
            <year iso-8601-date="1996">1996</year>
            <isbn>0612148157</isbn>
            <uri>https://dx.doi.org/10.14288/1.0051597</uri>
          </element-citation>
        </ref>
        <ref id="ref-yolo-nb-article">
          <element-citation publication-type="paper-conference">
            <person-group person-group-type="author">
              <name>
                <surname>Redmon</surname>
                <given-names>Joseph</given-names>
              </name>
              <name>
                <surname>Divvala</surname>
                <given-names>Santosh</given-names>
              </name>
              <name>
                <surname>Girshick</surname>
                <given-names>Ross</given-names>
              </name>
              <name>
                <surname>Farhadi</surname>
                <given-names>Ali</given-names>
              </name>
            </person-group>
            <article-title>You only look once: Unified, real-time object detection</article-title>
            <source>2016 IEEE conference on computer vision and pattern recognition (CVPR)</source>
            <year iso-8601-date="2016">2016</year>
            <pub-id pub-id-type="doi">10.1109/CVPR.2016.91</pub-id>
            <fpage>779</fpage>
            <lpage>788</lpage>
          </element-citation>
        </ref>
      </ref-list>
    </back>
  </sub-article>
</article>
