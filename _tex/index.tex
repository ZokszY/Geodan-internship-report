% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
]{report}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage[]{biblatex}
\addbibresource{references.bib}
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Tree object detection using airborne images and LiDAR point clouds},
  pdfauthor={Alexandre Bry},
  pdfkeywords={tree detection, deep learning},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Tree object detection using airborne images and LiDAR point
clouds}
\author{Alexandre Bry}
\date{2024-07-09}

\begin{document}
\maketitle
\begin{abstract}
This is the abstract. It can be on multiple lines and contain
\textbf{Markdown}.
\end{abstract}

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\chapter{Introduction}\label{introduction}

The goal of the internship was to study the possibility of combining
LiDAR point clouds and aerial images in a deep learning model to perform
instance segmentation of trees. The two types of data are indeed
complementary, as point clouds capture the shape of the worlds, while
images capture the colors. However, combining them into a format that
allows a model to handle them simultaneously is not an easy task because
they inherently have a very different spatial repartition and encoding.

The second major topic of the internship was to acquire a proper dataset
matching all the criteria required for the project. Most of the datasets
containing tree annotations only used either RGB images or LiDAR point
clouds, but not both. Therefore, I had to create such a dataset by
myself, using the openly available images and point clouds in the
Netherlands, by annotating trees by hand to properly train and evaluate
the methods.

\chapter{State-of-the-art}\label{state-of-the-art}

\section{Datasets}\label{datasets}

\subsection{Computer vision tasks}\label{computer-vision-tasks}

Difference between object detection and instance segmentation. The first
on is easier to train because it only requires bounding boxes

\subsection{Requirements}\label{requirements}

Before presenting the different promising datasets and the reasons why
they were not fully usable for the project, let's enumerate the
different conditions and requirements for the tree instance segmentation
task:

\begin{itemize}
\tightlist
\item
  Multiple types of data:

  \begin{itemize}
  \tightlist
  \item
    Aerial RGB images
  \item
    LiDAR point clouds (preferably aerial)
  \item
    (Optional) Aerial infrared images
  \end{itemize}
\item
  Tree crown annotations or bounding boxes
\item
  High-enough resolution:

  \begin{itemize}
  \tightlist
  \item
    For images, about 25~cm
  \item
    For point clouds, about 10~cm
  \end{itemize}
\end{itemize}

Here are the explanations for these requirements. As for the types of
data, RGB images and point clouds are required to experiment on the
ability of the model to combine the two very different kinds of
information they hold. Having infrared data as well could be beneficial,
but it was not necessary. Regarding tree annotations, it was necessary
to have a way to spatially identify them individually, using crown
contours or simply bounding boxes. Since the model outputs bounding
boxes, any kind of other format could easily be transformed to bounding
boxes. Finally, the resolution had to be high enough to identify
individual trees and be able to really use the data. For the point
clouds especially, the whole idea was to see if and how the topology of
the trees could be learnt, using at least the trunks and even the
biggest branches if possible. Therefore, even if they are not really
comparable, this is the reason why the required resolution is more
precise for the point clouds.

Unfortunately, none of the datasets that I found matched all these
criteria. Furthermore, I didn't find any overlapping datasets that I
could merge to create a dataset with all the required types of data. In
the next parts, I will go through the different kinds of datasets that
exist, the reasons why they did not really fit for the project and the
ideas I got when searching for a way to use them.

\subsection{Existing tree datasets}\label{existing-tree-datasets}

As explained above, there were quite a lot of requirements to fulfill to
have a complete dataset usable for the task. This means that almost all
the available datasets were missing something, as they were mainly
focusing on using one kind of data and trying to make the most out of
it, instead of trying to use all the types of data together.

The most comprehensive list of tree annotations datasets was published
in \textcite{OpenForest}. \textcite{FoMo-Bench} also lists several
interesting datasets, even though most of them can also be found in
\textcite{OpenForest}. Without enumerating all of them, there were
multiple kinds of datasets that all have their own flaws regarding the
requirements I was looking for.

Firstly, there are the forest inventories. \textcite{TALLO} is probably
the most interesting one in this category, because it contains a lot of
spatial information about almost 500K trees, with their locations, their
crown radii and their heights. Therefore, everything needed to localize
trees is in the dataset. However, I didn't manage to find RGB images or
LiDAR point clouds of the areas where the trees are located, making it
impossible to use these annotations to train tree detection.

Secondly, there are the RGB datasets. \textcite{ReforesTree} and
\textcite{MillionTrees} are two of them and the quality of their images
are high. The only drawback of these datasets is obviously that they
don't provide any kind of point cloud, which make them unsuitable for
the task.

Thirdly, there are the LiDAR datasets, such as \textcite{WildForest3D}
and \textcite{FOR-instance}. Similarly to RGB datasets, they lack one of
the data source for the task I worked on. But unlike them, they have the
advantage that the missing data could be much easier to acquire from
another source, since RGB aerial or satellite images are much more
common than LiDAR point clouds. However, this solution was abandoned for
two main reasons. First it is quite challenging to find the exact
locations where the point clouds were acquired. Then, even when the
location is known, it is often in the middle of a forest where the
quality of satellite imagery is very low.

Finally, I also found two datasets that had RGB and LiDAR components.
The first one is \textcite{MDAS}. This benchmark dataset encompasses RGB
images, hyperspectral images and Digital Surface Models (DSM). There
were however two major flaws. The obvious one was that this dataset was
created with land semantic segmentation tasks in mind, so there was no
tree annotations. The less obvious one was that a DSM is not a point
cloud, even though it is some kind of 3D information and was often
created using a LiDAR point cloud. As a consequence, I would have been
very limited in my ability to use the point cloud.

The only real dataset with RGB and LiDAR was \textcite{NEON}. This
dataset contains exactly all the data I was looking for, with RGB
images, hyperspectral images and LiDAR point clouds. With 30975 tree
annotations, it is also a quite large dataset, spanning across multiple
various forests. The reason why I decided not to use it despite all this
is that at the beginning of the project, I thought that the quality of
the images and the point clouds was too low. Looking back on this
decision, I think that I probably could have worked with this dataset
and gotten great results. This would have saved me the time spent
annotating the trees for my own dataset, which I will talk more about
later. My decision was also influenced by the quality of the images and
the point clouds available in the Netherlands, which I will talk about
in the next section.

\subsection{Public data}\label{public-data}

After rejecting all the available datasets I had found, the only
solution I had left was to create my own dataset. I won't dive too much
in this process that I will explain in Section~\ref{sec-dataset}. I just
want to mention all the publicly available datasets that I used or could
have used to create this custom dataset.

For practical reasons, the two countries where I mostly searched for
available data are France and the Netherlands. I was looking for three
different data types independently:

\begin{itemize}
\tightlist
\item
  RGB (and eventually infrared) images
\item
  LiDAR point clouds
\item
  Tree annotations
\end{itemize}

These three types of data are available in similar ways in both
countries, although the Netherlands have a small edge over France. RGB
images are really easy to find in France (\textcite{IGN_BDORTHO}) and in
the Netherlands (\textcite{Luchtfotos}), but the resolution is better in
the Netherlands (8~cm vs 20~cm). Hyperspectral images are also available
in both countries, although for those the resolution is only 25~cm in
the Netherlands.

As for LiDAR point clouds, the Netherlands have a small edge over
France, because they are at their forth version covering the whole
country with \textcite{AHN4}, and are already working on the fifth
version. In France, data acquisition for the first LiDAR point cloud
covering the whole country (\textcite{IGN_LiDARHD}) started a few years
ago. It is not yet finished, even though data is already available for
half of the country. The other advantage of the data from Netherlands
regarding LiDAR point clouds is that all flights are performed during
winter, which allows light beams to penetrate more deeply in trees and
reach trunks and branches. This is not the case in France.

The part that is missing in both countries is related to tree
annotations. Many municipalities have datasets containing information
about all the public trees they handle. This is for example the case for
\textcite{amsterdam_trees} and \textcite{bordeaux_trees}. However, these
datasets cannot really be used as ground truth for a custom dataset for
several reasons. First, many of them do not contain coordinates
indicating the position of each tree in the city. Then, even those that
contain coordinates are most of the time missing any kind of information
allowing to deduce a bounding box for the tree crowns. Finally, even if
they did contain everything, they only focus on public trees, and are
missing every single tree located in a private area. Since public and
private areas are obviously imbricated in all cities, it means that any
area we try to train the model on would be missing all the private
trees, making the training process impossible because we cannot have
only a partial annotation of images.

\subsection{Dataset augmentation
techniques}\label{dataset-augmentation-techniques}

\section{Models}\label{models}

\chapter{Dataset}\label{sec-dataset}

\chapter{Model}\label{model}

\chapter{Results}\label{results}

Beyond mAP: \textcite{BeyondMAP}

\chapter{Conclusion}\label{conclusion}

Blablabla


\printbibliography[title=Bibliography]



\end{document}
