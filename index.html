<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.47">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alexandre Bry">
<meta name="dcterms.date" content="2024-07-22">
<meta name="keywords" content="tree detection, deep learning">

<title>Tree object detection using airborne images and LiDAR point clouds</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<meta name="citation_title" content="Tree object detection using airborne images and LiDAR point clouds">
<meta name="citation_abstract" content="This is the abstract.
It can be on multiple lines and contain **Markdown**.
">
<meta name="citation_keywords" content="tree detection,deep learning">
<meta name="citation_author" content="Alexandre Bry">
<meta name="citation_publication_date" content="2024-07-22">
<meta name="citation_cover_date" content="2024-07-22">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-07-22">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=FoMo-bench: A multi-modal, multi-scale and multi-task forest monitoring benchmark for remote sensing foundation models;,citation_author=Nikolaos Ioannis Bountos;,citation_author=Arthur Ouaknine;,citation_author=David Rolnick;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.10114;,citation_journal_title=arXiv preprint arXiv:2312.10114;">
<meta name="citation_reference" content="citation_title=OpenForest: A data catalogue for machine learning in forest monitoring;,citation_author=Arthur Ouaknine;,citation_author=Teja Kattenborn;,citation_author=Etienne Laliberté;,citation_author=David Rolnick;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2311.00277;">
<meta name="citation_reference" content="citation_title=Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks;,citation_abstract=Remote sensing can transform the speed, scale, and cost of biodiversity and forestry surveys. Data acquisition currently outpaces the ability to identify individual organisms in high resolution imagery. We outline an approach for identifying tree-crowns in RGB imagery while using a semi-supervised deep learning detection network. Individual crown delineation has been a long-standing challenge in remote sensing and available algorithms produce mixed results. We show that deep learning models can leverage existing Light Detection and Ranging (LIDAR)-based unsupervised delineation to generate trees that are used for training an initial RGB crown detection model. Despite limitations in the original unsupervised detection approach, this noisy training data may contain information from which the neural network can learn initial tree features. We then refine the initial model using a small number of higher-quality hand-annotated RGB images. We validate our proposed approach while using an open-canopy site in the National Ecological Observation Network. Our results show that a model using 434,551 self-generated trees with the addition of 2848 hand-annotated trees yields accurate predictions in natural landscapes. Using an intersection-over-union threshold of 0.5, the full model had an average tree crown recall of 0.69, with a precision of 0.61 for the visually-annotated data. The model had an average tree detection rate of 0.82 for the field collected stems. The addition of a small number of hand-annotated trees improved the performance over the initial self-supervised model. This semi-supervised deep learning approach demonstrates that remote sensing can overcome a lack of labeled training data by generating noisy data for initial training using unsupervised methods and retraining the resulting models with high quality labeled data.;,citation_author=Ben G. Weinstein;,citation_author=Sergio Marconi;,citation_author=Stephanie Bohlman;,citation_author=Alina Zare;,citation_author=Ethan White;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://www.mdpi.com/2072-4292/11/11/1309;,citation_issue=11;,citation_doi=10.3390/rs11111309;,citation_issn=2072-4292;,citation_volume=11;,citation_journal_title=Remote Sensing;">
<meta name="citation_reference" content="citation_title=ReforesTree: A dataset for estimating tropical forest carbon stock with deep learning and aerial imagery;,citation_author=Gyri Reiersen;,citation_author=David Dao;,citation_author=Björn Lütjens;,citation_author=Konstantin Klemmer;,citation_author=Kenza Amara;,citation_author=Attila Steinegger;,citation_author=Ce Zhang;,citation_author=Xiaoxiang Zhu;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2201.11192;">
<meta name="citation_reference" content="citation_title=FOR-instance: A UAV laser scanning benchmark dataset for semantic and instance segmentation of individual trees;,citation_author=Stefano Puliti;,citation_author=Grant Pearse;,citation_author=Peter Surový;,citation_author=Luke Wallace;,citation_author=Markus Hollaus;,citation_author=Maciej Wielgosz;,citation_author=Rasmus Astrup;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2309.01279;">
<meta name="citation_reference" content="citation_title=MDAS: A new multimodal benchmark dataset for remote sensing;,citation_author=J. Hu;,citation_author=R. Liu;,citation_author=D. Hong;,citation_author=A. Camero;,citation_author=J. Yao;,citation_author=M. Schneider;,citation_author=F. Kurz;,citation_author=K. Segl;,citation_author=X. X. Zhu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://essd.copernicus.org/articles/15/113/2023/;,citation_issue=1;,citation_doi=10.5194/essd-15-113-2023;,citation_volume=15;,citation_journal_title=Earth System Science Data;">
<meta name="citation_reference" content="citation_title=Tallo: A global tree allometry and crown architecture database;,citation_abstract=Abstract Data capturing multiple axes of tree size and shape, such as a tree’s stem diameter, height and crown size, underpin a wide range of ecological research—from developing and testing theory on forest structure and dynamics, to estimating forest carbon stocks and their uncertainties, and integrating remote sensing imagery into forest monitoring programmes. However, these data can be surprisingly hard to come by, particularly for certain regions of the world and for specific taxonomic groups, posing a real barrier to progress in these fields. To overcome this challenge, we developed the Tallo database, a collection of 498,838 georeferenced and taxonomically standardized records of individual trees for which stem diameter, height and/or crown radius have been measured. These data were collected at 61,856 globally distributed sites, spanning all major forested and non-forested biomes. The majority of trees in the database are identified to species (88%), and collectively Tallo includes data for 5163 species distributed across 1453 genera and 187 plant families. The database is publicly archived under a CC-BY 4.0 licence and can be access from: https://doi.org/10.5281/zenodo.6637599. To demonstrate its value, here we present three case studies that highlight how the Tallo database can be used to address a range of theoretical and applied questions in ecology—from testing the predictions of metabolic scaling theory, to exploring the limits of tree allometric plasticity along environmental gradients and modelling global variation in maximum attainable tree height. In doing so, we provide a key resource for field ecologists, remote sensing researchers and the modelling community working together to better understand the role that trees play in regulating the terrestrial carbon cycle.;,citation_author=Tommaso Jucker;,citation_author=Fabian Jörg Fischer;,citation_author=Jérôme Chave;,citation_author=David A. Coomes;,citation_author=John Caspersen;,citation_author=Arshad Ali;,citation_author=Grace Jopaul Loubota Panzou;,citation_author=Ted R. Feldpausch;,citation_author=Daniel Falster;,citation_author=Vladimir A. Usoltsev;,citation_author=Stephen Adu-Bredu;,citation_author=Luciana F. Alves;,citation_author=Mohammad Aminpour;,citation_author=Ilondea B. Angoboy;,citation_author=Niels P. R. Anten;,citation_author=Cécile Antin;,citation_author=Yousef Askari;,citation_author=Rodrigo Muñoz;,citation_author=Narayanan Ayyappan;,citation_author=Patricia Balvanera;,citation_author=Lindsay Banin;,citation_author=Nicolas Barbier;,citation_author=John J. Battles;,citation_author=Hans Beeckman;,citation_author=Yannick E. Bocko;,citation_author=Ben Bond-Lamberty;,citation_author=Frans Bongers;,citation_author=Samuel Bowers;,citation_author=Thomas Brade;,citation_author=Michiel Breugel;,citation_author=Arthur Chantrain;,citation_author=Rajeev Chaudhary;,citation_author=Jingyu Dai;,citation_author=Michele Dalponte;,citation_author=Kangbéni Dimobe;,citation_author=Jean-Christophe Domec;,citation_author=Jean-Louis Doucet;,citation_author=Remko A. Duursma;,citation_author=Moisés Enríquez;,citation_author=Karin Y. Ewijk;,citation_author=William Farfán-Rios;,citation_author=Adeline Fayolle;,citation_author=Eric Forni;,citation_author=David I. Forrester;,citation_author=Hammad Gilani;,citation_author=John L. Godlee;,citation_author=Sylvie Gourlet-Fleury;,citation_author=Matthias Haeni;,citation_author=Jefferson S. Hall;,citation_author=Jie-Kun He;,citation_author=Andreas Hemp;,citation_author=José L. Hernández-Stefanoni;,citation_author=Steven I. Higgins;,citation_author=Robert J. Holdaway;,citation_author=Kiramat Hussain;,citation_author=Lindsay B. Hutley;,citation_author=Tomoaki Ichie;,citation_author=Yoshiko Iida;,citation_author=Hai-sheng Jiang;,citation_author=Puspa Raj Joshi;,citation_author=Hasan Kaboli;,citation_author=Maryam Kazempour Larsary;,citation_author=Tanaka Kenzo;,citation_author=Brian D. Kloeppel;,citation_author=Takashi Kohyama;,citation_author=Suwash Kunwar;,citation_author=Shem Kuyah;,citation_author=Jakub Kvasnica;,citation_author=Siliang Lin;,citation_author=Emily R. Lines;,citation_author=Hongyan Liu;,citation_author=Craig Lorimer;,citation_author=Jean-Joël Loumeto;,citation_author=Yadvinder Malhi;,citation_author=Peter L. Marshall;,citation_author=Eskil Mattsson;,citation_author=Radim Matula;,citation_author=Jorge A. Meave;,citation_author=Sylvanus Mensah;,citation_author=Xiangcheng Mi;,citation_author=Stéphane Momo;,citation_author=Glenn R. Moncrieff;,citation_author=Francisco Mora;,citation_author=Sarath P. Nissanka;,citation_author=Kevin L. O’Hara;,citation_author=Steven Pearce;,citation_author=Raphaël Pelissier;,citation_author=Pablo L. Peri;,citation_author=Pierre Ploton;,citation_author=Lourens Poorter;,citation_author=Mohsen Javanmiri Pour;,citation_author=Hassan Pourbabaei;,citation_author=Juan Manuel Dupuy-Rada;,citation_author=Sabina C. Ribeiro;,citation_author=Casey Ryan;,citation_author=Anvar Sanaei;,citation_author=Jennifer Sanger;,citation_author=Michael Schlund;,citation_author=Giacomo Sellan;,citation_author=Alexander Shenkin;,citation_author=Bonaventure Sonké;,citation_author=Frank J. Sterck;,citation_author=Martin Svátek;,citation_author=Kentaro Takagi;,citation_author=Anna T. Trugman;,citation_author=Farman Ullah;,citation_author=Matthew A. Vadeboncoeur;,citation_author=Ahmad Valipour;,citation_author=Mark C. Vanderwel;,citation_author=Alejandra G. Vovides;,citation_author=Weiwei Wang;,citation_author=Li-Qiu Wang;,citation_author=Christian Wirth;,citation_author=Murray Woods;,citation_author=Wenhua Xiang;,citation_author=Fabiano de Aquino Ximenes;,citation_author=Yaozhan Xu;,citation_author=Toshihiro Yamada;,citation_author=Miguel A. Zavala;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.16302;,citation_issue=17;,citation_doi=10.1111/gcb.16302;,citation_volume=28;,citation_journal_title=Global Change Biology;">
<meta name="citation_reference" content="citation_title=MillionTrees;,citation_author=Ben Weinstein;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://milliontrees.idtrees.org/;">
<meta name="citation_reference" content="citation_title=Multi-layer modeling of dense vegetation from aerial LiDAR scans;,citation_author=Ekaterina Kalinicheva;,citation_author=Loic Landrieu;,citation_author=Clément Mallet;,citation_author=Nesrine Chehata;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.11620;">
<meta name="citation_reference" content="citation_title=SortedAP: Rethinking evaluation metrics for instance segmentation;,citation_author=Long Chen;,citation_author=Yuli Wu;,citation_author=Johannes Stegmaier;,citation_author=Dorit Merhof;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html;,citation_conference_title=Proceedings of the IEEE/CVF international conference on computer vision (ICCV) workshops;">
<meta name="citation_reference" content="citation_title=AHN4 - Actual Height Model of the Netherlands;,citation_author=Actueel Hoogtebestand Nederland;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://www.ahn.nl/;">
<meta name="citation_reference" content="citation_title=Luchtfoto’s (Aerial Photographs);,citation_author=Beeldmateriaal Nederland;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.beeldmateriaal.nl/luchtfotos;">
<meta name="citation_reference" content="citation_title=LiDAR HD;,citation_author=Institut national de l’information géographique et forestière (IGN);,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://geoservices.ign.fr/lidarhd;">
<meta name="citation_reference" content="citation_title=BD ORTHO;,citation_author=Institut national de l’information géographique et forestière (IGN);,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://geoservices.ign.fr/bdortho;">
<meta name="citation_reference" content="citation_title=Bomenbestand Amsterdam (Amsterdam Tree Dataset);,citation_author=Gemeente Amsterdam;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://maps.amsterdam.nl/open_geodata/?k=505;">
<meta name="citation_reference" content="citation_title=Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole);,citation_author=Bordeaux Métropole;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee;">
<meta name="citation_reference" content="citation_title=Boom Register (Tree Register);,citation_author=Coöperatief Boomregister U.A.;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://boomregister.nl/;">
<meta name="citation_reference" content="citation_title=Challenges for computer vision as a tool for screening urban trees through street-view images;,citation_abstract=Urban forests play a fundamental and irreplaceable role within cities through the ecosystem services they provide, such as carbon capture. However, inadequate management of urban trees can heighten the risks they pose to society. For instance, mechanical failures of tree components, such as branches, can cause harm to individuals and property. Regular assessments of tree conditions are necessary to mitigate these tree-related hazards, yet such evaluations are labor-intensive and currently lack automation. Previous studies have proposed utilizing street view images to alleviate tree inspection and shown the feasibility of visually inspecting trees. However, only a limited number of studies have addressed the automatic evaluation of urban trees, a challenge that can potentially be addressed using deep learning networks. Particularly in urban environments, there is a pressing need for increased automation in unresolved computer vision tasks. Therefore, this research presents a comprehensive analysis of neural networks and publicly available datasets that can aid arborists in automatically identifying urban trees. Specifically, we investigate the potential of deep learning networks in classifying tree genera and segmenting individual trees and their trunks. We emphasize the utilization of transfer learning strategies to enhance tree identification. The results demonstrate that neural networks can be considered practical tools for assisting arborists in tree recognition. Nevertheless, there are still gaps that remain and require attention in future research endeavors.;,citation_author=Tito Arevalo-Ramirez;,citation_author=Anali Alfaro;,citation_author=José Figueroa;,citation_author=Mauricio Ponce-Donoso;,citation_author=Jose M. Saavedra;,citation_author=Matías Recabarren;,citation_author=José Delpiano;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S1618866724001146;,citation_doi=10.1016/j.ufug.2024.128316;,citation_issn=1618-8667;,citation_volume=95;,citation_journal_title=Urban Forestry &amp;amp;amp; Urban Greening;">
<meta name="citation_reference" content="citation_title=Olive tree biovolume from UAV multi-resolution image segmentation with mask r-CNN;,citation_abstract=Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.;,citation_author=Anastasiia Safonova;,citation_author=Emilio Guirado;,citation_author=Yuriy Maglinets;,citation_author=Domingo Alcaraz-Segura;,citation_author=Siham Tabik;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.mdpi.com/1424-8220/21/5/1617;,citation_issue=5;,citation_doi=10.3390/s21051617;,citation_issn=1424-8220;,citation_pmid=33668984;,citation_volume=21;,citation_journal_title=Sensors;">
<meta name="citation_reference" content="citation_title=Individual tree species identification for complex coniferous and broad-leaved mixed forests based on deep learning combined with UAV LiDAR data and RGB images;,citation_abstract=Automatic and accurate individual tree species identification is essential for the realization of smart forestry. Although existing studies have used unmanned aerial vehicle (UAV) remote sensing data for individual tree species identification, the effects of different spatial resolutions and combining multi-source remote sensing data for automatic individual tree species identification using deep learning methods still require further exploration, especially in complex forest conditions. Therefore, this study proposed an improved YOLOv8 model for individual tree species identification using multisource remote sensing data under complex forest stand conditions. Firstly, the RGB and LiDAR data of natural coniferous and broad-leaved mixed forests under complex conditions in Northeast China were acquired via a UAV. Then, different spatial resolutions, scales, and band combinations of multisource remote sensing data were explored, based on the YOLOv8 model for tree species identification. Subsequently, the Attention Multi-level Fusion (AMF) Gather-and-Distribute (GD) YOLOv8 model was proposed, according to the characteristics of the multisource remote sensing forest data, in which the two branches of the AMF Net backbone were able to extract and fuse features from multisource remote sensing data sources separately. Meanwhile, the GD mechanism was introduced into the neck of the model, in order to fully utilize the extracted features of the main trunk and complete the identification of eight individual tree species in the study area. The results showed that the YOLOv8x model based on RGB images combined with current mainstream object detection algorithms achieved the highest mAP of 75.3%. When the spatial resolution was within 8 cm, the accuracy of individual tree species identification exhibited only a slight variation. However, the accuracy decreased significantly with the decrease of spatial resolution when the resolution was greater than 15 cm. The identification results of different YOLOv8 scales showed that x, l, and m scales could exhibit higher accuracy compared with other scales. The DGB and PCA-D band combinations were superior to other band combinations for individual tree identification, with mAP of 75.5% and 76.2%, respectively. The proposed AMF GD YOLOv8 model had a more significant improvement in tree species identification accuracy than a single remote sensing sources and band combinations data, with a mAP of 81.0%. The study results clarified the impact of spatial resolution on individual tree species identification and demonstrated the excellent performance of the proposed AMF GD YOLOv8 model in individual tree species identification, which provides a new solution and technical reference for forestry resource investigation combined multisource remote sensing data.;,citation_author=Hao Zhong;,citation_author=Zheyu Zhang;,citation_author=Haoran Liu;,citation_author=Jinzhuo Wu;,citation_author=Wenshu Lin;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.mdpi.com/1999-4907/15/2/293;,citation_issue=2;,citation_doi=10.3390/f15020293;,citation_issn=1999-4907;,citation_volume=15;,citation_journal_title=Forests;">
<meta name="citation_reference" content="citation_title=A benchmark of lidar-based single tree detection methods using heterogeneous forest data from the alpine space;,citation_abstract=In this study, eight airborne laser scanning (ALS)-based single tree detection methods are benchmarked and investigated. The methods were applied to a unique dataset originating from different regions of the Alpine Space covering different study areas, forest types, and structures. This is the first benchmark ever performed for different forests within the Alps. The evaluation of the detection results was carried out in a reproducible way by automatically matching them to precise in situ forest inventory data using a restricted nearest neighbor detection approach. Quantitative statistical parameters such as percentages of correctly matched trees and omission and commission errors are presented. The proposed automated matching procedure presented herein shows an overall accuracy of 97%. Method based analysis, investigations per forest type, and an overall benchmark performance are presented. The best matching rate was obtained for single-layered coniferous forests. Dominated trees were challenging for all methods. The overall performance shows a matching rate of 47%, which is comparable to results of other benchmarks performed in the past. The study provides new insight regarding the potential and limits of tree detection with ALS and underlines some key aspects regarding the choice of method when performing single tree detection for the various forest types encountered in alpine regions.;,citation_author=Lothar Eysn;,citation_author=Markus Hollaus;,citation_author=Eva Lindberg;,citation_author=Frédéric Berger;,citation_author=Jean-Matthieu Monnet;,citation_author=Michele Dalponte;,citation_author=Milan Kobal;,citation_author=Marco Pellegrini;,citation_author=Emanuele Lingua;,citation_author=Domen Mongus;,citation_author=Norbert Pfeifer;,citation_publication_date=2015;,citation_cover_date=2015;,citation_year=2015;,citation_fulltext_html_url=https://www.mdpi.com/1999-4907/6/5/1721;,citation_issue=5;,citation_doi=10.3390/f6051721;,citation_issn=1999-4907;,citation_volume=6;,citation_journal_title=Forests;">
<meta name="citation_reference" content="citation_title=Individual tree crown delineation in high-resolution remote sensing images based on u-net;,citation_abstract=We present a deep learning-based framework for individual tree crown delineation in aerial and satellite images. This is an important task, e.g., for forest yield or carbon stock estimation. In contrast to earlier work, the presented method creates irregular polygons instead of bounding boxes and also provides a tree cover mask for areas that are not separable. Furthermore, it is trainable with low amounts of training data and does not need 3D height information from, e.g., laser sensors. We tested the approach in two scenarios: (1) with 30&nbsp;cm WorldView-3 satellite imagery from an urban region in Bengaluru, India, and (2) with 5&nbsp;cm aerial imagery of a densely forested area near Gartow, Germany. The intersection over union between the reference and predicted tree cover mask is 71.2% for the satellite imagery and 81.9% for the aerial images. On the polygon level, the method reaches an accuracy of 46.3% and a recall of 63.7% in the satellite images and an accuracy of 52% and recall of 66.2% in the aerial images, which is comparable to previous works that only predicted bounding boxes. Depending on the image resolution, limitations to separate individual tree crowns occur in situations where trees are hardly separable even for human image interpreters (e.g., homogeneous canopies, very small trees). The results indicate that the presented approach can efficiently delineate individual tree crowns in high-resolution optical images. Given the high availability of such imagery, the framework provides a powerful tool for tree monitoring. The source code and pretrained weights are publicly available at https://github.com/AWF-GAUG/TreeCrownDelineation.;,citation_author=Maximilian Freudenberg;,citation_author=Paul Magdon;,citation_author=Nils Nölke;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_issue=24;,citation_doi=10.1007/s00521-022-07640-4;,citation_issn=1433-3058;,citation_volume=34;,citation_journal_title=Neural Computing and Applications;">
<meta name="citation_reference" content="citation_title=DeepForest: A python package for RGB deep learning tree crown delineation;,citation_abstract=Abstract Remote sensing of forested landscapes can transform the speed, scale and cost of forest research. The delineation of individual trees in remote sensing images is an essential task in forest analysis. Here we introduce a new Python package, DeepForest that detects individual trees in high resolution RGB imagery using deep learning. While deep learning has proven highly effective in a range of computer vision tasks, it requires large amounts of training data that are typically difficult to obtain in ecological studies. DeepForest overcomes this limitation by including a model pretrained on over 30 million algorithmically generated crowns from 22 forests and fine-tuned using 10,000 hand-labelled crowns from six forests. The package supports the application of this general model to new data, fine tuning the model to new datasets with user labelled crowns, training new models and evaluating model predictions. This simplifies the process of using and retraining deep learning models for a range of forests, sensors and spatial resolutions. We illustrate the workflow of DeepForest using data from the National Ecological Observatory Network, a tropical forest in French Guiana, and street trees from Portland, Oregon.;,citation_author=Ben G. Weinstein;,citation_author=Sergio Marconi;,citation_author=Mélaine Aubry-Kientz;,citation_author=Gregoire Vincent;,citation_author=Henry Senyondo;,citation_author=Ethan P. White;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://besjournals.onlinelibrary.wiley.com/doi/abs/10.1111/2041-210X.13472;,citation_issue=12;,citation_doi=10.1111/2041-210X.13472;,citation_volume=11;,citation_journal_title=Methods in Ecology and Evolution;">
<meta name="citation_reference" content="citation_title=Data for the NeonTreeEvaluation benchmark (0.2.2);,citation_author=Ben Weinstein;,citation_author=Sergio Marconi;,citation_author=Ethan White;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_doi=10.5281/zenodo.5914554;,citation_publisher=Zenodo;">
<meta name="citation_reference" content="citation_title=International benchmarking of the individual tree detection methods for modeling 3-d canopy structure for silviculture and forest ecology using airborne laser scanning;,citation_author=Yunsheng Wang;,citation_author=Juha Hyyppä;,citation_author=Xinlian Liang;,citation_author=Harri Kaartinen;,citation_author=Xiaowei Yu;,citation_author=Eva Lindberg;,citation_author=Johan Holmgren;,citation_author=Yuchu Qin;,citation_author=Clément Mallet;,citation_author=António Ferraz;,citation_author=Hossein Torabzadeh;,citation_author=Felix Morsdorf;,citation_author=Lingli Zhu;,citation_author=Jingbin Liu;,citation_author=Petteri Alho;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_issue=9;,citation_doi=10.1109/TGRS.2016.2543225;,citation_volume=54;,citation_journal_title=IEEE Transactions on Geoscience and Remote Sensing;">
<meta name="citation_reference" content="citation_title=Individual tree crown segmentation and crown width extraction from a heightmap derived from aerial laser scanning data using a deep learning framework;,citation_abstract=Deriving individual tree crown (ITC) information from light detection and ranging (LiDAR) data is of great significance to forest resource assessment and smart management. After proof-of-concept studies, advanced deep learning methods have been shown to have high efficiency and accuracy in remote sensing data analysis and geoscience problem solving. This study proposes a novel concept for synergetic use of the YOLO-v4 deep learning network based on heightmaps directly generated from airborne LiDAR data for ITC segmentation and a computer graphics algorithm for refinement of the segmentation results involving overlapping tree crowns. This concept overcomes the limitations experienced by existing ITC segmentation methods that use aerial photographs to obtain texture and crown appearance information and commonly encounter interference due to heterogeneous solar illumination intensities or interlacing branches and leaves. Three generative adversarial networks (WGAN, CycleGAN, and SinGAN) were employed to generate synthetic images. These images were coupled with manually labeled training samples to train the network. Three forest plots, namely, a tree nursery, forest landscape and mixed tree plantation, were used to verify the effectiveness of our approach. The results showed that the overall recall of our method for detecting ITCs in the three forest plot types reached 83.6%, with an overall precision of 81.4%. Compared with reference field measurement data, the coefficient of determination &amp;amp;amp;lt;italic&amp;gt;R&amp;lt;/italic&amp;gt;<sup>2</sup> was ≥ 79.93% for tree crown width estimation, and the accuracy of our deep learning method was not influenced by the values of key parameters, yielding 3.9% greater accuracy than the traditional watershed method. The results demonstrate an enhancement of tree crown segmentation in the form of a heightmap for different forest plot types using the concept of deep learning, and our method bypasses the visual complications arising from aerial images featuring diverse textures and unordered scanned points with irregular geometrical properties.;,citation_author=Chenxin Sun;,citation_author=Chengwei Huang;,citation_author=Huaiqing Zhang;,citation_author=Bangqian Chen;,citation_author=Feng An;,citation_author=Liwen Wang;,citation_author=Ting Yun;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.frontiersin.org/journals/plant-science/articles/10.3389/fpls.2022.914974;,citation_doi=10.3389/fpls.2022.914974;,citation_issn=1664-462X;,citation_volume=13;,citation_journal_title=Frontiers in Plant Science;">
<meta name="citation_reference" content="citation_title=Albumentations: Fast and flexible image augmentations;,citation_author=Alexander Buslaev;,citation_author=Vladimir I. Iglovikov;,citation_author=Eugene Khvedchenya;,citation_author=Alex Parinov;,citation_author=Mikhail Druzhinin;,citation_author=Alexandr A. Kalinin;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://www.mdpi.com/2078-2489/11/2/125;,citation_issue=2;,citation_doi=10.3390/info11020125;,citation_issn=2078-2489;,citation_volume=11;,citation_journal_title=Information;">
<meta name="citation_reference" content="citation_title=Deep learning for LiDAR point cloud classification in remote sensing;,citation_author=Ahmed Diab;,citation_author=Rasha Kashef;,citation_author=Ahmed Shaker;,citation_publication_date=2022-10;,citation_cover_date=2022-10;,citation_year=2022;,citation_issue=20;,citation_doi=10.3390/s22207868;,citation_pmid=36298220;,citation_volume=22;,citation_journal_title=Sensors (Basel);">
<meta name="citation_reference" content="citation_title=Individual tree segmentation and tree species classification in subtropical broadleaf forests using UAV-based LiDAR, hyperspectral, and ultrahigh-resolution RGB data;,citation_abstract=Accurate classification of individual tree species is essential for inventorying, managing, and protecting forest resources. Individual tree species classification in subtropical forests remains challenging as existing individual tree segmentation algorithms typically result in over-segmentation in subtropical broadleaf forests, in which tree crowns often have multiple peaks. In this study, we proposed a watershed-spectral-texture-controlled normalized cut (WST-Ncut) algorithm, and applied it to delineate individual trees in a subtropical broadleaf forest situated in Shenzhen City of southern China (114°23′28″E, 22°43′50″N). Using this algorithm, we first obtained accurate crown boundary of individual broadleaf trees. We then extracted different suites of vertical structural, spectral, and textural features from UAV-based LiDAR, hyperspectral, and ultrahigh-resolution RGB data, and used these features as inputs to a random forest classifier to classify 18 tree species. The results showed that the proposed WST-Ncut algorithm could reduce the over-segmentation of the watershed segmentation algorithm, and thereby was effective for delineating individual trees in subtropical broadleaf forests (Recall&nbsp;=&nbsp;0.95, Precision&nbsp;=&nbsp;0.86, and F-score&nbsp;=&nbsp;0.91). Combining the structural, spectral, and textural features of individual trees provided the best tree species classification results, with overall accuracy reaching 91.8%, which was 10.2%, 13.6%, and 19.0% higher than that of using spectral, structural, and textural features alone, respectively. In addition, results showed that better individual tree segmentation would lead to higher accuracy of tree species classification, but the increase of the number of tree species would result in the decline of classification accuracy.;,citation_author=Haiming Qin;,citation_author=Weiqi Zhou;,citation_author=Yang Yao;,citation_author=Weimin Wang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0034425722002577;,citation_doi=10.1016/j.rse.2022.113143;,citation_issn=0034-4257;,citation_volume=280;,citation_journal_title=Remote Sensing of Environment;">
<meta name="citation_reference" content="citation_title=ACE r-CNN: An attention complementary and edge detection-based instance segmentation algorithm for individual tree species identification using UAV RGB images and LiDAR data;,citation_abstract=Accurate and automatic identification of tree species information at the individual tree scale is of great significance for fine-scale investigation and management of forest resources and scientific assessment of forest ecosystems. Despite the fact that numerous studies have been conducted on the delineation of individual tree crown and species classification using drone high-resolution red, green and blue (RGB) images, and Light Detection and Ranging (LiDAR) data, performing the above tasks simultaneously has rarely been explored, especially in complex forest environments. In this study, we improve upon the state of the Mask region-based convolution neural network (Mask R-CNN) with our proposed attention complementary network (ACNet) and edge detection R-CNN (ACE R-CNN) for individual tree species identification in high-density and complex forest environments. First, we propose ACNet as the feature extraction backbone network to fuse the weighted features extracted from RGB images and canopy height model (CHM) data through an attention complementary module, which is able to selectively fuse weighted features extracted from RGB and CHM data at different scales, and enables the network to focus on more effective information. Second, edge loss is added to the loss function to improve the edge accuracy of the segmentation, which is calculated through the edge detection filter introduced in the Mask branch of Mask R-CNN. We demonstrate the performance of ACE R-CNN for individual tree species identification in three experimental areas of different tree species in southern China with precision (P), recall (R), F1-score, and average precision (AP) above 0.9. Our proposed ACNet–the backbone network for feature extraction–has better performance in individual tree species identification compared with the ResNet50-FPN (feature pyramid network). The addition of the edge loss obtained by the Sobel filter further improves the identification accuracy of individual tree species and accelerates the convergence speed of the model training. This work demonstrates the improved performance of ACE R-CNN for individual tree species identification and provides a new solution for tree-level species identification in complex forest environments, which can support carbon stock estimation and biodiversity assessment.;,citation_author=Yingbo Li;,citation_author=Guoqi Chai;,citation_author=Yueting Wang;,citation_author=Lingting Lei;,citation_author=Xiaoli Zhang;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.mdpi.com/2072-4292/14/13/3035;,citation_issue=13;,citation_doi=10.3390/rs14133035;,citation_issn=2072-4292;,citation_volume=14;,citation_journal_title=Remote Sensing;">
<meta name="citation_reference" content="citation_title=Watersheds in digital spaces: An efficient algorithm based on immersion simulations;,citation_author=L. Vincent;,citation_author=P. Soille;,citation_publication_date=1991;,citation_cover_date=1991;,citation_year=1991;,citation_issue=6;,citation_doi=10.1109/34.87344;,citation_volume=13;,citation_journal_title=IEEE Transactions on Pattern Analysis and Machine Intelligence;">
<meta name="citation_reference" content="citation_title=Detection of individual trees and estimation of tree height using LiDAR data;,citation_abstract=For estimation of tree parameters at the single-tree level using light detection and ranging (LiDAR), detection and delineation of individual trees is an important starting point. This paper presents an approach for delineating individual trees and estimating tree heights using LiDAR in coniferous (Pinus koraiensis, Larix leptolepis) and deciduous (Quercus spp.) forests in South Korea. To detect tree tops, the extended maxima transformation of morphological image-analysis methods was applied to the digital canopy model (DCM). In order to monitor spurious local maxima in the DCM, which cause false tree tops, different h values in the extended maxima transformation were explored. For delineation of individual trees, watershed segmentation was applied to the distance-transformed image from the detected tree tops. The tree heights were extracted using the maximum value within the segmented crown boundary. Thereafter, individual tree data estimated by LiDAR were compared to the field measurement data under five categories (correct delineation, satisfied delineation, merged tree, split tree, and not found). In our study, P. koraiensis, L. leptolepis, and Quercus spp. had the best detection accuracies of 68.1% at h = 0.18, 86.7% at h = 0.12, and 67.4% at h = 0.02, respectively. The coefficients of determination for tree height estimation were 0.77, 0.80, and 0.74 for P. koraiensis, L. leptolepis, and Quercus spp., respectively.;,citation_author=Doo-Ahn Kwak;,citation_author=Woo-Kyun Lee;,citation_author=Jun-Hak Lee;,citation_author=Greg S. Biging;,citation_author=Peng Gong;,citation_publication_date=2007;,citation_cover_date=2007;,citation_year=2007;,citation_issue=6;,citation_doi=10.1007/s10310-007-0041-9;,citation_issn=1610-7403;,citation_volume=12;,citation_journal_title=Journal of Forest Research;">
<meta name="citation_reference" content="citation_title=Detection of tree crowns in very high spatial resolution images;,citation_author=Marilia Ferreira Gomes;,citation_author=Philippe Maillard;,citation_editor=Maged Marghany;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_doi=10.5772/62122;,citation_inbook_title=Environmental applications of remote sensing;">
<meta name="citation_reference" content="citation_title=Local maximum filtering for the extraction of tree locations and basal area from high spatial resolution imagery;,citation_abstract=In this study we investigate the use of local maximum (LM) filtering to locate trees on high spatial resolution (1-m) imagery. Results are considered in terms of commission error (falsely indicated trees) and omission error (missed trees). Tree isolation accuracy is also considered as a function of tree crown size. The success of LM filtering in locating trees depends on the size and distribution of trees in relation to the image spatial resolution. A static-sized 3×3 pixel LM filter provides an indication of the maximum number of trees that may be found in the imagery, yet high errors of commission reduce the integrity of the results. Variable window-size techniques may be applied to reduce both the errors of commission and omission, especially for larger trees. The distribution of the error by tree size is important since the large trees account for a greater proportion of the stand basal area than the smaller trees. An investigation of the success of tree identification by tree crown radius demonstrates the relationship between image spatial resolution and LM filtering success. At an image spatial resolution of 1 m, a tree crown radius of 1.5 m appears to be the minimum size for reliable identification of tree locations using LM filtering.;,citation_author=Mike Wulder;,citation_author=K.Olaf Niemann;,citation_author=David G. Goodenough;,citation_publication_date=2000;,citation_cover_date=2000;,citation_year=2000;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S0034425700001012;,citation_issue=1;,citation_doi=10.1016/S0034-4257(00)00101-2;,citation_issn=0034-4257;,citation_volume=73;,citation_journal_title=Remote Sensing of Environment;">
<meta name="citation_reference" content="citation_title=Automatic individual tree crown delineation using a valley-following algorithm and rule-based system;,citation_author=François A Gougeon;,citation_author=others;,citation_publication_date=1998;,citation_cover_date=1998;,citation_year=1998;,citation_fulltext_html_url=https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf;,citation_conference_title=Proc. International forum on automated interpretation of high spatial resolution digital imagery for forestry, victoria, british columbia, canada;,citation_conference=Citeseer;">
<meta name="citation_reference" content="citation_title=The automatic recognition of individual trees in aerial images of forests based on a synthetic tree crown image model;,citation_abstract=The thesis of this work is that individual tree crowns can be automatically recognized in monocular high spatial resolution optical images of scenes containing boreal or cool temperate forests in a leaved state. The thesis was advanced by developing and testing an automatic tree crown recognition procedure that is based on a model of the image formation process at the scale of an individual tree. This model provides a means of applying specific scene and image formation knowledge to the recognition task. The procedure associates instances of a three-dimensional shape description with locations in a scene image such that the descriptions estimate the visible scene extent of tree crowns that existed at the corresponding scene locations when the image was acquired. This provides an estimate of the average horizontal diameter of the vertical projection of individual recognized tree crowns, and a basis for species classification. This work makes a contribution to the overall effort to increase the level of automation in forest type mapping. This work also introduces and demonstrates the use of a pre-defined image model to support the manual acquisition of a sample of unmodelled tree crown image properties, and the use of constraints related to the spatial relationships among multiple neighbouring candidate recognition instances to resolve image interpretation conflicts.The procedure was tested with a scene of mixed uneven-aged forests in which the trees represent a wide variety of species, size, and growing conditions. The results were assessed on the basis of ground reference data and compared to those produced by human interpreters. The scene represented a greater level of difficulty than that which has been addressed by previous attempts at automating the tree crown recognition task. The test results show that the procedure was able to largely accommodate the variation represented by the test scene, but that human interpreters were better able to accommodate irregularities in tree crown form and irradiance that were caused by tight vertical and horizontal spacing of the crowns.;,citation_author=Richard James Pollock;,citation_publication_date=1996;,citation_cover_date=1996;,citation_year=1996;,citation_fulltext_html_url=https://dx.doi.org/10.14288/1.0051597;,citation_isbn=0612148157;,citation_dissertation_institution=The University of British Columbia (Canada);">
<meta name="citation_reference" content="citation_title=You only look once: Unified, real-time object detection;,citation_author=Joseph Redmon;,citation_author=Santosh Divvala;,citation_author=Ross Girshick;,citation_author=Ali Farhadi;,citation_publication_date=2016;,citation_cover_date=2016;,citation_year=2016;,citation_doi=10.1109/CVPR.2016.91;,citation_conference_title=2016 IEEE conference on computer vision and pattern recognition (CVPR);">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Tree object detection using airborne images and LiDAR point clouds</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Alexandre Bry <a href="mailto:alexandre.bry.21@polytechnique.org" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        <a href="https://portail.polytechnique.edu/informatique/fr/page-daccueil">
                        École polytechnique
                        </a>
                      </p>
                    <p class="affiliation">
                        <a href="https://research.geodan.nl/">
                        Geodan B.V.
                        </a>
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">July 22, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index_typst.pdf"><i class="bi bi-file-pdf"></i>Typst</a></p></div><div class="quarto-title-meta-contents"><p><a href="index_pdf.pdf"><i class="bi bi-file-pdf"></i>PDF</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This is the abstract. It can be on multiple lines and contain <strong>Markdown</strong>.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>tree detection, deep learning</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#state-of-the-art" id="toc-state-of-the-art" class="nav-link" data-scroll-target="#state-of-the-art"><span class="header-section-number">1</span> State-of-the-art</a>
  <ul class="collapse">
  <li><a href="#computer-vision-tasks-related-to-trees" id="toc-computer-vision-tasks-related-to-trees" class="nav-link" data-scroll-target="#computer-vision-tasks-related-to-trees"><span class="header-section-number">1.1</span> Computer vision tasks related to trees</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets"><span class="header-section-number">1.2</span> Datasets</a></li>
  <li><a href="#algorithms-and-models" id="toc-algorithms-and-models" class="nav-link" data-scroll-target="#algorithms-and-models"><span class="header-section-number">1.3</span> Algorithms and models</a></li>
  </ul></li>
  <li><a href="#objectives-and-motivations" id="toc-objectives-and-motivations" class="nav-link" data-scroll-target="#objectives-and-motivations"><span class="header-section-number">2</span> Objectives and motivations</a>
  <ul class="collapse">
  <li><a href="#data-and-model" id="toc-data-and-model" class="nav-link" data-scroll-target="#data-and-model"><span class="header-section-number">2.1</span> Data and model</a></li>
  <li><a href="#sec-obj-covered_trees" id="toc-sec-obj-covered_trees" class="nav-link" data-scroll-target="#sec-obj-covered_trees"><span class="header-section-number">2.2</span> Covered trees</a></li>
  <li><a href="#multiple-layers-of-chm" id="toc-multiple-layers-of-chm" class="nav-link" data-scroll-target="#multiple-layers-of-chm"><span class="header-section-number">2.3</span> Multiple layers of CHM</a></li>
  </ul></li>
  <li><a href="#sec-dataset" id="toc-sec-dataset" class="nav-link" data-scroll-target="#sec-dataset"><span class="header-section-number">3</span> Dataset creation</a>
  <ul class="collapse">
  <li><a href="#definition-and-content" id="toc-definition-and-content" class="nav-link" data-scroll-target="#definition-and-content"><span class="header-section-number">3.1</span> Definition and content</a></li>
  <li><a href="#sec-dataset-challenges" id="toc-sec-dataset-challenges" class="nav-link" data-scroll-target="#sec-dataset-challenges"><span class="header-section-number">3.2</span> Challenges and solutions</a></li>
  <li><a href="#augmentation-methods" id="toc-augmentation-methods" class="nav-link" data-scroll-target="#augmentation-methods"><span class="header-section-number">3.3</span> Augmentation methods</a></li>
  </ul></li>
  <li><a href="#model-and-training" id="toc-model-and-training" class="nav-link" data-scroll-target="#model-and-training"><span class="header-section-number">4</span> Model and training</a>
  <ul class="collapse">
  <li><a href="#model-architecture" id="toc-model-architecture" class="nav-link" data-scroll-target="#model-architecture"><span class="header-section-number">4.1</span> Model architecture</a></li>
  <li><a href="#training-pipeline" id="toc-training-pipeline" class="nav-link" data-scroll-target="#training-pipeline"><span class="header-section-number">4.2</span> Training pipeline</a></li>
  </ul></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results"><span class="header-section-number">5</span> Results</a>
  <ul class="collapse">
  <li><a href="#training-parameters" id="toc-training-parameters" class="nav-link" data-scroll-target="#training-parameters"><span class="header-section-number">5.1</span> Training parameters</a></li>
  <li><a href="#data-used" id="toc-data-used" class="nav-link" data-scroll-target="#data-used"><span class="header-section-number">5.2</span> Data used</a></li>
  <li><a href="#chm-layers" id="toc-chm-layers" class="nav-link" data-scroll-target="#chm-layers"><span class="header-section-number">5.3</span> CHM layers</a></li>
  <li><a href="#hard-trees" id="toc-hard-trees" class="nav-link" data-scroll-target="#hard-trees"><span class="header-section-number">5.4</span> Hard trees</a></li>
  </ul></li>
  <li><a href="#discussion" id="toc-discussion" class="nav-link" data-scroll-target="#discussion"><span class="header-section-number">6</span> Discussion</a>
  <ul class="collapse">
  <li><a href="#dataset" id="toc-dataset" class="nav-link" data-scroll-target="#dataset"><span class="header-section-number">6.1</span> Dataset</a></li>
  <li><a href="#combination-of-data-types" id="toc-combination-of-data-types" class="nav-link" data-scroll-target="#combination-of-data-types"><span class="header-section-number">6.2</span> Combination of data types</a></li>
  </ul></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
<div class="quarto-alternate-notebooks"><h2>Notebooks</h2><ul><li><a href="index-preview.html"><i class="bi bi-journal-code"></i>Article Notebook</a></li></ul></div></nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="introduction">Introduction</h2>
<p>The goal of the internship was to study the possibility of combining LiDAR point clouds and aerial images in a deep learning model to identify individual trees. The two types of data are indeed complementary, as point clouds capture geometric shapes, while images capture colors. However, combining them into a format that allows a model to handle them simultaneously is not a straightforward task because they inherently have a very different spatial repartition and encoding.</p>
<p>In this work, I focused on one specific deep learning model, and tried to improve it by using more information from the LiDAR point cloud. To do this, I had to create my own tree annotations dataset, with which I also tried to study the ability of this new model to detect trees that are covered by other trees.</p>
</section>
<section id="state-of-the-art" class="level2" data-number="1">
<h2 data-number="1" class="anchored" data-anchor-id="state-of-the-art"><span class="header-section-number">1</span> State-of-the-art</h2>
<section id="computer-vision-tasks-related-to-trees" class="level3" data-number="1.1">
<h3 data-number="1.1" class="anchored" data-anchor-id="computer-vision-tasks-related-to-trees"><span class="header-section-number">1.1</span> Computer vision tasks related to trees</h3>
<p>Before talking about models and datasets, let’s define properly the task that this project focused on, in the midst of all the various computer vision tasks, and specifically those related to tree detection.</p>
<p>The first main differentiation between tree recognition tasks comes from the acquisition of the data. There are some very different tasks and methods using either ground data or aerial/satellite data. This is especially true when focusing on urban trees, since a lot of street view data is available <span class="citation" data-cites="urban-trees">(<a href="#ref-urban-trees" role="doc-biblioref">Arevalo-Ramirez et al. 2024</a>)</span>.</p>
<p>This leads to the second variation, which is related to the kind of environment that we are interested in. There are mainly three types of environments, which among other things, influence the organization of the trees in space: urban areas, tree plantations and forests. This is important, because the tasks and the difficulty depends on the type of environment. Tree plantations are much easier to work with than completely wild forests, while urban areas contain various levels of difficulty ranging from alignment trees to private and disorganized gardens and parks. For this project, we mainly focused on urban areas, but everything should still be applicable to tree plantations and forests.</p>
<p>Then, the four fundamental computer vision tasks have their application when dealing with trees <span class="citation" data-cites="olive-tree">(<a href="#ref-olive-tree" role="doc-biblioref">Safonova et al. 2021</a>)</span>:</p>
<ul>
<li>Classification, although this is quite rare for airborne tree applications since there are multiple trees on each image most of the time</li>
<li>Detection, which consists in detecting objects and placing boxes around them</li>
<li>Semantic segmentation, which consists in associating a label to every pixel of an image</li>
<li>Instance segmentation, which consists in adding a layer of complexity to semantic segmentation by also differentiating between the different instances of each class</li>
</ul>
<p>These generic tasks can be extended by trying to get more information about the trees. The most common information are the species and the height, but some models also try to predict the health of the trees, or their carbon stock.</p>
<p>In this work, the task that is tackled is the detection of trees, with a special classification between several labels related to the discrepancies between the different kinds of data. The kind of model that is used would also have allowed to focus on some more advanced tasks, by replacing detection with instance segmentation and asking the model to also predict the species. But due to the difficulties regarding the dataset, a simpler task with a simpler dataset was used, without compromising the ability to experiment with different possible improvements of the model. The difficulties and the experiments are developed below.</p>
</section>
<section id="datasets" class="level3" data-number="1.2">
<h3 data-number="1.2" class="anchored" data-anchor-id="datasets"><span class="header-section-number">1.2</span> Datasets</h3>
<section id="sec-sota-datasets-requirements" class="level4" data-number="1.2.1">
<h4 data-number="1.2.1" class="anchored" data-anchor-id="sec-sota-datasets-requirements"><span class="header-section-number">1.2.1</span> Requirements</h4>
<p>Before presenting the different promising datasets and the reasons why they were not fully usable for the project, let’s enumerate the different conditions and requirements for the tree instance segmentation task:</p>
<ul>
<li>Multiple types of data:
<ul>
<li>Aerial RGB images</li>
<li>LiDAR point clouds (preferably aerial)</li>
<li>(Optional) Aerial infrared (CIR) images</li>
</ul></li>
<li>Tree crown annotations or bounding boxes</li>
<li>High-enough resolution:
<ul>
<li>For images, about 25&nbsp;cm</li>
<li>For point clouds, about 10&nbsp;cm</li>
</ul></li>
</ul>
<p>Here are the explanations for these requirements. As for the types of data, RGB images and point clouds are required to experiment on the ability of the model to combine the two very different kinds of information they hold. Having infrared data as well could be beneficial, but it was not necessary. Regarding tree annotations, it was necessary to have a way to spatially identify them individually, using crown contours or simply bounding boxes. Since the model outputs bounding boxes, any kind of other format could easily be transformed to bounding boxes. Finally, the resolution had to be high enough to identify individual trees and be able to really use the data. For the point clouds especially, the whole idea was to see if and how the topology of the trees could be learnt, using at least the trunks and even the biggest branches if possible. Therefore, even if they are not really comparable, this is the reason why the required resolution is more precise for the point clouds.</p>
<p>Unfortunately, none of the datasets that I found matched all these criteria. Furthermore, I didn’t find any overlapping datasets that I could merge to create a dataset with all the required types of data. In the next parts, I will go through the different kinds of datasets that exist, the reasons why they did not really fit for the project and the ideas I got when searching for a way to use them.</p>
</section>
<section id="existing-tree-datasets" class="level4" data-number="1.2.2">
<h4 data-number="1.2.2" class="anchored" data-anchor-id="existing-tree-datasets"><span class="header-section-number">1.2.2</span> Existing tree datasets</h4>
<p>As explained above, there were quite a lot of requirements to fulfill to have a complete dataset usable for the task. This means that almost all the available datasets were missing something, as they were mainly focusing on using one kind of data and trying to make the most out of it, instead of trying to use all the types of data together.</p>
<p>The most comprehensive list of tree annotations datasets was published in OpenForest <span class="citation" data-cites="OpenForest">(<a href="#ref-OpenForest" role="doc-biblioref">Ouaknine et al. 2023</a>)</span>. FoMo-Bench <span class="citation" data-cites="FoMo-Bench">(<a href="#ref-FoMo-Bench" role="doc-biblioref">Bountos, Ouaknine, and Rolnick 2023</a>)</span> also lists several interesting datasets, even though most of them can also be found in OpenForest. Without enumerating all of them, there were multiple kinds of datasets that all have their own flaws regarding the requirements I was looking for.</p>
<p>Firstly, there are the forest inventories. TALLO <span class="citation" data-cites="TALLO">(<a href="#ref-TALLO" role="doc-biblioref">Jucker et al. 2022</a>)</span> is probably the most interesting one in this category, because it contains a lot of spatial information about almost 500K trees, with their locations, their crown radii and their heights. Therefore, everything needed to localize trees is in the dataset. However, I didn’t manage to find RGB images or LiDAR point clouds of the areas where the trees are located, making it impossible to use these annotations to train tree detection.</p>
<p>Secondly, there are the RGB datasets. ReforesTree <span class="citation" data-cites="ReforesTree">(<a href="#ref-ReforesTree" role="doc-biblioref">Reiersen et al. 2022</a>)</span> and MillionTrees <span class="citation" data-cites="MillionTrees">(<a href="#ref-MillionTrees" role="doc-biblioref">B. Weinstein 2023</a>)</span> are two of them and the quality of their images are high. The only drawback of these datasets is obviously that they don’t provide any kind of point cloud, which make them unsuitable for the task.</p>
<p>Thirdly, there are the LiDAR datasets, such as <span class="citation" data-cites="WildForest3D">(<a href="#ref-WildForest3D" role="doc-biblioref">Kalinicheva et al. 2022</a>)</span> and <span class="citation" data-cites="FOR-instance">(<a href="#ref-FOR-instance" role="doc-biblioref">Puliti et al. 2023</a>)</span>. Similarly to RGB datasets, they lack one of the data source for the task I worked on. But unlike them, they have the advantage that the missing data could be much easier to acquire from another source, since RGB aerial or satellite images are much more common than LiDAR point clouds. However, this solution was abandoned for two main reasons. First it is quite challenging to find the exact locations where the point clouds were acquired. Then, even when the location is known, it is often in the middle of a forest where the quality of satellite imagery is very low.</p>
<p>Finally, I also found two datasets that had RGB and LiDAR components. The first one is MDAS <span class="citation" data-cites="MDAS">(<a href="#ref-MDAS" role="doc-biblioref">Hu et al. 2023</a>)</span>. This benchmark dataset encompasses RGB images, hyperspectral images and Digital Surface Models (DSM). There were however two major flaws. The obvious one was that this dataset was created with land semantic segmentation tasks in mind, so there was no tree annotations. The less obvious one was that a DSM is not a point cloud, even though it is some kind of 3D information and was often created using a LiDAR point cloud. As a consequence, I would have been very limited in my ability to use the point cloud.</p>
<p>The only real dataset with RGB and LiDAR came from NEON <span class="citation" data-cites="NEONdata">(<a href="#ref-NEONdata" role="doc-biblioref">B. Weinstein, Marconi, and White 2022</a>)</span>. This dataset contains exactly all the data I was looking for, with RGB images, hyperspectral images and LiDAR point clouds. With 30975 tree annotations, it is also a quite large dataset, spanning across multiple various forests. The reason why I decided not to use it despite all this is that at the beginning of the project, I thought that the quality of the images and the point clouds was too low. Looking back on this decision, I think that I probably could have worked with this dataset and gotten great results. This would have saved me the time spent annotating the trees for my own dataset, which I will talk more about later. My decision was also influenced by the quality of the images and the point clouds available in the Netherlands, which I will talk about in the next section.</p>
</section>
<section id="public-data" class="level4" data-number="1.2.3">
<h4 data-number="1.2.3" class="anchored" data-anchor-id="public-data"><span class="header-section-number">1.2.3</span> Public data</h4>
<p>After rejecting all the available datasets I had found, the only solution I had left was to create my own dataset. I won’t dive too much in this process that I will explain in <a href="#sec-dataset" class="quarto-xref">Section&nbsp;3</a>. I just want to mention all the publicly available datasets that I used or could have used to create this custom dataset.</p>
<p>For practical reasons, the two countries where I mostly searched for available data are France and the Netherlands. I was looking for three different data types independently:</p>
<ul>
<li>RGB (and if possible CIR) images</li>
<li>LiDAR point clouds</li>
<li>Tree annotations</li>
</ul>
<p>These three types of data are available in similar ways in both countries, although the Netherlands have a small edge over France. RGB images are really easy to find in France with the BD ORTHO <span class="citation" data-cites="IGN_BD_ORTHO">(<a href="#ref-IGN_BD_ORTHO" role="doc-biblioref">Institut national de l’information géographique et forestière (IGN) 2021</a>)</span> and in the Netherlands with the Luchtfotos <span class="citation" data-cites="Luchtfotos">(<a href="#ref-Luchtfotos" role="doc-biblioref">Beeldmateriaal Nederland 2024</a>)</span>, but the resolution is better in the Netherlands (8&nbsp;cm vs 20&nbsp;cm). Hyperspectral images are also available in both countries, although for those the resolution is only 25&nbsp;cm in the Netherlands.</p>
<p>As for LiDAR point clouds, the Netherlands have a small edge over France, because they have already completed their forth version covering the whole country with AHN4 <span class="citation" data-cites="AHN4">(<a href="#ref-AHN4" role="doc-biblioref">Actueel Hoogtebestand Nederland 2020</a>)</span>, and are working on the fifth version. In France, data acquisition for the first LiDAR point cloud covering the whole country started a few years ago <span class="citation" data-cites="IGN_LiDAR_HD">(<a href="#ref-IGN_LiDAR_HD" role="doc-biblioref">Institut national de l’information géographique et forestière (IGN) 2020</a>)</span>. It is not yet finished, even though data is already available for half of the country. The other advantage of the data from Netherlands regarding LiDAR point clouds is that all flights are performed during winter, which allows light beams to penetrate more deeply in trees and reach trunks and branches. This is not the case in France.</p>
<p>The part that is missing in both countries is related to tree annotations. Many municipalities have datasets containing information about all the public trees they handle. This is for example the case for Amsterdam <span class="citation" data-cites="amsterdam_trees">(<a href="#ref-amsterdam_trees" role="doc-biblioref">Gemeente Amsterdam 2024</a>)</span> and Bordeaux <span class="citation" data-cites="bordeaux_trees">(<a href="#ref-bordeaux_trees" role="doc-biblioref">Bordeaux Métropole 2024</a>)</span>. However, these datasets cannot really be used as ground truth for a custom dataset for several reasons. First, many of them do not contain coordinates indicating the position of each tree in the city. Then, even those that contain coordinates are most of the time missing any kind of information allowing to deduce a bounding box for the tree crowns. Finally, even if they did contain everything, they only focus on public trees, and are missing every single tree located in a private area. Since public and private areas are obviously imbricated in all cities, it means that any area we try to train the model on would be missing all the private trees, making the training process impossible because we cannot have only a partial annotation of images.</p>
<p>The other tree annotation source that we could have used is Boomregister <span class="citation" data-cites="boomregister">(<a href="#ref-boomregister" role="doc-biblioref">Coöperatief Boomregister U.A. 2014</a>)</span>. This work covers the whole of the Netherlands, including public and private trees. However, the precision of the masks is far from perfect, and many trees are missing or incorrectly segmented, especially when they are less than 9&nbsp;m heigh or have a crown diameter smaller than 4&nbsp;m. Therefore, even it is a very impressive piece of work, we thought that it could not be used as training data for a deep learning models due to its biases and imperfections.</p>
</section>
<section id="sec-sota-dataset-augment" class="level4" data-number="1.2.4">
<h4 data-number="1.2.4" class="anchored" data-anchor-id="sec-sota-dataset-augment"><span class="header-section-number">1.2.4</span> Dataset augmentation techniques</h4>
<p>When a dataset is too small to train a model, there are several ways of artificially enlarging it.</p>
<p>The most common way to do it is to randomly apply deterministic or random transformations to the data, during the training process, to be able to generate several unique and different realistic data instances from one real data instance. There are a lot of different transformations that can be applied to images, divided into two categories: pixel-level and spatial-level <span class="citation" data-cites="albumentations">(<a href="#ref-albumentations" role="doc-biblioref">Buslaev et al. 2020</a>)</span>. Pixel-level transformations modify the value of individual pixels, by applying different filters, such as random noise, color shifts and more complex effects like fog and sun flare. Spatial-level transformations modify the spatial arrangement of the image, without changing the pixel values. In other words, these transformations move the pixels in the image. The transformations range from simple rotations and croppings to complex spatial distortions. In the end, all these transformations are simply producing one artificial image out of one real image.</p>
<p>Another way to enlarge a dataset is to instead generate completely new input data sharing the same properties as the initial dataset. This can be done using Generative Adversarial Networks (GAN). These models usually have two parts, a generator and a discriminator, which are trained in parallel. The generator learns to produce realistic artificial data, while the discriminator learns to identify real data and artificial data produced by the generator. If the training is successful, we can then use the generator and random seeds to generate random but realistic artificial data similar to the dataset. This method has for example been successfully used to generate artificial tree height maps <span class="citation" data-cites="gan_data_augment">(<a href="#ref-gan_data_augment" role="doc-biblioref">Sun et al. 2022</a>)</span>.</p>
</section>
</section>
<section id="algorithms-and-models" class="level3" data-number="1.3">
<h3 data-number="1.3" class="anchored" data-anchor-id="algorithms-and-models"><span class="header-section-number">1.3</span> Algorithms and models</h3>
<p>In this section, the different algorithms and methods are grouped according to the type of data they use as input.</p>
<section id="images-only" class="level4" data-number="1.3.1">
<h4 data-number="1.3.1" class="anchored" data-anchor-id="images-only"><span class="header-section-number">1.3.1</span> Images only</h4>
<p>Then, there are methods that perform tree detection using only visible or hyperspectral images or both. Several different algorithms have been developed to analytically delineate tree crowns from RGB images, by using the particular shape of the trees and its effect on images <span class="citation" data-cites="rgb_analytical">(<a href="#ref-rgb_analytical" role="doc-biblioref">Gomes and Maillard 2016</a>)</span>. Without diving into the details, here are a few of them. The watershed algorithm identifies trees to inverted watersheds in the grey-scale image and tree crowns frontiers are found by incrementally flooding the watersheds <span class="citation" data-cites="watershed">(<a href="#ref-watershed" role="doc-biblioref">Vincent and Soille 1991</a>)</span>. The local maxima filtering uses the intensity of the pixels in the grey-scale image to identify the brightest points locally and use them as treetops <span class="citation" data-cites="local-maximum">(<a href="#ref-local-maximum" role="doc-biblioref">Wulder, Niemann, and Goodenough 2000</a>)</span>. Reversely, the valley-following algorithm uses the darkest pixels which are considered as the junctions between the trees since shaded areas are the lower part of the tree crowns <span class="citation" data-cites="valley-following">(<a href="#ref-valley-following" role="doc-biblioref">Gougeon et al. 1998</a>)</span>. Another interesting algorithm is template matching. This algorithm simulates the appearance of simple tree templates with the light effects, and tries to identify similar patterns in the grey-scale image <span class="citation" data-cites="template-matching">(<a href="#ref-template-matching" role="doc-biblioref">Pollock 1996</a>)</span>. Combinations of these techniques and others have also been proposed.</p>
<p>But with the recent developments of deep learning in image analysis, deep learning models are increasingly used to detect trees using RGB images. In some cases, deep learning is used to extract features that can then be the input of one of the algorithms described above. One example is the use of two neural networks to predict masks, outlines and distance transforms which can then be the input of a watershed algorithm <span class="citation" data-cites="rgb-dl-watershed">(<a href="#ref-rgb-dl-watershed" role="doc-biblioref">Freudenberg, Magdon, and Nölke 2022</a>)</span>. In other cases, a deep learning model is responsible of directly detecting tree masks or bounding boxes, often using CNNs, given the images <span class="citation" data-cites="DeepForest">(<a href="#ref-DeepForest" role="doc-biblioref">B. G. Weinstein et al. 2020</a>)</span>.</p>
</section>
<section id="lidar-only" class="level4" data-number="1.3.2">
<h4 data-number="1.3.2" class="anchored" data-anchor-id="lidar-only"><span class="header-section-number">1.3.2</span> LiDAR only</h4>
<p>Some of the methods to identify individual trees use LiDAR data only. There are a lot of different ways to use and analyze point clouds, but the one that is mostly used for trees is based on height maps, or Canopy Height Models (CHM).</p>
<p>A CHM is a raster computed as the subtraction of the Digital Terrain Model (DTM) to the Digital Surface Model (DSM). What it means is that a CHM contains the height above ground of the highest point in the area corresponding to each pixel. This CHM can for example be used as the input raster for the watershed algorithm, as it contains the height values that can be used to determine local maxima <span class="citation" data-cites="lidar_watershed">(<a href="#ref-lidar_watershed" role="doc-biblioref">Kwak et al. 2007</a>)</span>. A lot of different analytical methods and variations of the simple CHM were proposed to perform individual tree detection, but in the end, most of them still the concept of local maxima <span class="citation" data-cites="lidar_benchmark lidar_benchmark_2">(<a href="#ref-lidar_benchmark" role="doc-biblioref">Eysn et al. 2015</a>; <a href="#ref-lidar_benchmark_2" role="doc-biblioref">Wang et al. 2016</a>)</span>. A CHM can also be used as the input of any kind of convolutional neural network (CNN) because it is shaped exactly like any image. This allows to use a lot of different techniques usually applied to object detection in images.</p>
<p>Then, even though I finally used an approach similar to the CHM, I want to mention other kinds of deep learning techniques that exist and could potentially leverage all the information contained in a point cloud. These techniques can be divided in two categories: projection-based and point-based methods <span class="citation" data-cites="lidar_classification">(<a href="#ref-lidar_classification" role="doc-biblioref">Diab, Kashef, and Shaker 2022</a>)</span>. The main difference between the two is that projection-based techniques are based on grids while point-based methods take unstructured point clouds as input. Among projection-based methods, the most basic method is 2D CNN, which is how CHM can be processed. Then, multiview representation tries to tackle the 3D aspect by projecting the point cloud in multiple directions before merging them together. To really deal with 3D data, volumetric grid representation consists in using 3D occupancy grids, which are processed using 3D CNNs. Among point-based methods, there are methods based on PointNet, which are able to extract features and perform the classical computer vision tasks by taking point clouds as input. Finally, Convolutional Point Networks use a continuous generalization of convolutions to apply convolution kernels to arbitrarily distributed point clouds.</p>
</section>
<section id="lidar-and-images" class="level4" data-number="1.3.3">
<h4 data-number="1.3.3" class="anchored" data-anchor-id="lidar-and-images"><span class="header-section-number">1.3.3</span> LiDAR and images</h4>
<!-- TODO: ADD OTHER MODELS -->
<p>Let’s now talk about the models of interest for this work, which are machine learning pipelines using both LiDAR point cloud data and RGB images.</p>
<p>The first pipeline <span class="citation" data-cites="lidar_rgb_wst">(<a href="#ref-lidar_rgb_wst" role="doc-biblioref">Qin et al. 2022</a>)</span> uses a watershed algorithm to extract crown boundaries, before extracting individual tree features from the LiDAR point cloud, hyperspectral and RGB images. These features are then used by a random forest classifier to identify which species the tree belongs to. This pipeline therefore makes the most out of all data to identify species, but sticks to an improved variant of the watershed for individual tree segmentation, which only uses a CHM raster.</p>
<p>Other works focused on using only one model that is able to take both the CHM and the RGB data as input and combine them to make the most out of all the available data. Among other models, there are for example ACE R-CNN <span class="citation" data-cites="lidar_rgb_acnet">(<a href="#ref-lidar_rgb_acnet" role="doc-biblioref">Li et al. 2022</a>)</span>, an evolution of Mask region-based convolution neural network (Mask R-CNN) and AMF GD YOLOv8 <span class="citation" data-cites="amf_gd_yolov8">(<a href="#ref-amf_gd_yolov8" role="doc-biblioref">Zhong et al. 2024</a>)</span>, an evolution of YOLOv8. These two models have proven to give much better results when using both the images and the LiDAR data as a CHM thant when using only one of them.</p>
</section>
</section>
</section>
<section id="objectives-and-motivations" class="level2" data-number="2">
<h2 data-number="2" class="anchored" data-anchor-id="objectives-and-motivations"><span class="header-section-number">2</span> Objectives and motivations</h2>
<p>In this section, I will explain the objectives that I set for this internship and the motivations that led to them.</p>
<section id="data-and-model" class="level3" data-number="2.1">
<h3 data-number="2.1" class="anchored" data-anchor-id="data-and-model"><span class="header-section-number">2.1</span> Data and model</h3>
<p>The basis for this internship was to look at deep learning models to detect trees using LiDAR and aerial images. In fourth months, it would have been difficult to dive into the literature, think about a completely new approach and develop it. Therefore, I wanted to find an interesting and not too complicated deep learning model, and try a few changes that would hopefully improve the results.</p>
<p>This idea was also reinforced by the decision to create my own dataset for two reasons. The first reason was the small number of openly available tree annotation datasets which contained both LiDAR and RGB data. I therefore thought that creating a new dataset and making it available could be a great contribution. The second reason was to have more control over the definition and the characteristics of the dataset, to be able to experiment on the detection of specific trees.</p>
</section>
<section id="sec-obj-covered_trees" class="level3" data-number="2.2">
<h3 data-number="2.2" class="anchored" data-anchor-id="sec-obj-covered_trees"><span class="header-section-number">2.2</span> Covered trees</h3>
<p>The main thing that I wanted to experiment on was the possibility to make a better use of the LiDAR point cloud to be able to detect covered trees. Covered trees are the trees which are located partially or completely under another tree’s crown. This makes them impossible to completely delineate when using only data that is visible from above. These trees are not meaningless or negligible, because as demonstrated in this paper <span class="citation" data-cites="lidar_benchmark_2">(<a href="#ref-lidar_benchmark_2" role="doc-biblioref">Wang et al. 2016</a>)</span>, they can represent up to 50% of the trees in a forest.</p>
<p>However, doing this implied being able to process them on the whole pipeline. In practice, covered trees are never annotated in all the datasets that are created using only RGB images, because they are simply not visible. This means that creating my own dataset was the only solution to have a dataset containing really all trees including covered trees and be able to easily identify them.</p>
</section>
<section id="multiple-layers-of-chm" class="level3" data-number="2.3">
<h3 data-number="2.3" class="anchored" data-anchor-id="multiple-layers-of-chm"><span class="header-section-number">2.3</span> Multiple layers of CHM</h3>
<p>Being able to find covered trees meant finding a way to find more information out of the LiDAR point cloud than what is contained by the CHM. In fact, the CHM only contains a very small part of the point cloud and doesn’t really benefit from the 3D shape that is contained in the point cloud. This is particularly true when the point cloud is acquired in a season where trees don’t have their leaves, because the LiDAR then goes deep into the tree more easily, and can find the trunk and many of the largest branches.</p>
<p>Therefore, getting information below the tree crown surface was mandatory to find covered trees. But it could also be helpful for the model to find better separations between each tree, thanks to having access to the branches and the trunks. Even though I didn’t end up asking the model to also identify the species, this is another task that could have been improved a lot if the model could use the architecture of the branches.</p>
<p>To do this, I wanted to stick with a simple solution that would integrate well with the initial model and wouldn’t require too many changes. The idea I implemented is therefore very simple. Instead of having only one CHM raster, I would have multiple layers, each focusing on a different height interval. There are many ways to do this, but due to a lack of time, I only really tried what seemed to me the easiest and most straightforward way to do it, which consists in removing all the points above a certain height threshold, and compute the CHM with the points that are left. When doing this for multiple height thresholds, we get an interesting view of what the point cloud looks like at multiple levels, which gives a lot more information about the organization of the point cloud. Another way to do this, which is used in the third method of this paper <span class="citation" data-cites="lidar_benchmark">(<a href="#ref-lidar_benchmark" role="doc-biblioref">Eysn et al. 2015</a>)</span>, would be instead to use the previous CHM by removing all the points that are in the interval between the CHM height and 0.5&nbsp;m below, before computing an additional layer. It could be interesting to see if this method works better than dropping the points at pre-determined heights.</p>
</section>
</section>
<section id="sec-dataset" class="level2" data-number="3">
<h2 data-number="3" class="anchored" data-anchor-id="sec-dataset"><span class="header-section-number">3</span> Dataset creation</h2>
<p>The highest resolution of the CHM which keeps a high enough quality depends entirely on the density of the point cloud. Also, depending on the season when the point cloud is acquired, using a CHM might imply throwing away the majority of the information contained in the point cloud.</p>
<section id="definition-and-content" class="level3" data-number="3.1">
<h3 data-number="3.1" class="anchored" data-anchor-id="definition-and-content"><span class="header-section-number">3.1</span> Definition and content</h3>
<p>As explained in the section <a href="#sec-sota-datasets-requirements" class="quarto-xref">Section&nbsp;1.2.1</a>, the main requirements of the dataset that I wanted to create were to contain at the same time LiDAR data, RGB data and CIR data, with simple bounding box annotations for all trees. And as explained in <a href="#sec-obj-covered_trees" class="quarto-xref">Section&nbsp;2.2</a>, all trees means also annotating trees that are partially or completely covered by other trees.</p>
<p>Then, to make the most out of the point cloud resolution and the RGB images resolution, I decided to use a CHM resolution of 8&nbsp;cm, which is also the resolution of the RGB images. However, the resolution of CIR images is 25&nbsp;cm, which made it less optimal, but still usable.</p>
<p>To be able to get results even with a small dataset, I decided to focus on one specific area, to limit the diversity of trees and environments to something that could hopefully still be learnt with a small dataset. Therefore, the whole dataset is currently inside of a 1 km&nbsp;×&nbsp;1&nbsp;km square around Geodan office, in Amsterdam. It contains 2726 annotated trees spread over 241 images of size 640&nbsp;px&nbsp;×&nbsp;640&nbsp;px i.e.&nbsp;51.2&nbsp;m&nbsp;×&nbsp;51.2&nbsp;m. All tree annotations have at least a bounding box, and some of them have a more accurate polygon representing the shape of the crown. There are four classes, which I will detail in the next section <a href="#sec-dataset-challenges" class="quarto-xref">Section&nbsp;3.2</a>, and each tree belongs to one class.</p>
<p>Annotating all these trees took me about 100 hours, with a very high variation of the time spent on each tree depending on the complexity of the area.</p>
</section>
<section id="sec-dataset-challenges" class="level3" data-number="3.2">
<h3 data-number="3.2" class="anchored" data-anchor-id="sec-dataset-challenges"><span class="header-section-number">3.2</span> Challenges and solutions</h3>
<p>The creation of this dataset raised a number of challenges. The first one was the interval of time between the acquisition of the different types of data. While the point cloud data dated from 2020, the RGB images were acquired in 2023. It would have been possible to use images from 2021 or 2022 with the same resolution, but the quality of the 2023 images was much better. Consequently, there were a certain amount of changes regarding trees between these two periods of acquisition. Some large trees were cut off, while small trees were planted, sometimes even at the position of old trees that were previously cut off in the same time frame. For this reason, a non negligible number of trees were either present only in the point cloud, or only in the images. To try to handle this situation, I created two new class labels corresponding to these situation. This amounted up to 4 class labels:</p>
<ul>
<li>“Tree”: trees which are visible in the point cloud and the images</li>
<li>“Tree_LiDAR”: trees which are visible in the point cloud only but would be visible in the images if they had been there during the acquisition</li>
<li>“Tree_RGB”: trees which are visible in the images only but would be visible in the point cloud if they had been there during the acquisition</li>
<li>“Tree_covered”: trees that are visible in the point cloud only because they are covered by other trees.</li>
</ul>
<p>All these labels theoretically correspond to situations that have no intersection, even though it is more complicated in practice.</p>
<p>The next challenge was the misalignment of images and point cloud. This misalignment comes from the images not being perfectly orthonormal. Point clouds don’t have this problem, because the data is acquired and represented in 3D, but images have to be projected to a 2D plane after being acquired with an angle that is not perfectly orthogonal to the plane. Despite the post-processing that was surely performed on the images, they are therefore not perfect, and there is a shift between the positions of each object in the point cloud and in the images. This shift cannot really be solved, because it depends on the position. Because of this misalignment, a choice had to be made as to where tree annotations should be placed, using either the point clouds or the RGB images. I chose to the RGB images as it is simpler to visualize and annotate, but there was not really a perfect choice.</p>
<!-- TODO: PUT AN IMAGE TO SHOW THE MISALIGNMENT HERE -->
<p>Finally, the last challenge comes from the definition of what we consider as a tree and what we don’t. There are two main sub-problems. The first one comes from the threshold to set between bushes and trees. Large bushes can be much larger than small trees, and sometimes have a similar shape. Therefore, it is hard to keep coherent rules when annotating them. The second sub-problem comes from multi-stemmed and close trees. It can be very difficult to see, even with the point cloud, if a there is only one tree with two or more trunks dividing at the bottom, or multiple trees which are simply close to one another. (Un)fortunately I know that I was not the only one to face this problem because it was also mentioned in another paper <span class="citation" data-cites="DeepForestBefore">(<a href="#ref-DeepForestBefore" role="doc-biblioref">B. G. Weinstein et al. 2019</a>)</span>. In the end, it was just an unsolvable problem for which the most important was to remain consistent in the whole dataset.</p>
</section>
<section id="augmentation-methods" class="level3" data-number="3.3">
<h3 data-number="3.3" class="anchored" data-anchor-id="augmentation-methods"><span class="header-section-number">3.3</span> Augmentation methods</h3>
<p>Dataset augmentation methods are in the middle between dataset creation and deep learning model training, because they are a way to enhance the dataset but depend on the objective for which the model is trained. Their importance is inversely proportional with the size of the dataset, which made them very important for my small dataset.</p>
<p>As it was already explained in <a href="#sec-sota-dataset-augment" class="quarto-xref">Section&nbsp;1.2.4</a>, I used Albumentations <span class="citation" data-cites="albumentations">(<a href="#ref-albumentations" role="doc-biblioref">Buslaev et al. 2020</a>)</span> to apply two types of augmentations: pixel-level and spatial-level.</p>
<p>Spatial-level augmentations had to be in the exact same way to the whole dataset, to maintain the spatial coherence between RGB images, CIR images and the CHM layers. I used three different spatial transformations, applied with random parameters. The first one chooses one of the eight possible images we can get when flipping and rotating the image by angles that are multiples of 90°. The second one adds a perspective effect to the images. The third one adds a small distortion to the image.</p>
<!-- TODO: Add an example of an image before and after the transformations -->
<p>On the contrary, Pixel-level augmentations must be applied differently to RGB images and CHM layers because they represent different kinds of data, so the values of the pixels do not have the same meaning. In practice, a lot of transformations were conceived to reproduce camera effects on RGB images or to shift the color spectrum. Among others, I used random modifications of the brightness, the gamma value and added noise and a blurring effect randomly to RGB images. For both types of data, a channel dropout is also randomly applied, leaving a random number of channels and removing the others. A better way to augment the CHM data would have been to apply random displacements and deletions of points in the point cloud, before computing the CHM layers. However, these operations are too costly to be integrated in the training pipeline without consequently increasing the training time, so this idea was discarded.</p>
</section>
</section>
<section id="model-and-training" class="level2" data-number="4">
<h2 data-number="4" class="anchored" data-anchor-id="model-and-training"><span class="header-section-number">4</span> Model and training</h2>
<p>The deep learning model that is used is based on AMF GD YOLOv8, the model proposed in this paper <span class="citation" data-cites="amf_gd_yolov8">(<a href="#ref-amf_gd_yolov8" role="doc-biblioref">Zhong et al. 2024</a>)</span>.</p>
<section id="model-architecture" class="level3" data-number="4.1">
<h3 data-number="4.1" class="anchored" data-anchor-id="model-architecture"><span class="header-section-number">4.1</span> Model architecture</h3>
<p>The architecture of the model is conceptually simple. The model takes two inputs in the form of two rasters with the same height and width. The two inputs are processed using the backbone of the YOLOv8 model <span class="citation" data-cites="yolo">(<a href="#ref-yolo" role="doc-biblioref">Redmon et al. 2016</a>)</span> to extract features at different scales. Then Attention Multi-level Fusion (AMF) layers are used to fuse the features of the two inputs at each scale level. Then, a Gather-and-Distribute (GD) mechanism is used to propagate information between the different scales. This mechanism fuses the features from all scales before redistributing them to the features, two times in a row. Finally, the features of the three smallest scales are fed into detection layers responsible for extracting bounding boxes and assigning confidence scores and class probabilities to them.</p>
<!-- TODO: Add a diagram with the model structure -->
<p>In practical terms, the input rasters have a shape of <span class="math inline">\(640&nbsp;\times&nbsp;640&nbsp;\times&nbsp;c_{\text{RGB}}\)</span> and <span class="math inline">\(640&nbsp;\times&nbsp;640&nbsp;\times&nbsp;c_{\text{CHM}}\)</span>, where <span class="math inline">\(c_{\text{RGB}}\)</span> is equal to 6 when using RGB and CIR images, and 3 when using only one of them, and <span class="math inline">\(c_{\text{CHM}}\)</span> is the number of CHM layers that we decide to use for the model. Since the resolution that is used is 0.08&nbsp;m, this means that each image spans over 51.2&nbsp;m.</p>
<p>The only real modification that I made to the architecture compared to the initial paper is adding any number of channels in the CHM input, while we had <span class="math inline">\(c_{\text{CHM}} = 1\)</span> originally. Using CIR images in addition to RGB images is also new, but this is a less important modification.</p>
</section>
<section id="training-pipeline" class="level3" data-number="4.2">
<h3 data-number="4.2" class="anchored" data-anchor-id="training-pipeline"><span class="header-section-number">4.2</span> Training pipeline</h3>
<p>The training pipeline consists of three steps. First, the data is pre-processed to create the inputs to feed into the model. Then, the training loop runs until the end condition is reached. Finally, the final model is evaluated on all the datasets.</p>
<section id="data-preprocessing" class="level4" data-number="4.2.1">
<h4 data-number="4.2.1" class="anchored" data-anchor-id="data-preprocessing"><span class="header-section-number">4.2.1</span> Data preprocessing</h4>
<p>Data pre-processing is quite straightforward. The first step is to divide the dataset into a grid of <span class="math inline">\(640&nbsp;\times&nbsp;640\)</span> tiles. Then, all these tiles are placed into one of the training, validation and test sets.</p>
<p>As for RGB and CIR images, preprocessing only contains two simple steps: tiling the large images into small <span class="math inline">\(640&nbsp;\times&nbsp;640\)</span> images, and normalizing all images along each channel. When both data sources are used, RGB and CIR images are also merged into images with 6 channels, which will be the input of the model.</p>
<p>As for CHM layers, there are more steps. The first step is to compute a sort of flattened point cloud, by computing the DTM, which represents the height of the ground, and removing this height to the point cloud. Then, for each CHM layer, if the height interval is <span class="math inline">\([z_\text{bot}, z_\text{top}]\)</span>, we first extract all the points which have a height <span class="math inline">\(h\)</span> such that <span class="math inline">\(z_\text{bot} \leq h \leq z_\text{top}\)</span>, and we then compute the DSM for this smaller point cloud. Since the ground height was already removed from the point cloud, this DSM is the CHM. Then, all the layers are merged into one raster with multiple channels and we normalize the whole raster with the average and the standard deviation over all channels. Finally, we can simply tile these rasters exactly like RGB and CIR images, which gives us the inputs of the model.</p>
<p>All these operations are conceptually simple, but they can be computationally expensive. Therefore, I had to put a certain effort into accelerating with different methods. First, I made sure to save the most important and generic elements to avoid useless computations every time the model is trained again, without saturating the memory. Then, I also implemented multi-threading for every possible step to improve the raw speed of preprocessing. Finally, performance is also the reason why normalization if performed during preprocessing instead of during the initialization of the data in the training loop.</p>
</section>
<section id="training-loop" class="level4" data-number="4.2.2">
<h4 data-number="4.2.2" class="anchored" data-anchor-id="training-loop"><span class="header-section-number">4.2.2</span> Training loop</h4>
<p>The training loop is very generic, so I will only mention the most interesting parts. First, we use an Adam optimizer and a simple learning rate scheduler with a multiplier at each epoch i which is <span class="math inline">\(1/\sqrt{i+2}\)</span>.</p>
<p>Then, since the batch size cannot be very large because of the space required by all the images, there is the possibility to perform gradient accumulation, which means that backward propagation won’t be performed with each batch, but instead every two or more batches. The idea behind this is to add more stability to the training, since back-propagating on only a few images is prone to overfitting on a set of examples which are not representative of the whole dataset.</p>
<p>As for the criterion to stop the training session, we use the loss on the validation set. Once this loss didn’t improve during 50 iterations over the whole dataset, we stop and keep the model that had the best validation loss.</p>
<p>Besides these details, the training loop is very generic. We loop over the entire training set with batches to compute the loss and perform gradient back-propagation,. Then we compute the loss on the validation set and store this loss as the metric that decides when to stop.</p>
</section>
<section id="output-postprocessing" class="level4" data-number="4.2.3">
<h4 data-number="4.2.3" class="anchored" data-anchor-id="output-postprocessing"><span class="header-section-number">4.2.3</span> Output postprocessing</h4>
<p>Regarding postprocessing of the output of the model, there a few things to mention. First, the model outputs a lot of bounding boxes, which have to be cleaned using two criteria. The first criterion is the confidence score. We can just set a threshold below which bounding boxes are discarded. The second criterion is the intersection over union (IoU) with other bounding boxes. IoU is a metrics used to quantify how similar two bounding boxes are. It is a value between 0 and 1, which formula is:</p>
<p><span class="math display">\[
\text{IoU}(A, B) = \frac{\text{area}(A \cap B)}{\text{area}(A \cup B)}
\]</span></p>
<p>Using this metrics, we can detect bounding boxes which are too similar to each other, and simply keep the bounding box with the highest confidence score when two bounding boxes have an IoU larger than a certain threshold.</p>
<p>For the evaluation, the process is a little different, because we only perform the clean up relying on IoU, and keep all other bounding boxes. The main metric that we compute is called sortedAP <span class="citation" data-cites="sortedAP">(<a href="#ref-sortedAP" role="doc-biblioref">Chen et al. 2023</a>)</span>, which is an evolution of the mean (point) average precision (mAP). mAP is defined as follows:</p>
<p><span class="math display">\[
\begin{array}{rcl}
\text{mAP} &amp; = &amp; \frac{1}{N} \sum\limits_{t\in T} \text{AP}_t \\
\text{AP}_t &amp; = &amp; \frac{{TP}_t}{{TP}_t + {FP}_t + {FN}_t}
\end{array}
\]</span></p>
<p>where <span class="math inline">\(T=\{t_1, t_2, \dots, t_N\}\)</span> is a list of IoU threshold values, <span class="math inline">\({TP}_t\)</span> are the true positives when the the IoU threshold is <span class="math inline">\(t\)</span>, <span class="math inline">\({FP}_t\)</span> are false positives and <span class="math inline">\({FN}_t\)</span> are false negatives. The reason why <span class="math inline">\(TP\)</span>, <span class="math inline">\(FP\)</span> and <span class="math inline">\(FN\)</span> depend on <span class="math inline">\(t\)</span> is that a bounding box is considered to be true if its IoU with one of the ground-truth bounding boxes is larger than <span class="math inline">\(t\)</span>.</p>
<p>sortedAP is an improvement over this method because there is no need to select a list of IoU threshold values. Predicted bounding boxes are sorted according to their confidence score which allows to compute <span class="math inline">\(\text{AP}\)</span> incrementally for any value of <span class="math inline">\(t\)</span>. Then, the area of the curve of the AP with respect to the IoU threshold is used as a metric, between 0 and 1, 1 being the best possible value.</p>
</section>
</section>
</section>
<section id="results" class="level2" data-number="5">
<h2 data-number="5" class="anchored" data-anchor-id="results"><span class="header-section-number">5</span> Results</h2>
<p>In this section are the results of the experiments performed with the model and the dataset presented before.</p>
<section id="training-parameters" class="level3" data-number="5.1">
<h3 data-number="5.1" class="anchored" data-anchor-id="training-parameters"><span class="header-section-number">5.1</span> Training parameters</h3>
<p>The first experiment was a simple test over the different parameters regarding the training loop. There were two goals to this experiment. The first one was to find the best training parameters for the next experiments. The second one was to see if randomly dropping one of the inputs of the model (either RGB/CIR or CHM) could help the model by pushing it to learn to make the best out of the two types of data.</p>
<p>The different parameters that are tested here are:</p>
<ul>
<li>“Learn. rate”: the initial learning rate.</li>
<li>“Prob. drop”: the probability to drop either RGB/CIR or CHM. The probability is the same for the two types, which means that if the displayed value is 0.1, then all data will be used 80% of the time, while only RGB/CIR and only CHM both happen 10% of the time.</li>
<li>“Accum. count”: the accumulation count, which means the amount of training data to process and compute the loss on before performing gradient back-propagation.</li>
</ul>
<p>As you can see on <a href="#fig-training-parameters-experiments" class="quarto-xref">Figure&nbsp;1</a>, sortedAP reaches at best values just above 0.3. The reason why the column name is “Best sortedAP” is due to the dataset being too small. Since the dataset is small, the training process overfits quickly, and the model doesn’t have enough training steps to have confidence scores which reach very high values. As a consequence, it is difficult to know beforehand which confidence threshold to choose. Therefore, the sortedAP metric is computed over several different confidence thresholds, and the one that gives the best value of sortedAP is kept.</p>
<p>With this experiment, we can see that a learning rate of 0.01 seems to make the training too much unstable, while 0.001 doesn’t give very high score. Then, we can also see how unstable the training process is in general, which comes mostly from the dataset being too small. However, a learning rate between 0.0025 and 0.006 seems to give the most stable results, when the drop probability is 0. This seems to show that the idea of randomly dropping one of the two inputs doesn’t really help the model to learn.</p>
<div id="cell-fig-training-parameters-experiments" class="cell" data-execution_count="1">
<div class="cell-output cell-output-display">
<div id="fig-training-parameters-experiments" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-training-parameters-experiments-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-training-parameters-experiments-output-1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Results with different training parameters for all experiments"><img src="index_files/figure-html/fig-training-parameters-experiments-output-1.png" width="840" height="563" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-training-parameters-experiments-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Results with different training parameters for all experiments
</figcaption>
</figure>
</div>
</div>
<a class="quarto-notebook-link" id="nblink-1" href="index-preview.html#cell-fig-training-parameters-experiments">Source: Article Notebook</a></div>
<p>In the next plot <a href="#fig-training-parameters-data" class="quarto-xref">Figure&nbsp;2</a>, we can see more results for the same experiments. Here, the results are colored according to the data that we use to evaluate the model. In blue, we see the value of sortedAP when we evaluate the model with the CHM layers data and dummy zero arrays as RGB/CIR data. These dummy arrays are also those that are used as input when one of the channel is dropped during training, when we have a drop probability larger than 0. Some interesting patterns appear in some of the cells in this plot. Firstly, it looks like randomly dropping one of the two inputs with the same probability has a much larger influence over the results using RGB/CIR than CHM. While CHM gives better results than RGB/CIR when always training using everything, RGB/CIR seems to perform better alone when also trained alone, even outperforming the combination of both inputs in certain cases.</p>
<div id="cell-fig-training-parameters-data" class="cell" data-execution_count="2">
<div class="cell-output cell-output-display">
<div id="fig-training-parameters-data" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-training-parameters-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<a href="index_files/figure-html/fig-training-parameters-data-output-2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;2: Results with different training parameters"><img src="index_files/figure-html/fig-training-parameters-data-output-2.png" width="922" height="563" class="figure-img"></a>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-training-parameters-data-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Results with different training parameters
</figcaption>
</figure>
</div>
</div>
</div>
<p>From the results of this experiment, I decided to pick the following parameters for the next experiments:</p>
<ul>
<li>Initial learning rate: 0.004</li>
<li>Drop probability: 0</li>
<li>Accumulation count: 10</li>
</ul>
</section>
<section id="data-used" class="level3" data-number="5.2">
<h3 data-number="5.2" class="anchored" data-anchor-id="data-used"><span class="header-section-number">5.2</span> Data used</h3>
</section>
<section id="chm-layers" class="level3" data-number="5.3">
<h3 data-number="5.3" class="anchored" data-anchor-id="chm-layers"><span class="header-section-number">5.3</span> CHM layers</h3>
</section>
<section id="hard-trees" class="level3" data-number="5.4">
<h3 data-number="5.4" class="anchored" data-anchor-id="hard-trees"><span class="header-section-number">5.4</span> Hard trees</h3>
</section>
</section>
<section id="discussion" class="level2" data-number="6">
<h2 data-number="6" class="anchored" data-anchor-id="discussion"><span class="header-section-number">6</span> Discussion</h2>
<section id="dataset" class="level3" data-number="6.1">
<h3 data-number="6.1" class="anchored" data-anchor-id="dataset"><span class="header-section-number">6.1</span> Dataset</h3>
<ul>
<li>DeepForest: A Python package for RGB deep learning tree crown delineation <span class="citation" data-cites="DeepForest">(<a href="#ref-DeepForest" role="doc-biblioref">B. G. Weinstein et al. 2020</a>)</span>: uses only RGB data to detect trees, but uses LiDAR to create millions of annotations of moderate quality to pre-train the model, before using around 10,000 hand-annotations to finalize and specialize the training on a certain area.</li>
</ul>
</section>
<section id="combination-of-data-types" class="level3" data-number="6.2">
<h3 data-number="6.2" class="anchored" data-anchor-id="combination-of-data-types"><span class="header-section-number">6.2</span> Combination of data types</h3>
</section>
</section>
<section id="conclusion" class="level2 unnumbered">
<h2 class="unnumbered anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Blablabla</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-urban-trees" class="csl-entry" role="listitem">
Arevalo-Ramirez, Tito, Anali Alfaro, José Figueroa, Mauricio Ponce-Donoso, Jose M. Saavedra, Matías Recabarren, and José Delpiano. 2024. <span>“Challenges for Computer Vision as a Tool for Screening Urban Trees Through Street-View Images.”</span> <em>Urban Forestry &amp; Urban Greening</em> 95:128316. <a href="https://doi.org/10.1016/j.ufug.2024.128316">https://doi.org/10.1016/j.ufug.2024.128316</a>.
</div>
<div id="ref-olive-tree" class="csl-entry" role="listitem">
Safonova, Anastasiia, Emilio Guirado, Yuriy Maglinets, Domingo Alcaraz-Segura, and Siham Tabik. 2021. <span>“Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask r-CNN.”</span> <em>Sensors</em> 21 (5): 1617. <a href="https://doi.org/10.3390/s21051617">https://doi.org/10.3390/s21051617</a>.
</div>
<div id="ref-OpenForest" class="csl-entry" role="listitem">
Ouaknine, Arthur, Teja Kattenborn, Etienne Laliberté, and David Rolnick. 2023. <span>“OpenForest: A Data Catalogue for Machine Learning in Forest Monitoring.”</span> <a href="https://arxiv.org/abs/2311.00277">https://arxiv.org/abs/2311.00277</a>.
</div>
<div id="ref-FoMo-Bench" class="csl-entry" role="listitem">
Bountos, Nikolaos Ioannis, Arthur Ouaknine, and David Rolnick. 2023. <span>“FoMo-Bench: A Multi-Modal, Multi-Scale and Multi-Task Forest Monitoring Benchmark for Remote Sensing Foundation Models.”</span> <em>arXiv Preprint arXiv:2312.10114</em>. <a href="https://arxiv.org/abs/2312.10114">https://arxiv.org/abs/2312.10114</a>.
</div>
<div id="ref-TALLO" class="csl-entry" role="listitem">
Jucker, Tommaso, Fabian Jörg Fischer, Jérôme Chave, David A. Coomes, John Caspersen, Arshad Ali, Grace Jopaul Loubota Panzou, et al. 2022. <span>“Tallo: A Global Tree Allometry and Crown Architecture Database.”</span> <em>Global Change Biology</em> 28 (17): 5254–68. <a href="https://doi.org/10.1111/gcb.16302">https://doi.org/10.1111/gcb.16302</a>.
</div>
<div id="ref-ReforesTree" class="csl-entry" role="listitem">
Reiersen, Gyri, David Dao, Björn Lütjens, Konstantin Klemmer, Kenza Amara, Attila Steinegger, Ce Zhang, and Xiaoxiang Zhu. 2022. <span>“ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery.”</span> <a href="https://arxiv.org/abs/2201.11192">https://arxiv.org/abs/2201.11192</a>.
</div>
<div id="ref-MillionTrees" class="csl-entry" role="listitem">
Weinstein, Ben. 2023. <span>“MillionTrees.”</span> 2023. <a href="https://milliontrees.idtrees.org/">https://milliontrees.idtrees.org/</a>.
</div>
<div id="ref-WildForest3D" class="csl-entry" role="listitem">
Kalinicheva, Ekaterina, Loic Landrieu, Clément Mallet, and Nesrine Chehata. 2022. <span>“Multi-Layer Modeling of Dense Vegetation from Aerial LiDAR Scans.”</span> <a href="https://arxiv.org/abs/2204.11620">https://arxiv.org/abs/2204.11620</a>.
</div>
<div id="ref-FOR-instance" class="csl-entry" role="listitem">
Puliti, Stefano, Grant Pearse, Peter Surový, Luke Wallace, Markus Hollaus, Maciej Wielgosz, and Rasmus Astrup. 2023. <span>“FOR-Instance: A UAV Laser Scanning Benchmark Dataset for Semantic and Instance Segmentation of Individual Trees.”</span> <a href="https://arxiv.org/abs/2309.01279">https://arxiv.org/abs/2309.01279</a>.
</div>
<div id="ref-MDAS" class="csl-entry" role="listitem">
Hu, J., R. Liu, D. Hong, A. Camero, J. Yao, M. Schneider, F. Kurz, K. Segl, and X. X. Zhu. 2023. <span>“MDAS: A New Multimodal Benchmark Dataset for Remote Sensing.”</span> <em>Earth System Science Data</em> 15 (1): 113–31. <a href="https://doi.org/10.5194/essd-15-113-2023">https://doi.org/10.5194/essd-15-113-2023</a>.
</div>
<div id="ref-NEONdata" class="csl-entry" role="listitem">
Weinstein, Ben, Sergio Marconi, and Ethan White. 2022. <span>“Data for the NeonTreeEvaluation Benchmark (0.2.2).”</span> Zenodo. <a href="https://doi.org/10.5281/zenodo.5914554">https://doi.org/10.5281/zenodo.5914554</a>.
</div>
<div id="ref-IGN_BD_ORTHO" class="csl-entry" role="listitem">
Institut national de l’information géographique et forestière (IGN). 2021. <span>“<span>BD ORTHO</span>.”</span> <a href="https://geoservices.ign.fr/bdortho">https://geoservices.ign.fr/bdortho</a>.
</div>
<div id="ref-Luchtfotos" class="csl-entry" role="listitem">
Beeldmateriaal Nederland. 2024. <span>“<span class="nocase">Luchtfoto’s (Aerial Photographs)</span>.”</span> <a href="https://www.beeldmateriaal.nl/luchtfotos">https://www.beeldmateriaal.nl/luchtfotos</a>.
</div>
<div id="ref-AHN4" class="csl-entry" role="listitem">
Actueel Hoogtebestand Nederland. 2020. <span>“<span class="nocase">AHN4 - Actual Height Model of the Netherlands</span>.”</span> <a href="https://www.ahn.nl/">https://www.ahn.nl/</a>.
</div>
<div id="ref-IGN_LiDAR_HD" class="csl-entry" role="listitem">
Institut national de l’information géographique et forestière (IGN). 2020. <span>“<span>LiDAR HD</span>.”</span> <a href="https://geoservices.ign.fr/lidarhd">https://geoservices.ign.fr/lidarhd</a>.
</div>
<div id="ref-amsterdam_trees" class="csl-entry" role="listitem">
Gemeente Amsterdam. 2024. <span>“<span>Bomenbestand Amsterdam (Amsterdam Tree Dataset)</span>.”</span> <a href="https://maps.amsterdam.nl/open_geodata/?k=505">https://maps.amsterdam.nl/open_geodata/?k=505</a>.
</div>
<div id="ref-bordeaux_trees" class="csl-entry" role="listitem">
Bordeaux Métropole. 2024. <span>“<span class="nocase">Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole)</span>.”</span> <a href="https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee">https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee</a>.
</div>
<div id="ref-boomregister" class="csl-entry" role="listitem">
Coöperatief Boomregister U.A. 2014. <span>“<span>Boom Register (Tree Register)</span>.”</span> <a href="https://boomregister.nl/">https://boomregister.nl/</a>.
</div>
<div id="ref-albumentations" class="csl-entry" role="listitem">
Buslaev, Alexander, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020. <span>“Albumentations: Fast and Flexible Image Augmentations.”</span> <em>Information</em> 11 (2). <a href="https://doi.org/10.3390/info11020125">https://doi.org/10.3390/info11020125</a>.
</div>
<div id="ref-gan_data_augment" class="csl-entry" role="listitem">
Sun, Chenxin, Chengwei Huang, Huaiqing Zhang, Bangqian Chen, Feng An, Liwen Wang, and Ting Yun. 2022. <span>“Individual Tree Crown Segmentation and Crown Width Extraction from a Heightmap Derived from Aerial Laser Scanning Data Using a Deep Learning Framework.”</span> <em>Frontiers in Plant Science</em> 13. <a href="https://doi.org/10.3389/fpls.2022.914974">https://doi.org/10.3389/fpls.2022.914974</a>.
</div>
<div id="ref-rgb_analytical" class="csl-entry" role="listitem">
Gomes, Marilia Ferreira, and Philippe Maillard. 2016. <span>“Detection of Tree Crowns in Very High Spatial Resolution Images.”</span> In <em>Environmental Applications of Remote Sensing</em>, edited by Maged Marghany. Rijeka: IntechOpen. <a href="https://doi.org/10.5772/62122">https://doi.org/10.5772/62122</a>.
</div>
<div id="ref-watershed" class="csl-entry" role="listitem">
Vincent, L., and P. Soille. 1991. <span>“Watersheds in Digital Spaces: An Efficient Algorithm Based on Immersion Simulations.”</span> <em>IEEE Transactions on Pattern Analysis and Machine Intelligence</em> 13 (6): 583–98. <a href="https://doi.org/10.1109/34.87344">https://doi.org/10.1109/34.87344</a>.
</div>
<div id="ref-local-maximum" class="csl-entry" role="listitem">
Wulder, Mike, K.Olaf Niemann, and David G. Goodenough. 2000. <span>“Local Maximum Filtering for the Extraction of Tree Locations and Basal Area from High Spatial Resolution Imagery.”</span> <em>Remote Sensing of Environment</em> 73 (1): 103–14. <a href="https://doi.org/10.1016/S0034-4257(00)00101-2">https://doi.org/10.1016/S0034-4257(00)00101-2</a>.
</div>
<div id="ref-valley-following" class="csl-entry" role="listitem">
Gougeon, François A et al. 1998. <span>“Automatic Individual Tree Crown Delineation Using a Valley-Following Algorithm and Rule-Based System.”</span> In <em>Proc. International Forum on Automated Interpretation of High Spatial Resolution Digital Imagery for Forestry, Victoria, British Columbia, Canada</em>, 11–23. Citeseer. <a href="https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf">https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf</a>.
</div>
<div id="ref-template-matching" class="csl-entry" role="listitem">
Pollock, Richard James. 1996. <span>“The Automatic Recognition of Individual Trees in Aerial Images of Forests Based on a Synthetic Tree Crown Image Model.”</span> PhD thesis, The University of British Columbia (Canada). <a href="https://dx.doi.org/10.14288/1.0051597">https://dx.doi.org/10.14288/1.0051597</a>.
</div>
<div id="ref-rgb-dl-watershed" class="csl-entry" role="listitem">
Freudenberg, Maximilian, Paul Magdon, and Nils Nölke. 2022. <span>“Individual Tree Crown Delineation in High-Resolution Remote Sensing Images Based on u-Net.”</span> <em>Neural Computing and Applications</em> 34 (24): 22197–207. <a href="https://doi.org/10.1007/s00521-022-07640-4">https://doi.org/10.1007/s00521-022-07640-4</a>.
</div>
<div id="ref-DeepForest" class="csl-entry" role="listitem">
Weinstein, Ben G., Sergio Marconi, Mélaine Aubry-Kientz, Gregoire Vincent, Henry Senyondo, and Ethan P. White. 2020. <span>“DeepForest: A Python Package for RGB Deep Learning Tree Crown Delineation.”</span> <em>Methods in Ecology and Evolution</em> 11 (12): 1743–51. <a href="https://doi.org/10.1111/2041-210X.13472">https://doi.org/10.1111/2041-210X.13472</a>.
</div>
<div id="ref-lidar_watershed" class="csl-entry" role="listitem">
Kwak, Doo-Ahn, Woo-Kyun Lee, Jun-Hak Lee, Greg S. Biging, and Peng Gong. 2007. <span>“Detection of Individual Trees and Estimation of Tree Height Using LiDAR Data.”</span> <em>Journal of Forest Research</em> 12 (6): 425–34. <a href="https://doi.org/10.1007/s10310-007-0041-9">https://doi.org/10.1007/s10310-007-0041-9</a>.
</div>
<div id="ref-lidar_benchmark" class="csl-entry" role="listitem">
Eysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger, Jean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015. <span>“A Benchmark of Lidar-Based Single Tree Detection Methods Using Heterogeneous Forest Data from the Alpine Space.”</span> <em>Forests</em> 6 (5): 1721–47. <a href="https://doi.org/10.3390/f6051721">https://doi.org/10.3390/f6051721</a>.
</div>
<div id="ref-lidar_benchmark_2" class="csl-entry" role="listitem">
Wang, Yunsheng, Juha Hyyppä, Xinlian Liang, Harri Kaartinen, Xiaowei Yu, Eva Lindberg, Johan Holmgren, et al. 2016. <span>“International Benchmarking of the Individual Tree Detection Methods for Modeling 3-d Canopy Structure for Silviculture and Forest Ecology Using Airborne Laser Scanning.”</span> <em>IEEE Transactions on Geoscience and Remote Sensing</em> 54 (9): 5011–27. <a href="https://doi.org/10.1109/TGRS.2016.2543225">https://doi.org/10.1109/TGRS.2016.2543225</a>.
</div>
<div id="ref-lidar_classification" class="csl-entry" role="listitem">
Diab, Ahmed, Rasha Kashef, and Ahmed Shaker. 2022. <span>“Deep Learning for LiDAR Point Cloud Classification in Remote Sensing.”</span> <em>Sensors (Basel)</em> 22 (20): 7868. <a href="https://doi.org/10.3390/s22207868">https://doi.org/10.3390/s22207868</a>.
</div>
<div id="ref-lidar_rgb_wst" class="csl-entry" role="listitem">
Qin, Haiming, Weiqi Zhou, Yang Yao, and Weimin Wang. 2022. <span>“Individual Tree Segmentation and Tree Species Classification in Subtropical Broadleaf Forests Using UAV-Based LiDAR, Hyperspectral, and Ultrahigh-Resolution RGB Data.”</span> <em>Remote Sensing of Environment</em> 280:113143. <a href="https://doi.org/10.1016/j.rse.2022.113143">https://doi.org/10.1016/j.rse.2022.113143</a>.
</div>
<div id="ref-lidar_rgb_acnet" class="csl-entry" role="listitem">
Li, Yingbo, Guoqi Chai, Yueting Wang, Lingting Lei, and Xiaoli Zhang. 2022. <span>“ACE r-CNN: An Attention Complementary and Edge Detection-Based Instance Segmentation Algorithm for Individual Tree Species Identification Using UAV RGB Images and LiDAR Data.”</span> <em>Remote Sensing</em> 14 (13). <a href="https://doi.org/10.3390/rs14133035">https://doi.org/10.3390/rs14133035</a>.
</div>
<div id="ref-amf_gd_yolov8" class="csl-entry" role="listitem">
Zhong, Hao, Zheyu Zhang, Haoran Liu, Jinzhuo Wu, and Wenshu Lin. 2024. <span>“Individual Tree Species Identification for Complex Coniferous and Broad-Leaved Mixed Forests Based on Deep Learning Combined with UAV LiDAR Data and RGB Images.”</span> <em>Forests</em> 15 (2). <a href="https://doi.org/10.3390/f15020293">https://doi.org/10.3390/f15020293</a>.
</div>
<div id="ref-DeepForestBefore" class="csl-entry" role="listitem">
Weinstein, Ben G., Sergio Marconi, Stephanie Bohlman, Alina Zare, and Ethan White. 2019. <span>“Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks.”</span> <em>Remote Sensing</em> 11 (11). <a href="https://doi.org/10.3390/rs11111309">https://doi.org/10.3390/rs11111309</a>.
</div>
<div id="ref-yolo" class="csl-entry" role="listitem">
Redmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. <span>“You Only Look Once: Unified, Real-Time Object Detection.”</span> In <em>2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</em>, 779–88. <a href="https://doi.org/10.1109/CVPR.2016.91">https://doi.org/10.1109/CVPR.2016.91</a>.
</div>
<div id="ref-sortedAP" class="csl-entry" role="listitem">
Chen, Long, Yuli Wu, Johannes Stegmaier, and Dorit Merhof. 2023. <span>“SortedAP: Rethinking Evaluation Metrics for Instance Segmentation.”</span> In <em>Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops</em>, 3923–29. <a href="https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html">https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"selector":".lightbox","closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>