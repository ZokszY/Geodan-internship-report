<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Alexandre Bry">
<meta name="dcterms.date" content="2024-07-15">
<meta name="keywords" content="tree detection, deep learning">

<title>Tree object detection using airborne images and LiDAR point clouds</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script async="" src="https://hypothes.is/embed.js"></script>
<script>
  window.document.addEventListener("DOMContentLoaded", function (_event) {
    document.body.classList.add('hypothesis-enabled');
  });
</script>


<meta name="citation_title" content="Tree object detection using airborne images and LiDAR point clouds">
<meta name="citation_abstract" content="This is the abstract.
It can be on multiple lines and contain **Markdown**.
">
<meta name="citation_keywords" content="tree detection,deep learning">
<meta name="citation_author" content="Alexandre Bry">
<meta name="citation_publication_date" content="2024-07-15">
<meta name="citation_cover_date" content="2024-07-15">
<meta name="citation_year" content="2024">
<meta name="citation_online_date" content="2024-07-15">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=FoMo-bench: A multi-modal, multi-scale and multi-task forest monitoring benchmark for remote sensing foundation models;,citation_author=Nikolaos Ioannis Bountos;,citation_author=Arthur Ouaknine;,citation_author=David Rolnick;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2312.10114;,citation_journal_title=arXiv preprint arXiv:2312.10114;">
<meta name="citation_reference" content="citation_title=OpenForest: A data catalogue for machine learning in forest monitoring;,citation_author=Arthur Ouaknine;,citation_author=Teja Kattenborn;,citation_author=Etienne Laliberté;,citation_author=David Rolnick;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2311.00277;">
<meta name="citation_reference" content="citation_title=Individual tree-crown detection in RGB imagery using semi-supervised deep learning neural networks;,citation_abstract=Remote sensing can transform the speed, scale, and cost of biodiversity and forestry surveys. Data acquisition currently outpaces the ability to identify individual organisms in high resolution imagery. We outline an approach for identifying tree-crowns in RGB imagery while using a semi-supervised deep learning detection network. Individual crown delineation has been a long-standing challenge in remote sensing and available algorithms produce mixed results. We show that deep learning models can leverage existing Light Detection and Ranging (LIDAR)-based unsupervised delineation to generate trees that are used for training an initial RGB crown detection model. Despite limitations in the original unsupervised detection approach, this noisy training data may contain information from which the neural network can learn initial tree features. We then refine the initial model using a small number of higher-quality hand-annotated RGB images. We validate our proposed approach while using an open-canopy site in the National Ecological Observation Network. Our results show that a model using 434,551 self-generated trees with the addition of 2848 hand-annotated trees yields accurate predictions in natural landscapes. Using an intersection-over-union threshold of 0.5, the full model had an average tree crown recall of 0.69, with a precision of 0.61 for the visually-annotated data. The model had an average tree detection rate of 0.82 for the field collected stems. The addition of a small number of hand-annotated trees improved the performance over the initial self-supervised model. This semi-supervised deep learning approach demonstrates that remote sensing can overcome a lack of labeled training data by generating noisy data for initial training using unsupervised methods and retraining the resulting models with high quality labeled data.;,citation_author=Ben G. Weinstein;,citation_author=Sergio Marconi;,citation_author=Stephanie Bohlman;,citation_author=Alina Zare;,citation_author=Ethan White;,citation_publication_date=2019;,citation_cover_date=2019;,citation_year=2019;,citation_fulltext_html_url=https://www.mdpi.com/2072-4292/11/11/1309;,citation_issue=11;,citation_doi=10.3390/rs11111309;,citation_issn=2072-4292;,citation_volume=11;,citation_journal_title=Remote Sensing;">
<meta name="citation_reference" content="citation_title=ReforesTree: A dataset for estimating tropical forest carbon stock with deep learning and aerial imagery;,citation_author=Gyri Reiersen;,citation_author=David Dao;,citation_author=Björn Lütjens;,citation_author=Konstantin Klemmer;,citation_author=Kenza Amara;,citation_author=Attila Steinegger;,citation_author=Ce Zhang;,citation_author=Xiaoxiang Zhu;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2201.11192;">
<meta name="citation_reference" content="citation_title=FOR-instance: A UAV laser scanning benchmark dataset for semantic and instance segmentation of individual trees;,citation_author=Stefano Puliti;,citation_author=Grant Pearse;,citation_author=Peter Surový;,citation_author=Luke Wallace;,citation_author=Markus Hollaus;,citation_author=Maciej Wielgosz;,citation_author=Rasmus Astrup;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2309.01279;">
<meta name="citation_reference" content="citation_title=MDAS: A new multimodal benchmark dataset for remote sensing;,citation_author=J. Hu;,citation_author=R. Liu;,citation_author=D. Hong;,citation_author=A. Camero;,citation_author=J. Yao;,citation_author=M. Schneider;,citation_author=F. Kurz;,citation_author=K. Segl;,citation_author=X. X. Zhu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://essd.copernicus.org/articles/15/113/2023/;,citation_issue=1;,citation_doi=10.5194/essd-15-113-2023;,citation_volume=15;,citation_journal_title=Earth System Science Data;">
<meta name="citation_reference" content="citation_title=Tallo: A global tree allometry and crown architecture database;,citation_abstract=Abstract Data capturing multiple axes of tree size and shape, such as a tree’s stem diameter, height and crown size, underpin a wide range of ecological research—from developing and testing theory on forest structure and dynamics, to estimating forest carbon stocks and their uncertainties, and integrating remote sensing imagery into forest monitoring programmes. However, these data can be surprisingly hard to come by, particularly for certain regions of the world and for specific taxonomic groups, posing a real barrier to progress in these fields. To overcome this challenge, we developed the Tallo database, a collection of 498,838 georeferenced and taxonomically standardized records of individual trees for which stem diameter, height and/or crown radius have been measured. These data were collected at 61,856 globally distributed sites, spanning all major forested and non-forested biomes. The majority of trees in the database are identified to species (88%), and collectively Tallo includes data for 5163 species distributed across 1453 genera and 187 plant families. The database is publicly archived under a CC-BY 4.0 licence and can be access from: https://doi.org/10.5281/zenodo.6637599. To demonstrate its value, here we present three case studies that highlight how the Tallo database can be used to address a range of theoretical and applied questions in ecology—from testing the predictions of metabolic scaling theory, to exploring the limits of tree allometric plasticity along environmental gradients and modelling global variation in maximum attainable tree height. In doing so, we provide a key resource for field ecologists, remote sensing researchers and the modelling community working together to better understand the role that trees play in regulating the terrestrial carbon cycle.;,citation_author=Tommaso Jucker;,citation_author=Fabian Jörg Fischer;,citation_author=Jérôme Chave;,citation_author=David A. Coomes;,citation_author=John Caspersen;,citation_author=Arshad Ali;,citation_author=Grace Jopaul Loubota Panzou;,citation_author=Ted R. Feldpausch;,citation_author=Daniel Falster;,citation_author=Vladimir A. Usoltsev;,citation_author=Stephen Adu-Bredu;,citation_author=Luciana F. Alves;,citation_author=Mohammad Aminpour;,citation_author=Ilondea B. Angoboy;,citation_author=Niels P. R. Anten;,citation_author=Cécile Antin;,citation_author=Yousef Askari;,citation_author=Rodrigo Muñoz;,citation_author=Narayanan Ayyappan;,citation_author=Patricia Balvanera;,citation_author=Lindsay Banin;,citation_author=Nicolas Barbier;,citation_author=John J. Battles;,citation_author=Hans Beeckman;,citation_author=Yannick E. Bocko;,citation_author=Ben Bond-Lamberty;,citation_author=Frans Bongers;,citation_author=Samuel Bowers;,citation_author=Thomas Brade;,citation_author=Michiel Breugel;,citation_author=Arthur Chantrain;,citation_author=Rajeev Chaudhary;,citation_author=Jingyu Dai;,citation_author=Michele Dalponte;,citation_author=Kangbéni Dimobe;,citation_author=Jean-Christophe Domec;,citation_author=Jean-Louis Doucet;,citation_author=Remko A. Duursma;,citation_author=Moisés Enríquez;,citation_author=Karin Y. Ewijk;,citation_author=William Farfán-Rios;,citation_author=Adeline Fayolle;,citation_author=Eric Forni;,citation_author=David I. Forrester;,citation_author=Hammad Gilani;,citation_author=John L. Godlee;,citation_author=Sylvie Gourlet-Fleury;,citation_author=Matthias Haeni;,citation_author=Jefferson S. Hall;,citation_author=Jie-Kun He;,citation_author=Andreas Hemp;,citation_author=José L. Hernández-Stefanoni;,citation_author=Steven I. Higgins;,citation_author=Robert J. Holdaway;,citation_author=Kiramat Hussain;,citation_author=Lindsay B. Hutley;,citation_author=Tomoaki Ichie;,citation_author=Yoshiko Iida;,citation_author=Hai-sheng Jiang;,citation_author=Puspa Raj Joshi;,citation_author=Hasan Kaboli;,citation_author=Maryam Kazempour Larsary;,citation_author=Tanaka Kenzo;,citation_author=Brian D. Kloeppel;,citation_author=Takashi Kohyama;,citation_author=Suwash Kunwar;,citation_author=Shem Kuyah;,citation_author=Jakub Kvasnica;,citation_author=Siliang Lin;,citation_author=Emily R. Lines;,citation_author=Hongyan Liu;,citation_author=Craig Lorimer;,citation_author=Jean-Joël Loumeto;,citation_author=Yadvinder Malhi;,citation_author=Peter L. Marshall;,citation_author=Eskil Mattsson;,citation_author=Radim Matula;,citation_author=Jorge A. Meave;,citation_author=Sylvanus Mensah;,citation_author=Xiangcheng Mi;,citation_author=Stéphane Momo;,citation_author=Glenn R. Moncrieff;,citation_author=Francisco Mora;,citation_author=Sarath P. Nissanka;,citation_author=Kevin L. O’Hara;,citation_author=Steven Pearce;,citation_author=Raphaël Pelissier;,citation_author=Pablo L. Peri;,citation_author=Pierre Ploton;,citation_author=Lourens Poorter;,citation_author=Mohsen Javanmiri Pour;,citation_author=Hassan Pourbabaei;,citation_author=Juan Manuel Dupuy-Rada;,citation_author=Sabina C. Ribeiro;,citation_author=Casey Ryan;,citation_author=Anvar Sanaei;,citation_author=Jennifer Sanger;,citation_author=Michael Schlund;,citation_author=Giacomo Sellan;,citation_author=Alexander Shenkin;,citation_author=Bonaventure Sonké;,citation_author=Frank J. Sterck;,citation_author=Martin Svátek;,citation_author=Kentaro Takagi;,citation_author=Anna T. Trugman;,citation_author=Farman Ullah;,citation_author=Matthew A. Vadeboncoeur;,citation_author=Ahmad Valipour;,citation_author=Mark C. Vanderwel;,citation_author=Alejandra G. Vovides;,citation_author=Weiwei Wang;,citation_author=Li-Qiu Wang;,citation_author=Christian Wirth;,citation_author=Murray Woods;,citation_author=Wenhua Xiang;,citation_author=Fabiano de Aquino Ximenes;,citation_author=Yaozhan Xu;,citation_author=Toshihiro Yamada;,citation_author=Miguel A. Zavala;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://onlinelibrary.wiley.com/doi/abs/10.1111/gcb.16302;,citation_issue=17;,citation_doi=https://doi.org/10.1111/gcb.16302;,citation_volume=28;,citation_journal_title=Global Change Biology;">
<meta name="citation_reference" content="citation_title=MillionTrees;,citation_author=Ben Weinstein;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://milliontrees.idtrees.org/;">
<meta name="citation_reference" content="citation_title=Multi-layer modeling of dense vegetation from aerial LiDAR scans;,citation_author=Ekaterina Kalinicheva;,citation_author=Loic Landrieu;,citation_author=Clément Mallet;,citation_author=Nesrine Chehata;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2204.11620;">
<meta name="citation_reference" content="citation_title=Beyond mAP: Towards better evaluation of instance segmentation;,citation_author=Rohit Jena;,citation_author=Lukas Zhornyak;,citation_author=Nehal Doiphode;,citation_author=Pratik Chaudhari;,citation_author=Vivek Buch;,citation_author=James Gee;,citation_author=Jianbo Shi;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_journal_title=CVPR;">
<meta name="citation_reference" content="citation_title=AHN4 - Actual Height Model of the Netherlands;,citation_author=Actueel Hoogtebestand Nederland;,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://www.ahn.nl/;">
<meta name="citation_reference" content="citation_title=Luchtfoto’s (Aerial Photographs);,citation_author=Beeldmateriaal Nederland;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.beeldmateriaal.nl/luchtfotos;">
<meta name="citation_reference" content="citation_title=LiDAR HD;,citation_author=Institut national de l’information géographique et forestière (IGN);,citation_publication_date=2020;,citation_cover_date=2020;,citation_year=2020;,citation_fulltext_html_url=https://geoservices.ign.fr/lidarhd;">
<meta name="citation_reference" content="citation_title=BD ORTHO;,citation_author=Institut national de l’information géographique et forestière (IGN);,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://geoservices.ign.fr/bdortho;">
<meta name="citation_reference" content="citation_title=Bomenbestand Amsterdam (Amsterdam Tree Dataset);,citation_author=Gemeente Amsterdam;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://maps.amsterdam.nl/open_geodata/?k=505;">
<meta name="citation_reference" content="citation_title=Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole);,citation_author=Bordeaux Métropole;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee;">
<meta name="citation_reference" content="citation_title=Boom Register (Tree Register);,citation_author=Coöperatief Boomregister U.A.;,citation_publication_date=2014;,citation_cover_date=2014;,citation_year=2014;,citation_fulltext_html_url=https://boomregister.nl/;">
<meta name="citation_reference" content="citation_title=Challenges for computer vision as a tool for screening urban trees through street-view images;,citation_abstract=Urban forests play a fundamental and irreplaceable role within cities through the ecosystem services they provide, such as carbon capture. However, inadequate management of urban trees can heighten the risks they pose to society. For instance, mechanical failures of tree components, such as branches, can cause harm to individuals and property. Regular assessments of tree conditions are necessary to mitigate these tree-related hazards, yet such evaluations are labor-intensive and currently lack automation. Previous studies have proposed utilizing street view images to alleviate tree inspection and shown the feasibility of visually inspecting trees. However, only a limited number of studies have addressed the automatic evaluation of urban trees, a challenge that can potentially be addressed using deep learning networks. Particularly in urban environments, there is a pressing need for increased automation in unresolved computer vision tasks. Therefore, this research presents a comprehensive analysis of neural networks and publicly available datasets that can aid arborists in automatically identifying urban trees. Specifically, we investigate the potential of deep learning networks in classifying tree genera and segmenting individual trees and their trunks. We emphasize the utilization of transfer learning strategies to enhance tree identification. The results demonstrate that neural networks can be considered practical tools for assisting arborists in tree recognition. Nevertheless, there are still gaps that remain and require attention in future research endeavors.;,citation_author=Tito Arevalo-Ramirez;,citation_author=Anali Alfaro;,citation_author=José Figueroa;,citation_author=Mauricio Ponce-Donoso;,citation_author=Jose M. Saavedra;,citation_author=Matías Recabarren;,citation_author=José Delpiano;,citation_publication_date=2024;,citation_cover_date=2024;,citation_year=2024;,citation_fulltext_html_url=https://www.sciencedirect.com/science/article/pii/S1618866724001146;,citation_doi=10.1016/j.ufug.2024.128316;,citation_issn=1618-8667;,citation_volume=95;,citation_journal_title=Urban Forestry &amp;amp;amp; Urban Greening;">
<meta name="citation_reference" content="citation_title=Olive tree biovolume from UAV multi-resolution image segmentation with mask r-CNN;,citation_abstract=Olive tree growing is an important economic activity in many countries, mostly in the Mediterranean Basin, Argentina, Chile, Australia, and California. Although recent intensification techniques organize olive groves in hedgerows, most olive groves are rainfed and the trees are scattered (as in Spain and Italy, which account for 50% of the world’s olive oil production). Accurate measurement of trees biovolume is a first step to monitor their performance in olive production and health. In this work, we use one of the most accurate deep learning instance segmentation methods (Mask R-CNN) and unmanned aerial vehicles (UAV) images for olive tree crown and shadow segmentation (OTCS) to further estimate the biovolume of individual trees. We evaluated our approach on images with different spectral bands (red, green, blue, and near infrared) and vegetation indices (normalized difference vegetation index—NDVI—and green normalized difference vegetation index—GNDVI). The performance of red-green-blue (RGB) images were assessed at two spatial resolutions 3 cm/pixel and 13 cm/pixel, while NDVI and GNDV images were only at 13 cm/pixel. All trained Mask R-CNN-based models showed high performance in the tree crown segmentation, particularly when using the fusion of all dataset in GNDVI and NDVI (F1-measure from 95% to 98%). The comparison in a subset of trees of our estimated biovolume with ground truth measurements showed an average accuracy of 82%. Our results support the use of NDVI and GNDVI spectral indices for the accurate estimation of the biovolume of scattered trees, such as olive trees, in UAV images.;,citation_author=Anastasiia Safonova;,citation_author=Emilio Guirado;,citation_author=Yuriy Maglinets;,citation_author=Domingo Alcaraz-Segura;,citation_author=Siham Tabik;,citation_publication_date=2021;,citation_cover_date=2021;,citation_year=2021;,citation_fulltext_html_url=https://www.mdpi.com/1424-8220/21/5/1617;,citation_issue=5;,citation_doi=10.3390/s21051617;,citation_issn=1424-8220;,citation_pmid=33668984;,citation_volume=21;,citation_journal_title=Sensors;">
</head>

<body>

<header id="title-block-header" class="quarto-title-block default toc-left page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Tree object detection using airborne images and LiDAR point clouds</h1>
          </div>

    
    <div class="quarto-title-meta-container">
      <div class="quarto-title-meta-column-start">
            <div class="quarto-title-meta-author">
          <div class="quarto-title-meta-heading">Author</div>
          <div class="quarto-title-meta-heading">Affiliations</div>
          
                <div class="quarto-title-meta-contents">
            <p class="author">Alexandre Bry <a href="mailto:alexandre.bry.21@polytechnique.org" class="quarto-title-author-email"><i class="bi bi-envelope"></i></a> </p>
          </div>
                <div class="quarto-title-meta-contents">
                    <p class="affiliation">
                        <a href="https://portail.polytechnique.edu/informatique/fr/page-daccueil">
                        École polytechnique
                        </a>
                      </p>
                    <p class="affiliation">
                        <a href="https://research.geodan.nl/">
                        Geodan B.V.
                        </a>
                      </p>
                  </div>
                    </div>
        
        <div class="quarto-title-meta">

                      
                <div>
            <div class="quarto-title-meta-heading">Published</div>
            <div class="quarto-title-meta-contents">
              <p class="date">July 15, 2024</p>
            </div>
          </div>
          
                
              </div>
      </div>
      <div class="quarto-title-meta-column-end quarto-other-formats-target">
      <div class="quarto-alternate-formats"><div class="quarto-title-meta-heading">Other Formats</div><div class="quarto-title-meta-contents"><p><a href="index.docx"><i class="bi bi-file-word"></i>MS Word</a></p></div><div class="quarto-title-meta-contents"><p><a href="index_typst.pdf"><i class="bi bi-file-pdf"></i>Typst</a></p></div><div class="quarto-title-meta-contents"><p><a href="index_pdf.pdf"><i class="bi bi-file-pdf"></i>PDF</a></p></div><div class="quarto-title-meta-contents"><p><a href="index-meca.zip" data-meca-link="true"><i class="bi bi-archive"></i>MECA Bundle</a></p></div></div></div>
    </div>

    <div>
      <div class="abstract">
        <div class="block-title">Abstract</div>
        <p>This is the abstract. It can be on multiple lines and contain <strong>Markdown</strong>.</p>
      </div>
    </div>

    <div>
      <div class="keywords">
        <div class="block-title">Keywords</div>
        <p>tree detection, deep learning</p>
      </div>
    </div>

    <div class="quarto-other-links-text-target">
    </div>  </div>
</header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article toc-left">
<div id="quarto-sidebar-toc-left" class="sidebar toc-left">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">Introduction</a></li>
  <li><a href="#state-of-the-art" id="toc-state-of-the-art" class="nav-link" data-scroll-target="#state-of-the-art">State-of-the-art</a>
  <ul class="collapse">
  <li><a href="#computer-vision-tasks-related-to-trees" id="toc-computer-vision-tasks-related-to-trees" class="nav-link" data-scroll-target="#computer-vision-tasks-related-to-trees">Computer vision tasks related to trees</a></li>
  <li><a href="#datasets" id="toc-datasets" class="nav-link" data-scroll-target="#datasets">Datasets</a></li>
  <li><a href="#models" id="toc-models" class="nav-link" data-scroll-target="#models">Models</a></li>
  </ul></li>
  <li><a href="#sec-dataset" id="toc-sec-dataset" class="nav-link" data-scroll-target="#sec-dataset">Dataset</a></li>
  <li><a href="#model" id="toc-model" class="nav-link" data-scroll-target="#model">Model</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  </ul>
</nav>
</div>
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar zindex-bottom">
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">



  


<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">Introduction</h2>
<p>The goal of the internship was to study the possibility of combining LiDAR point clouds and aerial images in a deep learning model to perform instance segmentation of trees. The two types of data are indeed complementary, as point clouds capture the shape of the worlds, while images capture the colors. However, combining them into a format that allows a model to handle them simultaneously is not an easy task because they inherently have a very different spatial repartition and encoding.</p>
<p>The second major topic of the internship was to acquire a proper dataset matching all the criteria required for the project. Most of the datasets containing tree annotations only used either RGB images or LiDAR point clouds, but not both. Therefore, I had to create such a dataset by myself, using the openly available images and point clouds in the Netherlands, by annotating trees by hand to properly train and evaluate the methods.</p>
</section>
<section id="state-of-the-art" class="level2">
<h2 class="anchored" data-anchor-id="state-of-the-art">State-of-the-art</h2>
<section id="computer-vision-tasks-related-to-trees" class="level3">
<h3 class="anchored" data-anchor-id="computer-vision-tasks-related-to-trees">Computer vision tasks related to trees</h3>
<p>Before talking about models and datasets, let’s define properly the task that this project focused on, in the midst of all the various computer vision tasks, and specifically those related to tree detection.</p>
<p>The first main differentiation between tree recognition tasks comes from the acquisition of the data. There are some very different tasks and methods using either ground data or aerial/satellite data. This is especially true when focusing on urban trees as shown in <span class="citation" data-cites="urban-trees">(<a href="#ref-urban-trees" role="doc-biblioref">Arevalo-Ramirez et al. 2024</a>)</span>, since a lot of street view data is available.</p>
<p>This leads to the second variation, which is related to the kind of tree area that we are interested in. There are mainly three types of area, which among other things, influence the organization of the trees in space: urban areas, tree plantations and forests. This is important, because the tasks and the difficulty depends on the type of area. Tree plantations are much easier to work with than completely wild forests, while urban areas contain various levels of difficulty ranging from alignment trees to private and disorganized gardens and parks. For this project, we mainly focused on urban areas, but everything should still be applicable to tree plantations and forests.</p>
<p>Then, the four fundamental computer vision tasks have their application when dealing with trees, as explained in <span class="citation" data-cites="olive-tree">(<a href="#ref-olive-tree" role="doc-biblioref">Safonova et al. 2021</a>)</span>:</p>
<ul>
<li>Classification, although this is quite rare for airborne tree applications since there are multiple trees on each image most of the time</li>
<li>Detection, which consists in detecting objects and placing boxes around them</li>
<li>Semantic segmentation, which consists in associating a label to every pixel of an image</li>
<li>Instance segmentation, which consists in adding a layer of complexity to semantic segmentation by also differentiating between the different instances of each class</li>
</ul>
<p>These generic tasks can be extended by trying to get more information about the trees. The most common information are the species and the height.</p>
</section>
<section id="datasets" class="level3">
<h3 class="anchored" data-anchor-id="datasets">Datasets</h3>
<section id="requirements" class="level4">
<h4 class="anchored" data-anchor-id="requirements">Requirements</h4>
<p>Before presenting the different promising datasets and the reasons why they were not fully usable for the project, let’s enumerate the different conditions and requirements for the tree instance segmentation task:</p>
<ul>
<li>Multiple types of data:
<ul>
<li>Aerial RGB images</li>
<li>LiDAR point clouds (preferably aerial)</li>
<li>(Optional) Aerial infrared images</li>
</ul></li>
<li>Tree crown annotations or bounding boxes</li>
<li>High-enough resolution:
<ul>
<li>For images, about 25&nbsp;cm</li>
<li>For point clouds, about 10&nbsp;cm</li>
</ul></li>
</ul>
<p>Here are the explanations for these requirements. As for the types of data, RGB images and point clouds are required to experiment on the ability of the model to combine the two very different kinds of information they hold. Having infrared data as well could be beneficial, but it was not necessary. Regarding tree annotations, it was necessary to have a way to spatially identify them individually, using crown contours or simply bounding boxes. Since the model outputs bounding boxes, any kind of other format could easily be transformed to bounding boxes. Finally, the resolution had to be high enough to identify individual trees and be able to really use the data. For the point clouds especially, the whole idea was to see if and how the topology of the trees could be learnt, using at least the trunks and even the biggest branches if possible. Therefore, even if they are not really comparable, this is the reason why the required resolution is more precise for the point clouds.</p>
<p>Unfortunately, none of the datasets that I found matched all these criteria. Furthermore, I didn’t find any overlapping datasets that I could merge to create a dataset with all the required types of data. In the next parts, I will go through the different kinds of datasets that exist, the reasons why they did not really fit for the project and the ideas I got when searching for a way to use them.</p>
</section>
<section id="existing-tree-datasets" class="level4">
<h4 class="anchored" data-anchor-id="existing-tree-datasets">Existing tree datasets</h4>
<p>As explained above, there were quite a lot of requirements to fulfill to have a complete dataset usable for the task. This means that almost all the available datasets were missing something, as they were mainly focusing on using one kind of data and trying to make the most out of it, instead of trying to use all the types of data together.</p>
<p>The most comprehensive list of tree annotations datasets was published in <span class="citation" data-cites="OpenForest">(<a href="#ref-OpenForest" role="doc-biblioref">Ouaknine et al. 2023</a>)</span>. <span class="citation" data-cites="FoMo-Bench">(<a href="#ref-FoMo-Bench" role="doc-biblioref">Bountos, Ouaknine, and Rolnick 2023</a>)</span> also lists several interesting datasets, even though most of them can also be found in <span class="citation" data-cites="OpenForest">(<a href="#ref-OpenForest" role="doc-biblioref">Ouaknine et al. 2023</a>)</span>. Without enumerating all of them, there were multiple kinds of datasets that all have their own flaws regarding the requirements I was looking for.</p>
<p>Firstly, there are the forest inventories. <span class="citation" data-cites="TALLO">(<a href="#ref-TALLO" role="doc-biblioref">Jucker et al. 2022</a>)</span> is probably the most interesting one in this category, because it contains a lot of spatial information about almost 500K trees, with their locations, their crown radii and their heights. Therefore, everything needed to localize trees is in the dataset. However, I didn’t manage to find RGB images or LiDAR point clouds of the areas where the trees are located, making it impossible to use these annotations to train tree detection.</p>
<p>Secondly, there are the RGB datasets. <span class="citation" data-cites="ReforesTree">(<a href="#ref-ReforesTree" role="doc-biblioref">Reiersen et al. 2022</a>)</span> and <span class="citation" data-cites="MillionTrees">(<a href="#ref-MillionTrees" role="doc-biblioref">B. Weinstein 2023</a>)</span> are two of them and the quality of their images are high. The only drawback of these datasets is obviously that they don’t provide any kind of point cloud, which make them unsuitable for the task.</p>
<p>Thirdly, there are the LiDAR datasets, such as <span class="citation" data-cites="WildForest3D">(<a href="#ref-WildForest3D" role="doc-biblioref">Kalinicheva et al. 2022</a>)</span> and <span class="citation" data-cites="FOR-instance">(<a href="#ref-FOR-instance" role="doc-biblioref">Puliti et al. 2023</a>)</span>. Similarly to RGB datasets, they lack one of the data source for the task I worked on. But unlike them, they have the advantage that the missing data could be much easier to acquire from another source, since RGB aerial or satellite images are much more common than LiDAR point clouds. However, this solution was abandoned for two main reasons. First it is quite challenging to find the exact locations where the point clouds were acquired. Then, even when the location is known, it is often in the middle of a forest where the quality of satellite imagery is very low.</p>
<p>Finally, I also found two datasets that had RGB and LiDAR components. The first one is <span class="citation" data-cites="MDAS">(<a href="#ref-MDAS" role="doc-biblioref">Hu et al. 2023</a>)</span>. This benchmark dataset encompasses RGB images, hyperspectral images and Digital Surface Models (DSM). There were however two major flaws. The obvious one was that this dataset was created with land semantic segmentation tasks in mind, so there was no tree annotations. The less obvious one was that a DSM is not a point cloud, even though it is some kind of 3D information and was often created using a LiDAR point cloud. As a consequence, I would have been very limited in my ability to use the point cloud.</p>
<p>The only real dataset with RGB and LiDAR was <span class="citation" data-cites="NEON">(<a href="#ref-NEON" role="doc-biblioref">B. G. Weinstein et al. 2019</a>)</span>. This dataset contains exactly all the data I was looking for, with RGB images, hyperspectral images and LiDAR point clouds. With 30975 tree annotations, it is also a quite large dataset, spanning across multiple various forests. The reason why I decided not to use it despite all this is that at the beginning of the project, I thought that the quality of the images and the point clouds was too low. Looking back on this decision, I think that I probably could have worked with this dataset and gotten great results. This would have saved me the time spent annotating the trees for my own dataset, which I will talk more about later. My decision was also influenced by the quality of the images and the point clouds available in the Netherlands, which I will talk about in the next section.</p>
</section>
<section id="public-data" class="level4">
<h4 class="anchored" data-anchor-id="public-data">Public data</h4>
<p>After rejecting all the available datasets I had found, the only solution I had left was to create my own dataset. I won’t dive too much in this process that I will explain in <a href="#sec-dataset" class="quarto-xref">Section&nbsp;3</a>. I just want to mention all the publicly available datasets that I used or could have used to create this custom dataset.</p>
<p>For practical reasons, the two countries where I mostly searched for available data are France and the Netherlands. I was looking for three different data types independently:</p>
<ul>
<li>RGB (and eventually infrared) images</li>
<li>LiDAR point clouds</li>
<li>Tree annotations</li>
</ul>
<p>These three types of data are available in similar ways in both countries, although the Netherlands have a small edge over France. RGB images are really easy to find in France (<span class="citation" data-cites="IGN_BDORTHO">(<a href="#ref-IGN_BDORTHO" role="doc-biblioref">Institut national de l’information géographique et forestière (IGN) 2021</a>)</span>) and in the Netherlands (<span class="citation" data-cites="Luchtfotos">(<a href="#ref-Luchtfotos" role="doc-biblioref">Beeldmateriaal Nederland 2024</a>)</span>), but the resolution is better in the Netherlands (8&nbsp;cm vs 20&nbsp;cm). Hyperspectral images are also available in both countries, although for those the resolution is only 25&nbsp;cm in the Netherlands.</p>
<p>As for LiDAR point clouds, the Netherlands have a small edge over France, because they are at their forth version covering the whole country with <span class="citation" data-cites="AHN4">(<a href="#ref-AHN4" role="doc-biblioref">Actueel Hoogtebestand Nederland 2020</a>)</span>, and are already working on the fifth version. In France, data acquisition for the first LiDAR point cloud covering the whole country (<span class="citation" data-cites="IGN_LiDARHD">(<a href="#ref-IGN_LiDARHD" role="doc-biblioref">Institut national de l’information géographique et forestière (IGN) 2020</a>)</span>) started a few years ago. It is not yet finished, even though data is already available for half of the country. The other advantage of the data from Netherlands regarding LiDAR point clouds is that all flights are performed during winter, which allows light beams to penetrate more deeply in trees and reach trunks and branches. This is not the case in France.</p>
<p>The part that is missing in both countries is related to tree annotations. Many municipalities have datasets containing information about all the public trees they handle. This is for example the case for <span class="citation" data-cites="amsterdam_trees">(<a href="#ref-amsterdam_trees" role="doc-biblioref">Gemeente Amsterdam 2024</a>)</span> and <span class="citation" data-cites="bordeaux_trees">(<a href="#ref-bordeaux_trees" role="doc-biblioref">Bordeaux Métropole 2024</a>)</span>. However, these datasets cannot really be used as ground truth for a custom dataset for several reasons. First, many of them do not contain coordinates indicating the position of each tree in the city. Then, even those that contain coordinates are most of the time missing any kind of information allowing to deduce a bounding box for the tree crowns. Finally, even if they did contain everything, they only focus on public trees, and are missing every single tree located in a private area. Since public and private areas are obviously imbricated in all cities, it means that any area we try to train the model on would be missing all the private trees, making the training process impossible because we cannot have only a partial annotation of images.</p>
<p>The other tree annotation source that we could have used is Boomregister (<span class="citation" data-cites="boomregister">(<a href="#ref-boomregister" role="doc-biblioref">Coöperatief Boomregister U.A. 2014</a>)</span>). This work covers the whole of the Netherlands, including public and private trees. However, the precision of the masks is far from perfect, and many trees are missing or incorrectly segmented, especially when they are less than 9&nbsp;m heigh or have a crown diameter smaller than 4&nbsp;m. Therefore, even it is a very impressive piece of work, we thought that it could not be used as training data for a deep learning models due to its biases and imperfections.</p>
</section>
<section id="dataset-augmentation-techniques" class="level4">
<h4 class="anchored" data-anchor-id="dataset-augmentation-techniques">Dataset augmentation techniques</h4>
</section>
</section>
<section id="models" class="level3">
<h3 class="anchored" data-anchor-id="models">Models</h3>
</section>
</section>
<section id="sec-dataset" class="level2">
<h2 class="anchored" data-anchor-id="sec-dataset">Dataset</h2>
</section>
<section id="model" class="level2">
<h2 class="anchored" data-anchor-id="model">Model</h2>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>Beyond mAP: <span class="citation" data-cites="BeyondMAP">(<a href="#ref-BeyondMAP" role="doc-biblioref">Jena et al. 2023</a>)</span>.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>Blablabla</p>

</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-AHN4" class="csl-entry" role="listitem">
Actueel Hoogtebestand Nederland. 2020. <span>“<span class="nocase">AHN4 - Actual Height Model of the Netherlands</span>.”</span> <a href="https://www.ahn.nl/">https://www.ahn.nl/</a>.
</div>
<div id="ref-urban-trees" class="csl-entry" role="listitem">
Arevalo-Ramirez, Tito, Anali Alfaro, José Figueroa, Mauricio Ponce-Donoso, Jose M. Saavedra, Matías Recabarren, and José Delpiano. 2024. <span>“Challenges for Computer Vision as a Tool for Screening Urban Trees Through Street-View Images.”</span> <em>Urban Forestry &amp; Urban Greening</em> 95:128316. <a href="https://doi.org/10.1016/j.ufug.2024.128316">https://doi.org/10.1016/j.ufug.2024.128316</a>.
</div>
<div id="ref-Luchtfotos" class="csl-entry" role="listitem">
Beeldmateriaal Nederland. 2024. <span>“<span class="nocase">Luchtfoto’s (Aerial Photographs)</span>.”</span> <a href="https://www.beeldmateriaal.nl/luchtfotos">https://www.beeldmateriaal.nl/luchtfotos</a>.
</div>
<div id="ref-bordeaux_trees" class="csl-entry" role="listitem">
Bordeaux Métropole. 2024. <span>“<span class="nocase">Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole)</span>.”</span> <a href="https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee">https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee</a>.
</div>
<div id="ref-FoMo-Bench" class="csl-entry" role="listitem">
Bountos, Nikolaos Ioannis, Arthur Ouaknine, and David Rolnick. 2023. <span>“FoMo-Bench: A Multi-Modal, Multi-Scale and Multi-Task Forest Monitoring Benchmark for Remote Sensing Foundation Models.”</span> <em>arXiv Preprint arXiv:2312.10114</em>. <a href="https://arxiv.org/abs/2312.10114">https://arxiv.org/abs/2312.10114</a>.
</div>
<div id="ref-boomregister" class="csl-entry" role="listitem">
Coöperatief Boomregister U.A. 2014. <span>“<span>Boom Register (Tree Register)</span>.”</span> <a href="https://boomregister.nl/">https://boomregister.nl/</a>.
</div>
<div id="ref-amsterdam_trees" class="csl-entry" role="listitem">
Gemeente Amsterdam. 2024. <span>“<span>Bomenbestand Amsterdam (Amsterdam Tree Dataset)</span>.”</span> <a href="https://maps.amsterdam.nl/open_geodata/?k=505">https://maps.amsterdam.nl/open_geodata/?k=505</a>.
</div>
<div id="ref-MDAS" class="csl-entry" role="listitem">
Hu, J., R. Liu, D. Hong, A. Camero, J. Yao, M. Schneider, F. Kurz, K. Segl, and X. X. Zhu. 2023. <span>“MDAS: A New Multimodal Benchmark Dataset for Remote Sensing.”</span> <em>Earth System Science Data</em> 15 (1): 113–31. <a href="https://doi.org/10.5194/essd-15-113-2023">https://doi.org/10.5194/essd-15-113-2023</a>.
</div>
<div id="ref-IGN_LiDARHD" class="csl-entry" role="listitem">
Institut national de l’information géographique et forestière (IGN). 2020. <span>“<span>LiDAR HD</span>.”</span> <a href="https://geoservices.ign.fr/lidarhd">https://geoservices.ign.fr/lidarhd</a>.
</div>
<div id="ref-IGN_BDORTHO" class="csl-entry" role="listitem">
Institut national de l’information géographique et forestière (IGN). 2021. <span>“<span>BD ORTHO</span>.”</span> <a href="https://geoservices.ign.fr/bdortho">https://geoservices.ign.fr/bdortho</a>.
</div>
<div id="ref-BeyondMAP" class="csl-entry" role="listitem">
Jena, Rohit, Lukas Zhornyak, Nehal Doiphode, Pratik Chaudhari, Vivek Buch, James Gee, and Jianbo Shi. 2023. <span>“Beyond mAP: Towards Better Evaluation of Instance Segmentation.”</span> <em>CVPR</em>.
</div>
<div id="ref-TALLO" class="csl-entry" role="listitem">
Jucker, Tommaso, Fabian Jörg Fischer, Jérôme Chave, David A. Coomes, John Caspersen, Arshad Ali, Grace Jopaul Loubota Panzou, et al. 2022. <span>“Tallo: A Global Tree Allometry and Crown Architecture Database.”</span> <em>Global Change Biology</em> 28 (17): 5254–68. https://doi.org/<a href="https://doi.org/10.1111/gcb.16302">https://doi.org/10.1111/gcb.16302</a>.
</div>
<div id="ref-WildForest3D" class="csl-entry" role="listitem">
Kalinicheva, Ekaterina, Loic Landrieu, Clément Mallet, and Nesrine Chehata. 2022. <span>“Multi-Layer Modeling of Dense Vegetation from Aerial LiDAR Scans.”</span> <a href="https://arxiv.org/abs/2204.11620">https://arxiv.org/abs/2204.11620</a>.
</div>
<div id="ref-OpenForest" class="csl-entry" role="listitem">
Ouaknine, Arthur, Teja Kattenborn, Etienne Laliberté, and David Rolnick. 2023. <span>“OpenForest: A Data Catalogue for Machine Learning in Forest Monitoring.”</span> <a href="https://arxiv.org/abs/2311.00277">https://arxiv.org/abs/2311.00277</a>.
</div>
<div id="ref-FOR-instance" class="csl-entry" role="listitem">
Puliti, Stefano, Grant Pearse, Peter Surový, Luke Wallace, Markus Hollaus, Maciej Wielgosz, and Rasmus Astrup. 2023. <span>“FOR-Instance: A UAV Laser Scanning Benchmark Dataset for Semantic and Instance Segmentation of Individual Trees.”</span> <a href="https://arxiv.org/abs/2309.01279">https://arxiv.org/abs/2309.01279</a>.
</div>
<div id="ref-ReforesTree" class="csl-entry" role="listitem">
Reiersen, Gyri, David Dao, Björn Lütjens, Konstantin Klemmer, Kenza Amara, Attila Steinegger, Ce Zhang, and Xiaoxiang Zhu. 2022. <span>“ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery.”</span> <a href="https://arxiv.org/abs/2201.11192">https://arxiv.org/abs/2201.11192</a>.
</div>
<div id="ref-olive-tree" class="csl-entry" role="listitem">
Safonova, Anastasiia, Emilio Guirado, Yuriy Maglinets, Domingo Alcaraz-Segura, and Siham Tabik. 2021. <span>“Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask r-CNN.”</span> <em>Sensors</em> 21 (5): 1617. <a href="https://doi.org/10.3390/s21051617">https://doi.org/10.3390/s21051617</a>.
</div>
<div id="ref-MillionTrees" class="csl-entry" role="listitem">
Weinstein, Ben. 2023. <span>“MillionTrees.”</span> 2023. <a href="https://milliontrees.idtrees.org/">https://milliontrees.idtrees.org/</a>.
</div>
<div id="ref-NEON" class="csl-entry" role="listitem">
Weinstein, Ben G., Sergio Marconi, Stephanie Bohlman, Alina Zare, and Ethan White. 2019. <span>“Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks.”</span> <em>Remote Sensing</em> 11 (11). <a href="https://doi.org/10.3390/rs11111309">https://doi.org/10.3390/rs11111309</a>.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>