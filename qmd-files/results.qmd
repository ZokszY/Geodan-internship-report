# Results

In this section are the results of the experiments performed with the model and the dataset presented before.

## Training parameters

The first experiment was a simple test over the different parameters regarding the training loop. There were two goals to this experiment. The first one was to find the best training parameters for the next experiments. The second one was to see if randomly dropping one of the inputs of the model (either RGB/CIR or CHM) could help the model by pushing it to learn to make the best out of the two types of data.

The different parameters that are tested here are:

- "Learn. rate": the initial learning rate.
- "Prob. drop": the probability to drop either RGB/CIR or CHM. The probability is the same for the two types, which means that if the displayed value is 0.1, then all data will be used 80% of the time, while only RGB/CIR and only CHM both happen 10% of the time.
- "Accum. count": the accumulation count, which means the amount of training data to process and compute the loss on before performing gradient back-propagation.

As you can see on [@fig-training-parameters-experiments], sortedAP reaches at best values just above 0.3. The reason why the column name is "Best sortedAP" is due to the dataset being too small. Since the dataset is small, the training process overfits quickly, and the model doesn't have enough training steps to have confidence scores which reach very high values. As a consequence, it is difficult to know beforehand which confidence threshold to choose. Therefore, the sortedAP metric is computed over several different confidence thresholds, and the one that gives the best value of sortedAP is kept.

With this experiment, we can see that a learning rate of 0.01 seems to make the training too much unstable, while 0.001 doesn't give very high score. Then, we can also see how unstable the training process is in general, which comes mostly from the dataset being too small. However, a learning rate between 0.0025 and 0.006 seems to give the most stable results, when the drop probability is 0. This seems to show that the idea of randomly dropping one of the two inputs doesn't really help the model to learn.

```{python}
#| label: fig-training-parameters-experiments
#| fig-cap: "Results with different training parameters for all experiments"
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("../data/training_params_experiment_all.csv")
df = df[df["Data used for evaluation"] == "RGB, CIR and CHM"]
df["Prob. drop"] = df["proba_drop_chm"]
df.rename(columns = {
    "accumulate": "Accum. count",
    "Data used for evaluation": "Evaluation data",
    "lr": "Learn. rate",
    "repartition_name": "Exp. name"
  },
  inplace=True)
sns.set_style("ticks", {"axes.grid": True})
sns.catplot(
    data=df,
    kind="swarm",
    x="Accum. count",
    y="Best sortedAP",
    hue="Exp. name",
    hue_order=["exp0", "exp1", "exp2", "exp3", "exp4"],
    col="Learn. rate",
    row="Prob. drop",
    margin_titles=True,
    height=2,
    aspect=1,
    palette="colorblind",
    s=25
)
plt.show()
```

In the next graph ([@fig-training-parameters-data]), we can see more results for the same experiments. Here, the results are colored according to the data that we use to evaluate the model. In blue, we see the value of sortedAP when we evaluate the model with the CHM layers data and dummy zero arrays as RGB/CIR data. These dummy arrays are also those that are used as input when one of the channel is dropped during training, when we have a drop probability larger than 0. Some interesting patterns appear in some of the cells in this plot. Firstly, it looks like randomly dropping one of the two inputs with the same probability has a much larger influence over the results using RGB/CIR than CHM. While CHM gives better results than RGB/CIR when always training using everything, RGB/CIR seems to perform better alone when also trained alone, even outperforming the combination of both inputs in certain cases.

```{python}
#| label: fig-training-parameters-data
#| fig-cap: "Results with different training parameters for all evaluation data setups"
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

df = pd.read_csv("../data/training_params_experiment_all.csv")
df.sort_values(
    by=["proba_drop_chm", "Data used for evaluation"],
    inplace=True,
)
df["Prob. drop"] = df["proba_drop_chm"]
df.rename(columns = {
    "accumulate": "Accum. count",
    "Data used for evaluation": "Evaluation data",
    "lr": "Learn. rate"
  },
  inplace=True)
sns.set_style("ticks", {"axes.grid": True})
g = sns.catplot(
    data=df,
    kind="swarm",
    x="Accum. count",
    y="Best sortedAP",
    hue="Evaluation data",
    col="Learn. rate",
    row="Prob. drop",
    margin_titles=True,
    height=1.7,
    aspect=1.2,
    palette="colorblind",
    s=10
)

g.tight_layout()
plt.subplots_adjust(right=0.79)

plt.show()
```

From the results of this experiment, I decided to pick the following parameters for the next experiments:

- Initial learning rate: 0.004
- Drop probability: 0
- Accumulation count: 10

## CHM layers {#sec-results-chm}

The goal of the second experiment was to test the model with different layers of CHM, to see whether more layers can improve the results. I tried two different ways to cut the point cloud into intervals. In both cases, we first define the height thresholds $[t_1, t_2, \cdots, t_n]$ that will be used. Then, there are two possibilities for the height intervals to use to compute the CHM layers:
$$
\begin{array}{rl}
\text{Disjoint = True:} & [[-\infty, t_1], [t_1, t_2],[t_2, t_3], \cdots, [t_{n-1}, t_n], [t_n, \infty]] \\
\text{Disjoint = False:} & [[-\infty, t_1], [-\infty, t_2], \cdots, [-\infty, t_n], [-\infty, \infty]]
\end{array}
$$

The results of this experiment can be found in [@fig-chm-layers]. To experiment on other parameters, half of the models were trained to be agnostic while the other half was not, to see if the way trees are separated in the four classes has an impact on performance, either facilitating the learning or hindering the generalization. On this plot, the borders correspond to the list of height thresholds $[t_1, t_2, \cdots, t_n]$.

```{python}
#| label: fig-chm-layers
#| fig-cap: "Results with different CHM layers"
#| fig-width: 5
#| warning: false

from typing import List, Tuple

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import json

df = pd.read_csv("../data/chm_z_layers_all.csv")
df.sort_values(
    by=["chm_z_layers", "Data used for evaluation"],
    inplace=True,
)

def convert_to_list_with_inf(x):
    x = x.replace('inf', 'Infinity')
    try:
        return json.loads(x)
    except (ValueError, SyntaxError) as e:
        print(f"Error converting {x}: {e}")
        return x

df["chm_z_layers"] = df["chm_z_layers"].apply(convert_to_list_with_inf)

def chm_z_layers_to_str(chm_z_layers: List[Tuple[int, int]]) -> pd.Series:
  if all([chm_z_layer[0] == -np.inf for chm_z_layer in chm_z_layers]):
    disjoint = False
    borders = [chm_z_layers[i][1] for i in range(len(chm_z_layers) - 1)]
    return pd.Series([disjoint, str(borders).replace(" ", "")])
  else:
    disjoint = True
    borders = [chm_z_layers[i][1] for i in range(len(chm_z_layers) - 1)]
    return pd.Series([disjoint, str(borders).replace(" ", "")])

df[["Disjoint", "Borders"]] = df.apply(lambda x: chm_z_layers_to_str(x["chm_z_layers"]), axis=1)
condition = (df["Disjoint"] == False) & (df["Borders"] == "[]")
rows_to_duplicate = df[condition]
duplicated_rows = rows_to_duplicate.copy()
duplicated_rows["Disjoint"] = True
df = pd.concat([df, duplicated_rows], ignore_index=True)

df["Exp. name"] = df.apply(lambda x: x["repartition_name"].replace("exp", ""), axis=1)
df.rename(columns = {
    "Data used for evaluation": "Evaluation data",
    "best_epoch": "Best epoch",
    "agnostic": "Agnostic"
  },
  inplace=True)

df.sort_values(
    by=["Agnostic", "Disjoint"],
    ascending=[False, False],
    inplace=True,
)

df["Agnostic_Disjoint"] = "Agnostic = " + df["Agnostic"].astype(str) + "\nDisjoint = " + df["Disjoint"].astype(str)

sns.set_style("ticks", {"axes.grid": True})
g = sns.catplot(
    data=df,
    kind="swarm",
    x="Exp. name",
    order=["0", "1", "2", "3", "4"],
    y="Best sortedAP",
    hue="Evaluation data",
    col="Borders",
    row="Agnostic_Disjoint",
    margin_titles=True,
    height=1.7,
    aspect=1.2,
    palette="colorblind",
    s=10
)

g.set_titles(row_template='{row_name}')
g.tight_layout(h_pad=-6)
plt.subplots_adjust(right=0.78)

plt.show()
```

These results are difficult to interpret. The first clear effect that we can notice for the agnostic models is that in the disjoint method (first row starting from the top), when separating the LiDAR point cloud in too many layers, the model doesn't manage to make extract information from the CHM. When using only one CHM layer on the whole point cloud (which corresponds to (Borders = [])), we can see that the model performs poorly when using only one data type, but as well as the other models when using both. The explanation for this may be related to the data augmentation pipeline, because there is random channel dropout during the training. When there are multiple channels in the input, almost all of them can be randomly dropped out, with the limitation that at least one channel will always remain. This is the reason why there are red, green and blue (two channels dropped) but also purple and cyan (one channel dropped) images in [@fig-dataset-augmentation]. But when there is only one channel, this channel is never dropped, which doesn't force the model to learn how to use only part of the data.

Otherwise, it is hard to draw any conclusion from the rest of the experiments. There is too much variation in the results, which shows once again how unstable the training process is. From these results and the previous paragraph, one interpretation could be that the augmentation pipeline is related to this instability, as the results with the basic CHM layer have a much smaller variance.

## Covered trees

Then, if we try to look at the performance of the models on the covered trees, which are called "Tree_low_hard" in [@fig-results-hard-trees], it is also hard to draw any conclusion. The models have mainly learnt to find trees with the generic "Tree" label, and they are seemingly equally bad at finding the other classes of trees. In [@fig-results-hard-trees], we can see the performance of two models trained with the same repartition of the data into training, validation and test set. Both models were not agnostic, which means that they learnt to detect each class of trees and label them properly. The one of the left uses the largest number of CHM layers (with Borders = [1, 2, 3, 5, 7, 10, 15, 20] and Disjoint = False), whereas the one on the right only uses the default CHM layer (Borders = []).

::: {#fig-results-hard-trees layout="[[48, -4, 48], [-4], [48, -4, 48], [-4], [48, -4, 48]]" fig-pos="H"}

![Results with all layers on training set](../data/covered_trees/all_layers/ap_iou_per_label_train-set_RGB_CIR_CHM.png){#fig-results-hard-trees-all-train fig-align="center"}

![Results with one layer on training set](../data/covered_trees/one_layer/ap_iou_per_label_train-set_RGB_CIR_CHM.png){#fig-results-hard-trees-one-train fig-align="center"}

![Results with all layers on validation set](../data/covered_trees/all_layers/ap_iou_per_label_val-set_RGB_CIR_CHM.png){#fig-results-hard-trees-all-val fig-align="center"}

![Results with one layer on validation set](../data/covered_trees/one_layer/ap_iou_per_label_val-set_RGB_CIR_CHM.png){#fig-results-hard-trees-one-val fig-align="center"}

![Results with all layers on test set](../data/covered_trees/all_layers/ap_iou_per_label_test-set_RGB_CIR_CHM.png){#fig-results-hard-trees-all-test fig-align="center"}

![Results with one layer on test set](../data/covered_trees/one_layer/ap_iou_per_label_test-set_RGB_CIR_CHM.png){#fig-results-hard-trees-one-test fig-align="center"}

sortedAP curve with different CHM layers on training/validation/test set
:::

Unfortunately, these two examples are really representative of the results of the other models that were trained. There are only 207 covered trees in the dataset, which is too small to get the models to learn to identify them and get interesting results when comparing different configurations of CHM layers. Most of the differences that we see come from the normal random variation during the training.

## Visual results

In [@fig-results-visual] are the outputs of trained models on one instance from each of the training, validation and test sets. The models used are those which results are shown in [@fig-results-hard-trees].

::: {#fig-results-visual layout="[[48, -4, 48, -4, 48], [-4], [48, -4, 48, -4, 48], [-4], [48, -4, 48, -4, 48]]" fig-pos="H"}

![Ground-truth boxes on one training set instance](../images/Exp2_results/Ground_truth/Pred_boxes_train.png){#fig-results-visual-gt-train fig-align="center"}

![Predicted boxes with all layers on one training set instance](../images/Exp2_results/All_layers/Pred_boxes_train.png){#fig-results-visual-all-train fig-align="center"}

![Predicted boxes with one layer on one training set instance](../images/Exp2_results/One_layer/Pred_boxes_train.png){#fig-results-visual-one-train fig-align="center"}

![Ground-truth boxes on one validation set instance](../images/Exp2_results/Ground_truth/Pred_boxes_val.png){#fig-results-visual-gt-val fig-align="center"}

![Predicted boxes all layers on one validation set instance](../images/Exp2_results/All_layers/Pred_boxes_val.png){#fig-results-visual-all-val fig-align="center"}

![Predicted boxes one layer on one validation set instance](../images/Exp2_results/One_layer/Pred_boxes_val.png){#fig-results-visual-one-val fig-align="center"}

![Ground-truth boxes on one test set instance](../images/Exp2_results/Ground_truth/Pred_boxes_test.png){#fig-results-visual-gt-test fig-align="center"}

![Predicted boxes all layers on one test set instance](../images/Exp2_results/All_layers/Pred_boxes_test.png){#fig-results-visual-all-test fig-align="center"}

![Predicted boxes one layer on one test set instance](../images/Exp2_results/One_layer/Pred_boxes_test.png){#fig-results-visual-one-test fig-align="center"}

Ground-truth boxes and predictions from two models
:::

The confidence thresholds used to remove low confidence predictions are the ones that gave the best sortedAP values in [@fig-results-hard-trees]. One aspect that is partly visible here is that all the categories except the basic "Tree" have confidence scores which are so low that they are rejected after the selection with the confidence threshold. This is the case on the whole dataset, including the training set, and shows how the models learns more slowly to find the other categories.