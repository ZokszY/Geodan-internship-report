# Objectives and motivations

In this section, I will explain the objectives that I set for this internship and the motivations that led to them.

## General idea

The basis for this internship was to look at deep learning models to detect trees using LiDAR and aerial images. In four months, it would have been difficult to dive into the literature, think about a completely new approach and develop it. Therefore, I wanted to find an interesting and not too complicated deep learning model, and try a few changes that would hopefully improve the results.

This idea was also reinforced by the decision to create my own dataset, which stemmed from two reasons. The first reason was the small number of openly available tree annotation datasets which contained both LiDAR and RGB data. I therefore thought that creating a new dataset and making it available could be a great contribution. The second reason was to have more control over the definition and the characteristics of the dataset, to be able to experiment on the detection of specific trees.

## Covered trees {#sec-obj-covered_trees}

The main thing that I wanted to experiment on was the possibility to make better use of the LiDAR point cloud to be able to detect covered trees. Covered trees are the trees which are located partially or completely under another tree crown. This makes them impossible to completely delineate when using only data that is visible from above. These trees are not meaningless or negligible, because as demonstrated in this paper [@lidar_benchmark_2], they can represent up to 50% of the trees in a forest.

However, doing this implied being able to process them on the whole pipeline. In practice, covered trees are never annotated in all the datasets that are created using only RGB images, because they are simply not visible. This means that creating my own dataset was the only solution to have a dataset containing all trees including covered trees and be able to easily identify them.

## Multiple layers of CHM

Being able to find covered trees meant finding a way to extract more information out of the LiDAR point cloud than what is contained in the CHM. In fact, the CHM only contains a very small part of the point cloud and doesn't really benefit from the 3D information that is contained inside the point cloud, only from its 3D appearance from above. This is particularly true when the point cloud is acquired in a season where trees don't have their leaves, because the LiDAR then goes deep into the tree more easily, and can find the trunk and many of the largest branches.

Therefore, getting information below the tree crown surface was mandatory to find covered trees. But it could also be helpful for the model to find better separations between each tree, thanks to having access to the branches and the trunks.

To do this, I wanted to stuck to a simple solution that would integrate well with the initial model and wouldn't require too many changes. The idea I implemented is therefore very simple. Instead of having only one CHM raster, the model will take multiple layers, each focusing on a different height interval. One way to do this, which is used in the third method of this paper [@lidar_benchmark], would be to use the previous CHM by removing all the points that are in the interval between the CHM height and 0.5Â m below, before computing an additional layer. Instead, I tried a more simple and straightforward way to do it, by removing all the points above a certain height threshold, and compute the CHM with the points that are left. Doing this for multiple height thresholds creates a more complete view of what the point cloud looks like at multiple levels, which gives a lot more information about the organization of the point cloud.

