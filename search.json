[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Internship Report",
    "section": "",
    "text": "Abstract\nThe goal of the internship was to study the combination of LiDAR point clouds and aerial images in a deep learning model to identify individual trees, and in particular those covered by other trees. To do this, I modified a model capable of merging LiDAR and RGB data to feed it with more information about the geometry below the canopy surface. This required to create my own tree dataset, using publicly available data from the Netherlands. A few interesting results emerged, but due to the dataset being too small, I couldn’t really draw conclusions about potential improvements of this new pipeline. Therefore, this new pipeline should be evaluated on a larger dataset to precisely determine its influence on the results.\nThe source code for this report can be found here1 and the online report can be found here2.",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Internship Report",
    "section": "",
    "text": "https://github.com/ZokszY/Geodan-internship-report↩︎\nhttps://zokszy.github.io/Geodan-internship-report↩︎",
    "crumbs": [
      "Abstract"
    ]
  },
  {
    "objectID": "qmd-files/acknowledgments.html",
    "href": "qmd-files/acknowledgments.html",
    "title": "Acknowledgments",
    "section": "",
    "text": "Firstly I would like to express my gratitude to Tom van Tilburg, who helped me a lot at every step of the internship. From data management and processing to the architecture of the model, he brought a lot of new ideas and encouraged me to question my own. Brian de Vogel also deserves my heartfelt thanks for his ideas and his unconditional help regarding any practical and material matter. Together with Tom, they took from their time to supervise me every week, answer to any of my questions and introduce me to several geo-spatial topics that I knew nothing about beforehand.\nThen I would like to thank the two people who made this internship possible in the first place. I am grateful to Henk Scholten, who supported my internship all along and enabled me to meet new people in several major institutions. A special thanks also to Rene Bruinink for his direct and sincere interest towards my spontaneous application, as well as for his transparency and honesty from the beginning of the internship discussion.\nFinally I am sincerely thankful to the rest of the Research Team, Anne Blankert, Bert Temme, Kamiel Verhelst, Stefan de Graaf and Tim Ebben for warmly welcoming and integrating me to the team. Loïc de Jong also deserves a special thanks for his help regarding many practical matters and administrative tasks that were specific to living in the Netherlands. He also supported me a lot in my adaptation to this new country, even though it is not so different from France.",
    "crumbs": [
      "Acknowledgments"
    ]
  },
  {
    "objectID": "qmd-files/intro.html",
    "href": "qmd-files/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Individual detection of trees is an increasingly important topic in computer vision, because of its numerous applications and the recent major improvements in object detection. Trees are more and more monitored, from forest carbon stocks to urban tree inventories, to allow for a better assessment of their evolution and a better management of forests and urban trees. Most of the tasks related to trees either require or would benefit from a method allowing precise delineation and separation of trees as individual entities.\nTo identify trees, the two most common types of data used are LiDAR point clouds and visible/hyperspectral images. The two types of data are indeed complementary, as point clouds capture geometric shapes, while images capture colors. However, combining them into a single pipeline is not an easy task because they inherently have a very different spatial repartition and encoding. This difficulty holds true for both analytical methods and deep learning models. However, some deep learning models still manage to merge point clouds and images by creating rasters from the surface of the point cloud, which can then be handled similarly to visible and hyperspectral images.\nIn this work, I focused on one specific deep learning model, and tried to improve it by extracting more information from the LiDAR point cloud, which also contains a lot of data below the canopy surface. Thank to this modification, I also hoped to be able to experiment on the ability of the model to detect covered trees, which are trees invisible from above because of a larger tree. This is a task that is completely impossible for the initial model before my modifications, because all the data it takes as input is limited to what is visible from above.\nTo experiment on this, I had to create my own tree annotations dataset, using high-quality data available on the whole of the Netherlands, and hand-annotations for the trees. I then implemented the whole training pipeline and the model from scratch, before conducting experiments to assess the impact of my modifications.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "qmd-files/sota.html",
    "href": "qmd-files/sota.html",
    "title": "1  State-of-the-art",
    "section": "",
    "text": "1.1 Computer vision tasks related to trees\nBefore talking about models and datasets, let’s define properly the task that this project focused on, in the midst of all the various computer vision tasks, and specifically those related to tree detection.\nThe first main differentiation between tree recognition tasks comes from the acquisition of the data. There are some very different tasks and methods using either ground data or aerial/satellite data. This is especially true when focusing on urban trees, since a lot of street view data is available (Arevalo-Ramirez et al. 2024).\nThis leads to the second variation, which is related to the kind of environment that we are interested in. There are mainly three types of environments, which among other things, influence the organization of the trees in space: urban areas, tree plantations and forests. This is important, because the tasks and the difficulty depends on the type of environment. Tree plantations are much easier to work with than completely wild forests, while urban areas contain various levels of difficulty ranging from alignment trees to private and disorganized gardens and parks. For this project, we mainly focused on urban areas, but everything should still be applicable to tree plantations and forests.\nThen, the four fundamental computer vision tasks have their application when dealing with trees (Safonova et al. 2021):\nThese generic tasks can be extended by trying to get more information about the trees. The most common information are the species and the height, but some models also try to predict the health of the trees, or their carbon stock.\nIn this work, the task that is tackled is the detection of trees, with a special classification between several labels related to the discrepancies between the different kinds of data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>State-of-the-art</span>"
    ]
  },
  {
    "objectID": "qmd-files/sota.html#computer-vision-tasks-related-to-trees",
    "href": "qmd-files/sota.html#computer-vision-tasks-related-to-trees",
    "title": "1  State-of-the-art",
    "section": "",
    "text": "Classification, which consists in assigning one class to an image, although this is quite rare for airborne tree applications since there are multiple trees on each image most of the time\nDetection, which consists in detecting objects and placing boxes around them\nSemantic segmentation, which consists in associating a label to every pixel of an image\nInstance segmentation, which consists in adding a layer of complexity to semantic segmentation by also differentiating between the different instances of each class",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>State-of-the-art</span>"
    ]
  },
  {
    "objectID": "qmd-files/sota.html#datasets",
    "href": "qmd-files/sota.html#datasets",
    "title": "1  State-of-the-art",
    "section": "1.2 Datasets",
    "text": "1.2 Datasets\n\n1.2.1 Requirements\nBefore presenting the different promising datasets and the reasons why they were not fully usable for the project, let’s enumerate the different conditions and requirements for the tree instance segmentation task:\n\nMultiple types of data:\n\nAerial RGB images\nLiDAR point clouds (preferably aerial)\nAerial infrared (CIR) images (optional)\n\nTree crown annotations or bounding boxes\nHigh-enough resolution:\n\nFor images, about 25 cm\nFor point clouds, about 10 cm\n\n\nHere are the explanations for these requirements. As for the types of data, RGB images and point clouds are required to experiment on the ability of the model to combine the two very different kinds of information they hold. Infrared data can also improve tree detection, but it was optional for this work because RGB images are enough to study the combination. Regarding tree annotations, it is necessary to have a way to spatially identify them individually, using crown contours or simply bounding boxes. Since the model outputs bounding boxes, any kind of other format can easily be transformed to bounding boxes. Finally, the resolution has to be high enough to identify individual trees. For the point clouds especially, the whole idea is to see if and how the topology of the trees can be learnt, using at least the trunks and even the biggest branches if possible. Therefore, even if they are not really comparable, this is the reason why the required resolution is more precise for the point clouds.\n\n\n\n1.2.2 Existing tree datasets\nAs explained above, there are quite a lot of requirements to fulfill to have a complete dataset usable for the task. This means that almost all the available datasets are missing something, as they are mainly focusing on using one kind of data and try to make the most out of it, instead of trying to use all the types of data together.\nThe most comprehensive list of tree annotations datasets was published in OpenForest (Ouaknine et al. 2023). FoMo-Bench (Bountos, Ouaknine, and Rolnick 2023) also lists several interesting datasets, even though most of them can also be found in OpenForest. Without enumerating all of them, there are multiple kinds of datasets that all have their own flaws regarding the requirements of this work.\nFirstly, there are the forest inventories. TALLO (Jucker et al. 2022) is probably the most interesting one in this category, because it contains a lot of spatial information about almost 500K trees, with their locations, their crown radii and their heights. Therefore, everything needed to localize trees is in the dataset. However, I didn’t manage to find RGB images or LiDAR point clouds of the areas where the trees are located, making it impossible to use these annotations to train tree detection.\nSecondly, there are the RGB datasets. ReforesTree (Reiersen et al. 2022) and MillionTrees (B. Weinstein 2023) are two of them and the quality of their images are high. The only drawback of these datasets is obviously that they don’t provide any kind of point cloud, which makes them unsuitable for the task.\nThirdly, there are the LiDAR datasets, such as (Kalinicheva et al. 2022) and (Puliti et al. 2023). Similarly to RGB datasets, they lack one of the data source for the task I worked on. But unlike them, they have the advantage that the missing data could be much easier to acquire from another source, since RGB aerial or satellite images are much more common than LiDAR point clouds. However, this solution was abandoned for two main reasons. First it is often quite challenging to find the exact locations where the point clouds were acquired. Then, even when the location is known, it is often in the middle of a forest where the quality of openly available satellite imagery is still very low.\nFinally, I also found two datasets that had RGB and LiDAR components. The first one is MDAS (Hu et al. 2023). This benchmark dataset encompasses RGB images, hyperspectral images and Digital Surface Models (DSM). There are however two major flaws. The obvious one is that this dataset was created with land semantic segmentation tasks in mind, so there is no tree annotations. The less obvious one is that a DSM is not a point cloud, even though it is some kind of 3D information and is often created using a LiDAR point cloud. As a consequence, this substantially limits the ability to experiment with the point cloud.\nThe only real dataset with RGB and LiDAR comes from NEON (B. Weinstein, Marconi, and White 2022). This dataset contains exactly all the data I was looking for, with RGB images, hyperspectral images and LiDAR point clouds. With 30975 tree annotations, it is also a quite large dataset, spanning across multiple various forests. The main reason why I decided not to use it in the end is the quality of the data, which is not bad but not as great as the one from the data available for the Netherlands, which I will talk about in the next section Section 1.2.3.\n\n\n1.2.3 Public data\nAfter rejecting all the available datasets I had found, the only remaining solution was to create my own dataset. I won’t dive too much in this process that I will explain in Chapter 3. I just want to mention all the publicly available raw data that I used or could have used to create this custom dataset.\nFor practical reasons, the two countries where I mostly searched for available data are France and the Netherlands. I was looking for three different data types independently:\n\nRGB (and if possible CIR) images\nLiDAR point clouds\nTree annotations\n\nThese three types of data are available in similar ways in both countries, although the Netherlands have a small edge over France. RGB images are really easy to find in France with the BD ORTHO (Institut national de l’information géographique et forestière (IGN) 2021) and in the Netherlands with the Luchtfotos (Beeldmateriaal Nederland 2024), but the resolution is better in the Netherlands (8 cm vs 20 cm). Hyperspectral images are also available in both countries, although for those the resolution is only 25 cm in the Netherlands.\nAs for LiDAR point clouds, the Netherlands have a small edge over France, because they have already completed their forth version covering the whole country with AHN4 (Actueel Hoogtebestand Nederland 2020), and are working on the fifth version. In France, data acquisition for the first LiDAR point cloud covering the whole country started a few years ago (Institut national de l’information géographique et forestière (IGN) 2020). It is not yet finished, even though the data is already available for half of the country. The other advantage of the data from the Netherlands regarding LiDAR point clouds is that all flights are performed during winter, which allows light beams to penetrate more deeply in trees and reach trunks and branches. This is not the case in France.\nThe part that is missing in both countries is related to tree annotations. Many municipalities have datasets containing information about all the public trees they handle. This is for example the case for Amsterdam (Gemeente Amsterdam 2024) and Bordeaux (Bordeaux Métropole 2024). However, these datasets cannot really be used as ground truth for a custom dataset for several reasons. First, many of them do not contain coordinates indicating the position of each tree in the city. Then, even those that contain coordinates are most of the time missing any kind of information allowing to deduce a bounding box for the tree crowns. Finally, even if they did contain everything, they only focus on public trees, and are missing every single tree located in a private area. Since public and private areas are obviously imbricated in all cities, it means that any area we try to train the model on would be missing all the private trees, making the training process impossible because we cannot have only a partial annotation of images.\nThe other tree annotation source that we could have used is Boomregister (Coöperatief Boomregister U.A. 2014). This work covers the whole of the Netherlands, including public and private trees. However, the precision of the masks is far from perfect, and many trees are missing or incorrectly segmented, especially when they are less than 9 m heigh or have a crown diameter smaller than 4 m. Therefore, even though it is a very impressive piece of work, I thought that it could not be used as training data for a deep learning models due to its biases and imperfections.\n\n\n1.2.4 Dataset augmentation techniques\nWhen a dataset is too small to train a model, there are several ways of artificially enlarging it.\nThe most common way is to randomly apply deterministic or random transformations to the data, during the training process, to be able to generate several unique and different realistic data instances from one real data instance. There are a lot of different transformations that can be applied to images, divided into two categories: pixel-level and spatial-level (Buslaev et al. 2020). Pixel-level transformations modify the value of individual pixels, by applying different filters, such as random noise, color shifts and more complex effects like fog and sun flare. Spatial-level transformations modify the spatial arrangement of the image, without changing the pixel values. In other words, these transformations move the pixels in the image. These transformations range from simple rotations and croppings to complex spatial distortions. In the end, all these transformations are simply producing one artificial image out of one real image.\nAnother way to enlarge a dataset is to instead generate completely new input data sharing the same properties as the initial dataset. This can be done using Generative Adversarial Networks (GAN). These models usually have two parts, a generator and a discriminator, which are trained in parallel. The generator learns to produce realistic artificial data, while the discriminator learns to discriminate between real data and artificial data produced by the generator. If the training is successful, we can then use the generator and random seeds to generate random but realistic artificial data similar to the dataset. This method has for example been successfully used to generate artificial tree height maps (Sun et al. 2022).\nHowever, training GANs can be very unstable, and I haven’t found any paper applying this technique to generate LiDAR and RGB data at the same time. The artificial instances would need to be consistent between the two types of data, which might be very difficult. Therefore, I only used the random image transformations during the training process, because the chances of training of successful GAN seemed too low to be worth it.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>State-of-the-art</span>"
    ]
  },
  {
    "objectID": "qmd-files/sota.html#algorithms-and-models",
    "href": "qmd-files/sota.html#algorithms-and-models",
    "title": "1  State-of-the-art",
    "section": "1.3 Algorithms and models",
    "text": "1.3 Algorithms and models\nIn this section, the different algorithms and methods are grouped according to the type of data they use as input.\n\n1.3.1 Images only\nFirst, there are methods that perform tree detection using only visible or hyperspectral images or both. Several different algorithms have been developed to analytically delineate tree crowns from RGB images, by using the particular shape of the trees and its effect on images (Gomes and Maillard 2016). Without diving into the details, here are a few of them. The watershed algorithm identifies trees to inverted watersheds in the grey-scale image and tree crowns frontiers are found by incrementally flooding the watersheds (Vincent and Soille 1991). The local maxima filtering uses the intensity of the pixels in the grey-scale image to identify the brightest points locally and use them as treetops (Wulder, Niemann, and Goodenough 2000). Reversely, the valley-following algorithm uses the darkest pixels which are considered as the junctions between the trees since shaded areas are the lower part of the tree crowns (Gougeon et al. 1998). Another interesting algorithm is template matching. This algorithm simulates the appearance of simple tree templates with the light effects, and tries to identify similar patterns in the grey-scale image (Pollock 1996). Combinations of these techniques and others have also been proposed.\nBut with the recent developments of deep learning in image analysis, deep learning models are increasingly used to detect trees using RGB images. In some cases, deep learning is used to extract features that can then be the input of one of the algorithms described above. One example is the use of two neural networks to predict masks, outlines and distance transforms which can then be the input of a watershed algorithm (Freudenberg, Magdon, and Nölke 2022). In other cases, a deep learning model is responsible of directly detecting tree masks or bounding boxes, often using CNNs, given the images (B. G. Weinstein et al. 2020).\n\n\n1.3.2 LiDAR only\nReversely, some of the methods to identify individual trees use LiDAR data only. There are a lot of different ways to use and analyze point clouds, but the one that is mostly used for trees is based on height maps, or Canopy Height Models (CHM).\nA CHM is a raster computed as the subtraction of the Digital Terrain Model (DTM) to the Digital Surface Model (DSM). What it means is that a CHM contains the height above ground of the highest point in the area corresponding to each pixel. This CHM can for example be used as the input raster for the watershed algorithm, as it contains the height values that can be used to determine local maxima (Kwak et al. 2007). A lot of different analytical methods and variations of the simple CHM were proposed to perform individual tree detection, but in the end, most of them still use the concept of local maxima (Eysn et al. 2015; Wang et al. 2016). A CHM can also be used as the input of any kind of convolutional neural network (CNN) because it is shaped exactly like any image. This allows to use a lot of different techniques usually applied to object detection in images.\nThen, even though I finally used an approach similar to the CHM, I want to mention other kinds of deep learning techniques that exist and could potentially leverage all the information contained in a point cloud. These techniques can be divided in two categories: projection-based and point-based methods (Diab, Kashef, and Shaker 2022). The main difference between the two is that projection-based techniques are based on grids while point-based methods take unstructured point clouds as input. Among projection-based methods, the most basic method is 2D CNN, which is how CHM can be processed. Then, multiview representation tries to tackle the 3D aspect by projecting the point cloud in multiple directions before merging them together. To really deal with 3D data, volumetric grid representation consists in using 3D occupancy grids, which are processed using 3D CNNs. Among point-based methods, there are methods based on PointNet, which are able to extract features and perform the classical computer vision tasks by taking point clouds as input. Finally, Convolutional Point Networks use a continuous generalization of convolutions to apply convolution kernels to arbitrarily distributed point clouds.\n\n\n1.3.3 LiDAR and images\nLet’s now talk about the models of interest for this work, which are machine learning pipelines using both LiDAR point cloud data and RGB images.\nOne example is a pipeline which uses a watershed algorithm to extract crown boundaries, before extracting individual tree features from the LiDAR point cloud, hyperspectral and RGB images (Qin et al. 2022). These features are then used by a random forest classifier to identify which species the tree belongs to. This pipeline therefore makes the most out of all data to identify species, but sticks to an improved variant of the watershed algorithm for individual tree segmentation, which only uses a CHM raster.\nOther works focused on using only one model that is able to take both the CHM and the RGB data as input and combine them to make the most out of all the available data. Among other models, there are for example ACE R-CNN (Li et al. 2022), an evolution of Mask region-based convolution neural network (Mask R-CNN) and AMF GD YOLOv8 (Zhong et al. 2024), an evolution of YOLOv8. These two models have proven to give much better results when using both the images and the LiDAR data as a CHM than when using only one of them.\nIn this work, we focus on AMF GD YOLOv8, which looks promising and flexible thanks to the flexibility of YOLOv8, which has different variants dealing with different computer vision tasks.\n\n\n\n\nActueel Hoogtebestand Nederland. 2020. “AHN4 - Actual Height Model of the Netherlands.” https://www.ahn.nl/.\n\n\nArevalo-Ramirez, Tito, Anali Alfaro, José Figueroa, Mauricio Ponce-Donoso, Jose M. Saavedra, Matías Recabarren, and José Delpiano. 2024. “Challenges for Computer Vision as a Tool for Screening Urban Trees Through Street-View Images.” Urban Forestry & Urban Greening 95: 128316. https://doi.org/10.1016/j.ufug.2024.128316.\n\n\nBeeldmateriaal Nederland. 2024. “Luchtfoto’s (Aerial Photographs).” https://www.beeldmateriaal.nl/luchtfotos.\n\n\nBordeaux Métropole. 2024. “Patrimoine arboré de Bordeaux Métropole (Tree Heritage of Bordeaux Metropole).” https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee.\n\n\nBountos, Nikolaos Ioannis, Arthur Ouaknine, and David Rolnick. 2023. “FoMo-Bench: A Multi-Modal, Multi-Scale and Multi-Task Forest Monitoring Benchmark for Remote Sensing Foundation Models.” arXiv Preprint arXiv:2312.10114. https://arxiv.org/abs/2312.10114.\n\n\nBuslaev, Alexander, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020. “Albumentations: Fast and Flexible Image Augmentations.” Information 11 (2). https://doi.org/10.3390/info11020125.\n\n\nCoöperatief Boomregister U.A. 2014. “Boom Register (Tree Register).” https://boomregister.nl/.\n\n\nDiab, Ahmed, Rasha Kashef, and Ahmed Shaker. 2022. “Deep Learning for LiDAR Point Cloud Classification in Remote Sensing.” Sensors (Basel) 22 (20): 7868. https://doi.org/10.3390/s22207868.\n\n\nEysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger, Jean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015. “A Benchmark of Lidar-Based Single Tree Detection Methods Using Heterogeneous Forest Data from the Alpine Space.” Forests 6 (5): 1721–47. https://doi.org/10.3390/f6051721.\n\n\nFreudenberg, Maximilian, Paul Magdon, and Nils Nölke. 2022. “Individual Tree Crown Delineation in High-Resolution Remote Sensing Images Based on u-Net.” Neural Computing and Applications 34 (24): 22197–207. https://doi.org/10.1007/s00521-022-07640-4.\n\n\nGemeente Amsterdam. 2024. “Bomenbestand Amsterdam (Amsterdam Tree Dataset).” https://maps.amsterdam.nl/open_geodata/?k=505.\n\n\nGomes, Marilia Ferreira, and Philippe Maillard. 2016. “Detection of Tree Crowns in Very High Spatial Resolution Images.” In Environmental Applications of Remote Sensing, edited by Maged Marghany. Rijeka: IntechOpen. https://doi.org/10.5772/62122.\n\n\nGougeon, François A et al. 1998. “Automatic Individual Tree Crown Delineation Using a Valley-Following Algorithm and Rule-Based System.” In Proc. International Forum on Automated Interpretation of High Spatial Resolution Digital Imagery for Forestry, Victoria, British Columbia, Canada, 11–23. Citeseer. https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf.\n\n\nHu, J., R. Liu, D. Hong, A. Camero, J. Yao, M. Schneider, F. Kurz, K. Segl, and X. X. Zhu. 2023. “MDAS: A New Multimodal Benchmark Dataset for Remote Sensing.” Earth System Science Data 15 (1): 113–31. https://doi.org/10.5194/essd-15-113-2023.\n\n\nInstitut national de l’information géographique et forestière (IGN). 2020. “LiDAR HD.” https://geoservices.ign.fr/lidarhd.\n\n\n———. 2021. “BD ORTHO.” https://geoservices.ign.fr/bdortho.\n\n\nJucker, Tommaso, Fabian Jörg Fischer, Jérôme Chave, David A. Coomes, John Caspersen, Arshad Ali, Grace Jopaul Loubota Panzou, et al. 2022. “Tallo: A Global Tree Allometry and Crown Architecture Database.” Global Change Biology 28 (17): 5254–68. https://doi.org/10.1111/gcb.16302.\n\n\nKalinicheva, Ekaterina, Loic Landrieu, Clément Mallet, and Nesrine Chehata. 2022. “Multi-Layer Modeling of Dense Vegetation from Aerial LiDAR Scans.” https://arxiv.org/abs/2204.11620.\n\n\nKwak, Doo-Ahn, Woo-Kyun Lee, Jun-Hak Lee, Greg S. Biging, and Peng Gong. 2007. “Detection of Individual Trees and Estimation of Tree Height Using LiDAR Data.” Journal of Forest Research 12 (6): 425–34. https://doi.org/10.1007/s10310-007-0041-9.\n\n\nLi, Yingbo, Guoqi Chai, Yueting Wang, Lingting Lei, and Xiaoli Zhang. 2022. “ACE r-CNN: An Attention Complementary and Edge Detection-Based Instance Segmentation Algorithm for Individual Tree Species Identification Using UAV RGB Images and LiDAR Data.” Remote Sensing 14 (13). https://doi.org/10.3390/rs14133035.\n\n\nOuaknine, Arthur, Teja Kattenborn, Etienne Laliberté, and David Rolnick. 2023. “OpenForest: A Data Catalogue for Machine Learning in Forest Monitoring.” https://arxiv.org/abs/2311.00277.\n\n\nPollock, Richard James. 1996. “The Automatic Recognition of Individual Trees in Aerial Images of Forests Based on a Synthetic Tree Crown Image Model.” PhD thesis, The University of British Columbia (Canada). https://dx.doi.org/10.14288/1.0051597.\n\n\nPuliti, Stefano, Grant Pearse, Peter Surový, Luke Wallace, Markus Hollaus, Maciej Wielgosz, and Rasmus Astrup. 2023. “FOR-Instance: A UAV Laser Scanning Benchmark Dataset for Semantic and Instance Segmentation of Individual Trees.” https://arxiv.org/abs/2309.01279.\n\n\nQin, Haiming, Weiqi Zhou, Yang Yao, and Weimin Wang. 2022. “Individual Tree Segmentation and Tree Species Classification in Subtropical Broadleaf Forests Using UAV-Based LiDAR, Hyperspectral, and Ultrahigh-Resolution RGB Data.” Remote Sensing of Environment 280: 113143. https://doi.org/10.1016/j.rse.2022.113143.\n\n\nReiersen, Gyri, David Dao, Björn Lütjens, Konstantin Klemmer, Kenza Amara, Attila Steinegger, Ce Zhang, and Xiaoxiang Zhu. 2022. “ReforesTree: A Dataset for Estimating Tropical Forest Carbon Stock with Deep Learning and Aerial Imagery.” https://arxiv.org/abs/2201.11192.\n\n\nSafonova, Anastasiia, Emilio Guirado, Yuriy Maglinets, Domingo Alcaraz-Segura, and Siham Tabik. 2021. “Olive Tree Biovolume from UAV Multi-Resolution Image Segmentation with Mask r-CNN.” Sensors 21 (5): 1617. https://doi.org/10.3390/s21051617.\n\n\nSun, Chenxin, Chengwei Huang, Huaiqing Zhang, Bangqian Chen, Feng An, Liwen Wang, and Ting Yun. 2022. “Individual Tree Crown Segmentation and Crown Width Extraction from a Heightmap Derived from Aerial Laser Scanning Data Using a Deep Learning Framework.” Frontiers in Plant Science 13. https://doi.org/10.3389/fpls.2022.914974.\n\n\nVincent, L., and P. Soille. 1991. “Watersheds in Digital Spaces: An Efficient Algorithm Based on Immersion Simulations.” IEEE Transactions on Pattern Analysis and Machine Intelligence 13 (6): 583–98. https://doi.org/10.1109/34.87344.\n\n\nWang, Yunsheng, Juha Hyyppä, Xinlian Liang, Harri Kaartinen, Xiaowei Yu, Eva Lindberg, Johan Holmgren, et al. 2016. “International Benchmarking of the Individual Tree Detection Methods for Modeling 3-d Canopy Structure for Silviculture and Forest Ecology Using Airborne Laser Scanning.” IEEE Transactions on Geoscience and Remote Sensing 54 (9): 5011–27. https://doi.org/10.1109/TGRS.2016.2543225.\n\n\nWeinstein, Ben. 2023. “MillionTrees.” 2023. https://milliontrees.idtrees.org/.\n\n\nWeinstein, Ben G., Sergio Marconi, Mélaine Aubry-Kientz, Gregoire Vincent, Henry Senyondo, and Ethan P. White. 2020. “DeepForest: A Python Package for RGB Deep Learning Tree Crown Delineation.” Methods in Ecology and Evolution 11 (12): 1743–51. https://doi.org/10.1111/2041-210X.13472.\n\n\nWeinstein, Ben, Sergio Marconi, and Ethan White. 2022. “Data for the NeonTreeEvaluation Benchmark (0.2.2).” Zenodo. https://doi.org/10.5281/zenodo.5914554.\n\n\nWulder, Mike, K.Olaf Niemann, and David G. Goodenough. 2000. “Local Maximum Filtering for the Extraction of Tree Locations and Basal Area from High Spatial Resolution Imagery.” Remote Sensing of Environment 73 (1): 103–14. https://doi.org/10.1016/S0034-4257(00)00101-2.\n\n\nZhong, Hao, Zheyu Zhang, Haoran Liu, Jinzhuo Wu, and Wenshu Lin. 2024. “Individual Tree Species Identification for Complex Coniferous and Broad-Leaved Mixed Forests Based on Deep Learning Combined with UAV LiDAR Data and RGB Images.” Forests 15 (2). https://doi.org/10.3390/f15020293.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>State-of-the-art</span>"
    ]
  },
  {
    "objectID": "qmd-files/objectives.html",
    "href": "qmd-files/objectives.html",
    "title": "2  Objectives and motivations",
    "section": "",
    "text": "2.1 General idea\nThe basis for this internship was to look at deep learning models to detect trees using LiDAR and aerial images. In fourth months, it would have been difficult to dive into the literature, think about a completely new approach and develop it. Therefore, I wanted to find an interesting and not too complicated deep learning model, and try a few changes that would hopefully improve the results.\nThis idea was also reinforced by the decision to create my own dataset, which stemmed from two reasons. The first reason was the small number of openly available tree annotation datasets which contained both LiDAR and RGB data. I therefore thought that creating a new dataset and making it available could be a great contribution. The second reason was to have more control over the definition and the characteristics of the dataset, to be able to experiment on the detection of specific trees.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objectives and motivations</span>"
    ]
  },
  {
    "objectID": "qmd-files/objectives.html#sec-obj-covered_trees",
    "href": "qmd-files/objectives.html#sec-obj-covered_trees",
    "title": "2  Objectives and motivations",
    "section": "2.2 Covered trees",
    "text": "2.2 Covered trees\nThe main thing that I wanted to experiment on was the possibility to make a better use of the LiDAR point cloud to be able to detect covered trees. Covered trees are the trees which are located partially or completely under another tree crown. This makes them impossible to completely delineate when using only data that is visible from above. These trees are not meaningless or negligible, because as demonstrated in this paper (Wang et al. 2016), they can represent up to 50% of the trees in a forest.\nHowever, doing this implied being able to process them on the whole pipeline. In practice, covered trees are never annotated in all the datasets that are created using only RGB images, because they are simply not visible. This means that creating my own dataset was the only solution to have a dataset containing all trees including covered trees and be able to easily identify them.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objectives and motivations</span>"
    ]
  },
  {
    "objectID": "qmd-files/objectives.html#multiple-layers-of-chm",
    "href": "qmd-files/objectives.html#multiple-layers-of-chm",
    "title": "2  Objectives and motivations",
    "section": "2.3 Multiple layers of CHM",
    "text": "2.3 Multiple layers of CHM\nBeing able to find covered trees meant finding a way to extract more information out of the LiDAR point cloud than what is contained in the CHM. In fact, the CHM only contains a very small part of the point cloud and doesn’t really benefit from the 3D shape that is contained inside the point cloud, only from its 3D appearance from above. This is particularly true when the point cloud is acquired in a season where trees don’t have their leaves, because the LiDAR then goes deep into the tree more easily, and can find the trunk and many of the largest branches.\nTherefore, getting information below the tree crown surface was mandatory to find covered trees. But it could also be helpful for the model to find better separations between each tree, thanks to having access to the branches and the trunks. Even though I didn’t end up asking the model to also identify the species, this is another task that could have been improved a lot if the model could use the architecture of the branches.\nTo do this, I wanted to stick with a simple solution that would integrate well with the initial model and wouldn’t require too many changes. The idea I implemented is therefore very simple. Instead of having only one CHM raster, I would have multiple layers, each focusing on a different height interval. There are many ways to do this, but due to a lack of time, I only really tried what seemed to me the easiest and most straightforward way to do it, which consists in removing all the points above a certain height threshold, and compute the CHM with the points that are left. When doing this for multiple height thresholds, we get an interesting view of what the point cloud looks like at multiple levels, which gives a lot more information about the organization of the point cloud. Another way to do this, which is used in the third method of this paper (Eysn et al. 2015), would be instead to use the previous CHM by removing all the points that are in the interval between the CHM height and 0.5 m below, before computing an additional layer. It could be interesting to see if this method works better than dropping the points at pre-determined heights.\n\n\n\n\nEysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger, Jean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015. “A Benchmark of Lidar-Based Single Tree Detection Methods Using Heterogeneous Forest Data from the Alpine Space.” Forests 6 (5): 1721–47. https://doi.org/10.3390/f6051721.\n\n\nWang, Yunsheng, Juha Hyyppä, Xinlian Liang, Harri Kaartinen, Xiaowei Yu, Eva Lindberg, Johan Holmgren, et al. 2016. “International Benchmarking of the Individual Tree Detection Methods for Modeling 3-d Canopy Structure for Silviculture and Forest Ecology Using Airborne Laser Scanning.” IEEE Transactions on Geoscience and Remote Sensing 54 (9): 5011–27. https://doi.org/10.1109/TGRS.2016.2543225.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Objectives and motivations</span>"
    ]
  },
  {
    "objectID": "qmd-files/dataset.html",
    "href": "qmd-files/dataset.html",
    "title": "3  Dataset creation",
    "section": "",
    "text": "3.1 Definition and content\nAs explained in the section Section 1.2.1, the main requirements of the dataset that I wanted to create were to contain at the same time LiDAR data, RGB data and CIR data, with simple bounding box annotations for all trees. And as explained in Section 2.2, all trees means also annotating trees that are partially or completely covered by other trees.\nThen, to make the most out of the point cloud resolution and the RGB images resolution, I decided to use a CHM resolution of 8 cm, which is also the resolution of the RGB images. However, the resolution of CIR images is 25 cm, which made it less optimal, but still usable.\nTo be able to get results even with a small dataset, I decided to focus on one specific area, to limit the diversity of trees and environments to something that could hopefully still be learnt with a small dataset. Therefore, the whole dataset is currently inside of a 1 km × 1 km square around Geodan office, in Amsterdam. It contains 2726 annotated trees spread over 241 images of size 640 px × 640 px i.e. 51.2 m × 51.2 m. All tree annotations have at least a bounding box, and some of them have a more accurate polygon representing the shape of the crown. There are four classes, which I will detail in the next section Section 3.2, and each tree belongs to one class.\nAnnotating all these trees took me about 100 hours, with a very high variation of the time spent on each tree depending on the complexity of the area. On Figure 3.1, you can see what the dataset finally looks like, with all data sources and bounding boxes.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset creation</span>"
    ]
  },
  {
    "objectID": "qmd-files/dataset.html#definition-and-content",
    "href": "qmd-files/dataset.html#definition-and-content",
    "title": "3  Dataset creation",
    "section": "",
    "text": "(a) RGB image\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) CIR image\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(c) LiDAR point cloud\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(d) CHM raster\n\n\n\n\n\n\n\nFigure 3.1: One data instance with ground-truth bounding boxes",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset creation</span>"
    ]
  },
  {
    "objectID": "qmd-files/dataset.html#sec-dataset-challenges",
    "href": "qmd-files/dataset.html#sec-dataset-challenges",
    "title": "3  Dataset creation",
    "section": "3.2 Challenges and solutions",
    "text": "3.2 Challenges and solutions\nThe creation of this dataset raised a number of challenges. The first one was the interval of time between the acquisition of the different types of data. While the point cloud data dated from 2020, the RGB images were acquired in 2023. It would have been possible to use images from 2021 or 2022 with the same resolution, but the quality of the 2023 images was much better. Consequently, there were a certain amount of changes regarding trees between these two periods of acquisition. Some large trees were cut off, while small trees were planted, sometimes even at the position of old trees that were previously cut off in the same time frame. For this reason, a non negligible number of trees were either present only in the point cloud, or only in the images. An example of such a situation can be found in Figure 3.2To try to handle this situation, I created two new class labels corresponding to these situation. This amounted up to 4 class labels:\n\n“Tree”: trees which are visible in the point cloud and the images\n“Tree_LiDAR”: trees which are visible in the point cloud only but would be visible in the images if they had been there during the acquisition\n“Tree_RGB”: trees which are visible in the images only but would be visible in the point cloud if they had been there during the acquisition\n“Tree_covered”: trees that are visible in the point cloud only because they are covered by other trees.\n\n\n\n\n\n\n\n\n\n\n\n\n(a) RGB image\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) CHM raster\n\n\n\n\n\n\n\nFigure 3.2: A tree that was cut off and replaced\n\n\n\nThe next challenge was the misalignment of images and point cloud. This misalignment comes from the images not being perfectly orthonormal. Point clouds don’t have this problem, because the data is acquired and represented in 3D, but images have to be projected to a 2D plane after being acquired with an angle that is not perfectly orthogonal to the plane. Despite the post-processing that was surely performed on the images, they are therefore not perfect, and there is a shift between the positions of each object in the point cloud and in the images. This shift cannot really be solved, because it depends on the position. Because of this misalignment, a choice had to be made as to where tree annotations should be placed, using either the point clouds or the RGB images. I chose to the RGB images as it is simpler to visualize and annotate, but there was not really a perfect choice.\nOn the example below (Figure 3.3), you can see two of the issues. First, you can see that a bounding box that is well-centered around the tree in the RGB image is completely off on the CIR image, and also not really centered on the CHM raster. Then, you can see that the bounding box is much smaller on the CHM, mainly for two reasons: the tree grew between the acquisition of the LiDAR point cloud and the RGB image and small branches on the outside of the tree are hard to capture for LiDAR beams.\n\n\n\n\n\n\n\n\n\n\n\n(a) RGB image\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) CIR image\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(c) LiDAR point cloud\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(d) CHM raster\n\n\n\n\n\n\n\nFigure 3.3: Example of data misalignment\n\n\n\nFinally, the last challenge comes from the definition of what we consider as a tree and what we don’t. There are two main sub-problems. The first one comes from the threshold to set between bushes and trees. Large bushes can be much larger than small trees, and sometimes have a similar shape. Therefore, it is hard to keep coherent rules when annotating them. The second sub-problem comes from multi-stemmed and close trees. It can be very difficult to see, even with the point cloud, if a there is only one tree with two or more trunks dividing at the bottom, or multiple trees which are simply close to one another. (Un)fortunately I know that I was not the only one to face this problem because it was also mentioned in another paper (Weinstein et al. 2019). In the end, it was just an unsolvable problem for which the most important was to remain consistent in the whole dataset.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset creation</span>"
    ]
  },
  {
    "objectID": "qmd-files/dataset.html#augmentation-methods",
    "href": "qmd-files/dataset.html#augmentation-methods",
    "title": "3  Dataset creation",
    "section": "3.3 Augmentation methods",
    "text": "3.3 Augmentation methods\nDataset augmentation methods are in the middle between dataset creation and deep learning model training, because they are a way to enhance the dataset but depend on the objective for which the model is trained. Their importance is inversely proportional with the size of the dataset, which made them very important for my small dataset.\nAs it was already explained in Section 1.2.4, I used Albumentations (Buslaev et al. 2020) to apply two types of augmentations: pixel-level and spatial-level.\nSpatial-level augmentations had to be in the exact same way to the whole dataset, to maintain the spatial coherence between RGB images, CIR images and the CHM layers. I used three different spatial transformations, applied with random parameters. The first one chooses one of the eight possible images we can get when flipping and rotating the image by angles that are multiples of 90°. The second one adds a perspective effect to the images. The third one adds a small distortion to the image.\nOn the contrary, pixel-level augmentations must be applied differently to RGB images and CHM layers because they represent different kinds of data, so the values of the pixels do not have the same meaning. In practice, a lot of transformations were conceived to reproduce camera effects on RGB images or to shift the color spectrum. Among others, I used random modifications of the brightness, the gamma value and added noise and a blurring effect randomly to RGB images. For both types of data, a channel dropout is also randomly applied, leaving a random number of channels and removing the others. A better way to augment the CHM data would have been to apply random displacements and deletions of points in the point cloud, before computing the CHM layers. However, these operations are too costly to be integrated in the training pipeline without consequently increasing the training time, so this idea was discarded.\nOn Figure 3.4, you can see an RGB image and 15 random augmentations of this image, generated with the transformations and the probabilities used during training. The most visible change happens when one or two color channels are dropped, but we can also see luminosity changes in images n°4 and 14, perspective changes in n°5, 8 and 13, blurring in n°1 and 14, and distortions in n°10 and 12. All these effects and some other less identifiable augmentations (like noise), are randomly combined to produce many different images, with bounding boxes being modified accordingly.\n\n\n\n\n\n\n\n\n\n\n\n(a) Initial image\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Transformed image n°1\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(c) Transformed image n°2\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(d) Transformed image n°3\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(e) Transformed image n°4\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(f) Transformed image n°5\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(g) Transformed image n°6\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(h) Transformed image n°7\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(i) Transformed image n°8\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(j) Transformed image n°9\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(k) Transformed image n°10\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(l) Transformed image n°11\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(m) Transformed image n°12\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(n) Transformed image n°13\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(o) Transformed image n°14\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(p) Transformed image n°15\n\n\n\n\n\n\n\nFigure 3.4: Examples of data augmentations on an RGB image with the probabilities used when training the model.\n\n\n\n\n\n\n\nBuslaev, Alexander, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex Parinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020. “Albumentations: Fast and Flexible Image Augmentations.” Information 11 (2). https://doi.org/10.3390/info11020125.\n\n\nWeinstein, Ben G., Sergio Marconi, Stephanie Bohlman, Alina Zare, and Ethan White. 2019. “Individual Tree-Crown Detection in RGB Imagery Using Semi-Supervised Deep Learning Neural Networks.” Remote Sensing 11 (11). https://doi.org/10.3390/rs11111309.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Dataset creation</span>"
    ]
  },
  {
    "objectID": "qmd-files/model.html",
    "href": "qmd-files/model.html",
    "title": "4  Model and training",
    "section": "",
    "text": "4.1 Model architecture\nThe architecture of the model is conceptually simple. The model takes two inputs in the form of two rasters with the same height and width. The two inputs are processed using the backbone of the YOLOv8 model (Redmon et al. 2016) to extract features at different scales. Then Attention Multi-level Fusion (AMF) layers are used to fuse the features of the two inputs at each scale level. Then, a Gather-and-Distribute (GD) mechanism is used to propagate information between the different scales. This mechanism fuses the features from all scales before redistributing them to the features, two times in a row. Finally, the features of the three smallest scales are fed into detection layers responsible for extracting bounding boxes and assigning confidence scores and class probabilities to them.\nIn practical terms, the input rasters have a shape of \\(640 \\times 640 \\times c_{\\text{RGB}}\\) and \\(640 \\times 640 \\times c_{\\text{CHM}}\\), where \\(c_{\\text{RGB}}\\) is equal to 6 when using RGB and CIR images, and 3 when using only one of them, and \\(c_{\\text{CHM}}\\) is the number of CHM layers that we decide to use for the model. Since the resolution that is used is 0.08 m, this means that each image spans over 51.2 m.\nThe only real modification that I made to the architecture compared to the initial paper is adding any number of channels in the CHM input, while we had \\(c_{\\text{CHM}} = 1\\) originally. Using CIR images in addition to RGB images is also new, but this is a less important modification.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model and training</span>"
    ]
  },
  {
    "objectID": "qmd-files/model.html#model-architecture",
    "href": "qmd-files/model.html#model-architecture",
    "title": "4  Model and training",
    "section": "",
    "text": "Model architecture",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model and training</span>"
    ]
  },
  {
    "objectID": "qmd-files/model.html#training-pipeline",
    "href": "qmd-files/model.html#training-pipeline",
    "title": "4  Model and training",
    "section": "4.2 Training pipeline",
    "text": "4.2 Training pipeline\nThe training pipeline consists of three steps. First, the data is pre-processed to create the inputs to feed into the model. Then, the training loop runs until the end condition is reached. Finally, the final model is evaluated on all the datasets.\n\n4.2.1 Data preprocessing\nData pre-processing is quite straightforward. The first step is to divide the dataset into a grid of \\(640 \\times 640\\) tiles. Then, all these tiles are placed into one of the training, validation and test sets.\nAs for RGB and CIR images, preprocessing only contains two simple steps: tiling the large images into small \\(640 \\times 640\\) images, and normalizing all images along each channel. When both data sources are used, RGB and CIR images are also merged into images with 6 channels, which will be the input of the model.\nAs for CHM layers, there are more steps. The first step is to compute a sort of flattened point cloud, by computing the DTM, which represents the height of the ground, and removing this height to the point cloud. Then, for each CHM layer, if the height interval is \\([z_\\text{bot}, z_\\text{top}]\\), we first extract all the points which have a height \\(h\\) such that \\(z_\\text{bot} \\leq h \\leq z_\\text{top}\\), and we then compute the DSM for this smaller point cloud. Since the ground height was already removed from the point cloud, this DSM is the CHM. Then, all the layers are merged into one raster with multiple channels and we normalize the whole raster with the average and the standard deviation over all channels. Finally, we can simply tile these rasters exactly like RGB and CIR images, which gives us the inputs of the model.\nAll these operations are conceptually simple, but they can be computationally expensive. Therefore, I had to put a certain effort into accelerating with different methods. First, I made sure to save the most important and generic elements to avoid useless computations every time the model is trained again, without saturating the memory. Then, I also implemented multi-threading for every possible step to improve the raw speed of preprocessing. Finally, performance is also the reason why normalization if performed during preprocessing instead of during the initialization of the data in the training loop.\n\n\n4.2.2 Training loop\nThe training loop is very generic, so I will only mention the most interesting parts. First, we use an Adam optimizer and a simple learning rate scheduler with a multiplier at each epoch i which is \\(1/\\sqrt{i+2}\\).\nThen, since the batch size cannot be very large because of the space required by all the images, there is the possibility to perform gradient accumulation, which means that backward propagation won’t be performed with each batch, but instead every two or more batches. The idea behind this is to add more stability to the training, since back-propagating on only a few images is prone to overfitting on a set of examples which are not representative of the whole dataset.\nAs for the criterion to stop the training session, we use the loss on the validation set. Once this loss didn’t improve during 50 iterations over the whole dataset, we stop and keep the model that had the best validation loss.\nBesides these details, the training loop is very generic. We loop over the entire training set with batches to compute the loss and perform gradient back-propagation,. Then we compute the loss on the validation set and store this loss as the metric that decides when to stop.\n\n\n4.2.3 Output postprocessing\nRegarding postprocessing of the output of the model, there a few things to mention. First, the model outputs a lot of bounding boxes, which have to be cleaned using two criteria. The first criterion is the confidence score. We can just set a threshold below which bounding boxes are discarded. The second criterion is the intersection over union (IoU) with other bounding boxes. IoU is a metrics used to quantify how similar two bounding boxes are. It is a value between 0 and 1, which formula is:\n\\[\n\\text{IoU}(A, B) = \\frac{\\text{area}(A \\cap B)}{\\text{area}(A \\cup B)}\n\\]\nUsing this metrics, we can detect bounding boxes which are too similar to each other, and simply keep the bounding box with the highest confidence score when two bounding boxes have an IoU larger than a certain threshold.\nFor the evaluation, the process is a little different, because we only perform the clean up relying on IoU, and keep all other bounding boxes. The main metric that we compute is called sortedAP (Chen et al. 2023), which is an evolution of the mean (point) average precision (mAP). mAP is defined as follows:\n\\[\n\\begin{array}{rcl}\n\\text{mAP} & = & \\frac{1}{N} \\sum\\limits_{t\\in T} \\text{AP}_t \\\\\n\\text{AP}_t & = & \\frac{{TP}_t}{{TP}_t + {FP}_t + {FN}_t}\n\\end{array}\n\\]\nwhere \\(T=\\{t_1, t_2, \\dots, t_N\\}\\) is a list of IoU threshold values, \\({TP}_t\\) are the true positives when the the IoU threshold is \\(t\\), \\({FP}_t\\) are false positives and \\({FN}_t\\) are false negatives. The reason why \\(TP\\), \\(FP\\) and \\(FN\\) depend on \\(t\\) is that a bounding box is considered to be true if its IoU with one of the ground-truth bounding boxes is larger than \\(t\\).\nsortedAP is an improvement over this method because there is no need to select a list of IoU threshold values. Predicted bounding boxes are sorted according to their confidence score which allows to compute AP incrementally for any value of \\(t\\). Then, the area of the curve of the AP with respect to the IoU threshold is used as a metric, between 0 and 1, 1 being the best possible value. You can see in Figure 4.1 an example of the output of the sortedAP method. The curves represent the AP at any IoU threshold value. Then, the integral of this curve gives the value of sortedAP, which is displayed in the legend here.\n\n\n\n\n\n\nFigure 4.1: Example of three sortedAP curves with the results of a model with three different data combinations\n\n\n\nFinally, it is necessary to choose a fixed confidence threshold to compute the curve, because this value decides which boxes will be kept and how the predicted and ground-truth boxes will be matched. Therefore, the value of the best confidence threshold has a very high impact over the value of sortedAP, and cannot easily be determined during the training, as the confidence of the model evolves quickly. Therefore, computing sortedAP for different confidence threshold allows to always estimate the best performance it can have. In Figure 4.2 we can see how the value of sortedAP evolves and how the confidence threshold values in the previous figure (Figure 4.1) were chosen.\n\n\n\n\n\n\nFigure 4.2: Example of the values of sortedAP depending on the confidence threshold\n\n\n\n\n\n\n\nChen, Long, Yuli Wu, Johannes Stegmaier, and Dorit Merhof. 2023. “SortedAP: Rethinking Evaluation Metrics for Instance Segmentation.” In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, 3923–29. https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html.\n\n\nRedmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016. “You Only Look Once: Unified, Real-Time Object Detection.” In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 779–88. https://doi.org/10.1109/CVPR.2016.91.\n\n\nZhong, Hao, Zheyu Zhang, Haoran Liu, Jinzhuo Wu, and Wenshu Lin. 2024. “Individual Tree Species Identification for Complex Coniferous and Broad-Leaved Mixed Forests Based on Deep Learning Combined with UAV LiDAR Data and RGB Images.” Forests 15 (2). https://doi.org/10.3390/f15020293.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Model and training</span>"
    ]
  },
  {
    "objectID": "qmd-files/results.html",
    "href": "qmd-files/results.html",
    "title": "5  Results",
    "section": "",
    "text": "5.1 Training parameters\nThe first experiment was a simple test over the different parameters regarding the training loop. There were two goals to this experiment. The first one was to find the best training parameters for the next experiments. The second one was to see if randomly dropping one of the inputs of the model (either RGB/CIR or CHM) could help the model by pushing it to learn to make the best out of the two types of data.\nThe different parameters that are tested here are:\nAs you can see on Figure 5.1, sortedAP reaches at best values just above 0.3. The reason why the column name is “Best sortedAP” is due to the dataset being too small. Since the dataset is small, the training process overfits quickly, and the model doesn’t have enough training steps to have confidence scores which reach very high values. As a consequence, it is difficult to know beforehand which confidence threshold to choose. Therefore, the sortedAP metric is computed over several different confidence thresholds, and the one that gives the best value of sortedAP is kept.\nWith this experiment, we can see that a learning rate of 0.01 seems to make the training too much unstable, while 0.001 doesn’t give very high score. Then, we can also see how unstable the training process is in general, which comes mostly from the dataset being too small. However, a learning rate between 0.0025 and 0.006 seems to give the most stable results, when the drop probability is 0. This seems to show that the idea of randomly dropping one of the two inputs doesn’t really help the model to learn.\nFigure 5.1: Results with different training parameters for all experiments\nIn the next graph (Figure 5.2), we can see more results for the same experiments. Here, the results are colored according to the data that we use to evaluate the model. In blue, we see the value of sortedAP when we evaluate the model with the CHM layers data and dummy zero arrays as RGB/CIR data. These dummy arrays are also those that are used as input when one of the channel is dropped during training, when we have a drop probability larger than 0. Some interesting patterns appear in some of the cells in this plot. Firstly, it looks like randomly dropping one of the two inputs with the same probability has a much larger influence over the results using RGB/CIR than CHM. While CHM gives better results than RGB/CIR when always training using everything, RGB/CIR seems to perform better alone when also trained alone, even outperforming the combination of both inputs in certain cases.\nFigure 5.2: Results with different training parameters for all evaluation data setups\nFrom the results of this experiment, I decided to pick the following parameters for the next experiments:",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "qmd-files/results.html#training-parameters",
    "href": "qmd-files/results.html#training-parameters",
    "title": "5  Results",
    "section": "",
    "text": "“Learn. rate”: the initial learning rate.\n“Prob. drop”: the probability to drop either RGB/CIR or CHM. The probability is the same for the two types, which means that if the displayed value is 0.1, then all data will be used 80% of the time, while only RGB/CIR and only CHM both happen 10% of the time.\n“Accum. count”: the accumulation count, which means the amount of training data to process and compute the loss on before performing gradient back-propagation.\n\n\n\n\n\n\n\n\nInitial learning rate: 0.004\nDrop probability: 0\nAccumulation count: 10",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "qmd-files/results.html#sec-results-chm",
    "href": "qmd-files/results.html#sec-results-chm",
    "title": "5  Results",
    "section": "5.2 CHM layers",
    "text": "5.2 CHM layers\nThe goal of the second experiment was to test the model with different layers of CHM, to see whether more layers can improve the results. I tried two different ways to cut the point cloud into intervals. In both cases, we first define the height thresholds \\([t_1, t_2, \\cdots, t_n]\\) that will be used. Then, there are two possibilities for the height intervals to use to compute the CHM layers: \\[\n\\begin{array}{rl}\n\\text{Disjoint = True:} & [[-\\infty, t_1], [t_1, t_2],[t_2, t_3], \\cdots, [t_{n-1}, t_n], [t_n, \\infty]] \\\\\n\\text{Disjoint = False:} & [[-\\infty, t_1], [-\\infty, t_2], \\cdots, [-\\infty, t_n], [-\\infty, \\infty]]\n\\end{array}\n\\]\nThe results of this experiment can be found in Figure 5.3. To experiment on other parameters, half of the models were trained to be agnostic while the other half was not, to see if the way trees are separated in the four classes has an impact on performance, either facilitating the learning or hindering the generalization. On this plot, the borders correspond to the list of height thresholds \\([t_1, t_2, \\cdots, t_n]\\).\n\n\n\n\n\n\n\n\nFigure 5.3: Results with different CHM layers\n\n\n\n\n\nThese results are difficult to interpret. The first clear effect that we can notice for the agnostic models is that in the disjoint method (first row starting from the top), when separating the LiDAR point cloud in too many layers, the model doesn’t manage to make extract information from the CHM. When using only one CHM layer on the whole point cloud (which corresponds to (Borders = [])), we can see that the model performs poorly when using only one data type, but as well as the other models when using both. The explanation for this may be related to the data augmentation pipeline, because there is random channel dropout during the training. When there are multiple channels in the input, almost all of them can be randomly dropped out, with the limitation that at least one channel will always remain. This is the reason why there are red, green and blue (two channels dropped) but also purple and cyan (one channel dropped) images in Figure 3.4. But when there is only one channel, this channel is never dropped, which doesn’t force the model to learn how to use only part of the data.\nThe other correlation that is visible relates to how models perform using only RGB/CIR or CHM when being agnostic or not. It is logical for this parameter to have an impact on the results, since teaching the model to either make a difference between some of the trees or not will have an impact on what it learns and what it has to focus on to identify the right class when not being agnostic. From what we see here in Figure 5.3, it looks like having to make a difference between the different classes is significantly hinders its performance whatever combination of input data is used, even though it is more prevalent when using only one input.\nBesides these observations, it is hard to draw anymore conclusion from the rest of the experiments. There is too much variation in the results, which shows once again how unstable the training process is. From these results and the previous paragraph, one interpretation could be that the augmentation pipeline is related to this instability, as the results with the basic CHM layer have a much smaller variance.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "qmd-files/results.html#covered-trees",
    "href": "qmd-files/results.html#covered-trees",
    "title": "5  Results",
    "section": "5.3 Covered trees",
    "text": "5.3 Covered trees\nThen, if we try to look at the performance of the models on the covered trees, which are called “Tree_low_hard” in Figure 5.4, it is also hard to draw any conclusion. The models have mainly learnt to find trees with the generic “Tree” label, and they are seemingly equally bad at finding the other classes of trees. In Figure 5.4, we can see the performance of two models trained with the same repartition of the data into training, validation and test set. Both models were not agnostic, which means that they learnt to detect each class of trees and label them properly. The one of the left uses the largest number of CHM layers (with Borders = [1, 2, 3, 5, 7, 10, 15, 20] and Disjoint = False), whereas the one on the right only uses the default CHM layer (Borders = []).\n\n\n\n\n\n\n\n\n\n\n\n(a) Results with all layers on training set\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Results with one layer on training set\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(c) Results with all layers on validation set\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(d) Results with one layer on validation set\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(e) Results with all layers on test set\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(f) Results with one layer on test set\n\n\n\n\n\n\n\nFigure 5.4: sortedAP curve with different CHM layers on training/validation/test set\n\n\n\nUnfortunately, these two examples are really representative of the results of the other models that were trained. There are only 207 covered trees in the dataset, which is too small to get the models to learn to identify them and get interesting results when comparing different configurations of CHM layers. Most of the differences that we see come from the normal random variation during the training.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "qmd-files/results.html#visual-results",
    "href": "qmd-files/results.html#visual-results",
    "title": "5  Results",
    "section": "5.4 Visual results",
    "text": "5.4 Visual results\nIn Figure 5.5 are the outputs of trained models on one instance from each of the training, validation and test sets. The models used are those which results are shown in Figure 5.4.\n\n\n\n\n\n\n\n\n\n\n\n(a) Ground-truth boxes on one training set instance\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(b) Predicted boxes with all layers on one training set instance\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(c) Predicted boxes with one layer on one training set instance\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(d) Ground-truth boxes on one validation set instance\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(e) Predicted boxes all layers on one validation set instance\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(f) Predicted boxes one layer on one validation set instance\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n(g) Ground-truth boxes on one test set instance\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(h) Predicted boxes all layers on one test set instance\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n(i) Predicted boxes one layer on one test set instance\n\n\n\n\n\n\n\nFigure 5.5: Ground-truth boxes and predictions from two models\n\n\n\nThe confidence thresholds used to remove low confidence predictions are the ones that gave the best sortedAP values in Figure 5.4. One aspect that is partly visible here is that all the categories except the basic “Tree” have confidence scores which are so low that they are rejected after the selection with the confidence threshold. This is the case on the whole dataset, including the training set, and shows how the models learns more slowly to find the other categories.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Results</span>"
    ]
  },
  {
    "objectID": "qmd-files/discussion.html",
    "href": "qmd-files/discussion.html",
    "title": "6  Discussion and improvements",
    "section": "",
    "text": "6.1 Dataset\nFrom the results of this work, the only certitude is that more training data would be necessary to confirm any conclusion. Even with augmentation techniques, the dataset is too small to completely train a model and really experiment with the small changes applied to it. Since the training loop quickly reaches overfitting, we don’t really get to see how the model could perform in the most interesting cases, which are the small, covered or hardly visible trees.\nTherefore, the biggest and conceptually simplest improvement that could be done to this work would be to improve and extend the dataset. Improving with more diversity, covering a larger part of the Netherlands (or even beyond, but we can only ensure consistency over images and point clouds in this country), and extending with more images and more trees.\nAt the time of writing this report, the newest version of the point cloud has also been released for one third of the Netherlands, including the area of the current dataset. This new data could be better for this project, because it was acquired in 2023, the same year as the images that are used in the dataset.\nAnother approach to generate a larger dataset could be to create a large artificially annotated dataset, like it was done in another paper (B. G. Weinstein et al. 2020). Their approach was to create a very large dataset with medium-quality data which can be used to pre-train the model. Then, they use hand-annotated data to finish the training. They created this large dataset using classical non-machine learning techniques, using only the point cloud.\nFinally, it would also be interesting to see if the NEON tree dataset (B. Weinstein, Marconi, and White 2022) can be used to train a model and make some experiments. This would not allow to experiment on covered trees, but it could still show whether multiple layers of CHM can help delineating individual trees.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discussion and improvements</span>"
    ]
  },
  {
    "objectID": "qmd-files/discussion.html#instability",
    "href": "qmd-files/discussion.html#instability",
    "title": "6  Discussion and improvements",
    "section": "6.2 Instability",
    "text": "6.2 Instability\nAs explained in the previous section Section 6.1, the size of the dataset is probably the main reason for the instability of the training pipeline. But other reasons might also be responsible for this instability. As explained in Section 5.2, the random channel dropouts might also destabilize the training by creating inputs with very different repartition of the information, since some channels are randomly removed. The value to use as a replacement of these channels is also not easy to choose, as on the normalized CHM rasters, a value of 0 might be equivalent to a height above ground of 10 m for example, and a flat surface at 10 m is not exactly no data and might also be misleading if the CHM usually goes from 0 to 4 m.\nMore generally, it is possible that other factors are responsible for the instability of the training pipeline, and it would be useful to find and correct them.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discussion and improvements</span>"
    ]
  },
  {
    "objectID": "qmd-files/discussion.html#model-performance",
    "href": "qmd-files/discussion.html#model-performance",
    "title": "6  Discussion and improvements",
    "section": "6.3 Model performance",
    "text": "6.3 Model performance\nEven though the results displayed in Chapter 5 are far from perfect, they are still promising for several reasons.\nFirst, the overall results of the model on the general trees are quite impressive, given that only about 1600 trees are used to train each model, with only about 150 images covering about 0.4 km². Even though the whole dataset is quite homogenous, this still shows that the model is capable of generalizing relatively well.\nThen, a few correlations have been noticed regarding some parameters or combinations of parameters, even though more robust experiments would be necessary to confirm them. This is for example the case between agnostic and non-agnostic models, where the former seem to perform better in general.\nHowever, the models don’t perform good enough in the difficult areas to be able to draw conclusion on the modifications of the architecture. The overall performance of the model, which rarely gets a sortedAP score above 0.3, is also relatively low. But this low score mostly comes from the amount of small trees which have a relatively high importance in the computation of sortedAP because each tree has the same weight, no matter their size. In contrast, the models perform quite well on the large trees, finding most of them in situations that are not too much complex.\nFinally, it could be interesting to try another method to extract multiple CHM layers, which consists in removing only the points of the point cloud which corresponds to the previous CHM and all those which are close below them, and repeating the process to scan downwards. This has the advantage of being more adaptive than using fixed height thresholds for the whole dataset.\n\n\n\n\nWeinstein, Ben G., Sergio Marconi, Mélaine Aubry-Kientz, Gregoire Vincent, Henry Senyondo, and Ethan P. White. 2020. “DeepForest: A Python Package for RGB Deep Learning Tree Crown Delineation.” Methods in Ecology and Evolution 11 (12): 1743–51. https://doi.org/10.1111/2041-210X.13472.\n\n\nWeinstein, Ben, Sergio Marconi, and Ethan White. 2022. “Data for the NeonTreeEvaluation Benchmark (0.2.2).” Zenodo. https://doi.org/10.5281/zenodo.5914554.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discussion and improvements</span>"
    ]
  },
  {
    "objectID": "qmd-files/conclusion.html",
    "href": "qmd-files/conclusion.html",
    "title": "Conclusion",
    "section": "",
    "text": "All in all, the results of this internship are interesting and promising, even if not decisive.\nRegarding datasets, the new dataset that was created is promising for several reasons. First, its spatial extent can easily be extended since the raw data that is used is publicly available and covers the whole of the Netherlands. Then, its quality will also surely keep increasing in the future with the different iterations of the images and the point clouds, at the cost of small annotations modifications to update the dataset with new and cut-off trees, as well as tree growth. The diversity of trees and environments from the Netherlands is obviously not even close from what we can find all around the earth, which wouldn’t make it a great dataset to train a global model, but it hat the potential to be a perfect playground for testing new methods. Finally, the main drawback of this dataset are the spatial and temporal shifts between each type of raw data. But this shift has at least proven to be manageable by the deep learning models that were trained here. Having this shift is also interesting because counting on having perfectly aligned RGB images and point cloud is even less likely than having both of them available in the first place.\nRegarding the model, it is unclear whether having multiple layers of CHM really improves the results. This is because these layers would have the biggest impact in the detection of covered trees, which are a specific case that is harder than the other trees. And the training dataset was too small, the model overfitted quickly and could really reach the state when it start learning to find these harder trees. Therefore, more experiments on a bigger dataset, maybe using better augmentation techniques, would be required to get an answer. Besides that, the architecture in itself proved to provide great performance and is quickly able to learn to detect the medium and large trees. Some interesting improvements could easily be added to the model, such as the prediction of mask instead of bounding boxes, which only requires to change the detection heads, or the prediction of species. However, these changes would require the dataset to be substantially with species and precise delineations for all trees.",
    "crumbs": [
      "Conclusion"
    ]
  },
  {
    "objectID": "qmd-files/references.html",
    "href": "qmd-files/references.html",
    "title": "Bibliography",
    "section": "",
    "text": "Actueel Hoogtebestand Nederland. 2020. “AHN4\n- Actual Height Model of the Netherlands.” https://www.ahn.nl/.\n\n\nArevalo-Ramirez, Tito, Anali Alfaro, José Figueroa, Mauricio\nPonce-Donoso, Jose M. Saavedra, Matías Recabarren, and José Delpiano.\n2024. “Challenges for Computer Vision as a Tool for Screening\nUrban Trees Through Street-View Images.” Urban Forestry &\nUrban Greening 95: 128316. https://doi.org/10.1016/j.ufug.2024.128316.\n\n\nBeeldmateriaal Nederland. 2024. “Luchtfoto’s\n(Aerial Photographs).” https://www.beeldmateriaal.nl/luchtfotos.\n\n\nBordeaux Métropole. 2024. “Patrimoine arboré\nde Bordeaux Métropole (Tree Heritage of Bordeaux\nMetropole).” https://opendata.bordeaux-metropole.fr/explore/dataset/ec_arbre_p/information/?disjunctive.insee.\n\n\nBountos, Nikolaos Ioannis, Arthur Ouaknine, and David Rolnick. 2023.\n“FoMo-Bench: A Multi-Modal, Multi-Scale and Multi-Task Forest\nMonitoring Benchmark for Remote Sensing Foundation Models.”\narXiv Preprint arXiv:2312.10114. https://arxiv.org/abs/2312.10114.\n\n\nBuslaev, Alexander, Vladimir I. Iglovikov, Eugene Khvedchenya, Alex\nParinov, Mikhail Druzhinin, and Alexandr A. Kalinin. 2020.\n“Albumentations: Fast and Flexible Image Augmentations.”\nInformation 11 (2). https://doi.org/10.3390/info11020125.\n\n\nChen, Long, Yuli Wu, Johannes Stegmaier, and Dorit Merhof. 2023.\n“SortedAP: Rethinking Evaluation Metrics for Instance\nSegmentation.” In Proceedings of the IEEE/CVF International\nConference on Computer Vision (ICCV) Workshops, 3923–29. https://openaccess.thecvf.com/content/ICCV2023W/BIC/html/Chen_SortedAP_Rethinking_Evaluation_Metrics_for_Instance_Segmentation_ICCVW_2023_paper.html.\n\n\nCoöperatief Boomregister U.A. 2014. “Boom Register (Tree\nRegister).” https://boomregister.nl/.\n\n\nDiab, Ahmed, Rasha Kashef, and Ahmed Shaker. 2022. “Deep Learning\nfor LiDAR Point Cloud Classification in Remote Sensing.”\nSensors (Basel) 22 (20): 7868. https://doi.org/10.3390/s22207868.\n\n\nEysn, Lothar, Markus Hollaus, Eva Lindberg, Frédéric Berger,\nJean-Matthieu Monnet, Michele Dalponte, Milan Kobal, et al. 2015.\n“A Benchmark of Lidar-Based Single Tree Detection Methods Using\nHeterogeneous Forest Data from the Alpine Space.”\nForests 6 (5): 1721–47. https://doi.org/10.3390/f6051721.\n\n\nFreudenberg, Maximilian, Paul Magdon, and Nils Nölke. 2022.\n“Individual Tree Crown Delineation in High-Resolution Remote\nSensing Images Based on u-Net.” Neural Computing and\nApplications 34 (24): 22197–207. https://doi.org/10.1007/s00521-022-07640-4.\n\n\nGemeente Amsterdam. 2024. “Bomenbestand Amsterdam (Amsterdam\nTree Dataset).” https://maps.amsterdam.nl/open_geodata/?k=505.\n\n\nGomes, Marilia Ferreira, and Philippe Maillard. 2016. “Detection\nof Tree Crowns in Very High Spatial Resolution Images.” In\nEnvironmental Applications of Remote Sensing, edited by Maged\nMarghany. Rijeka: IntechOpen. https://doi.org/10.5772/62122.\n\n\nGougeon, François A et al. 1998. “Automatic Individual Tree Crown\nDelineation Using a Valley-Following Algorithm and Rule-Based\nSystem.” In Proc. International Forum on Automated\nInterpretation of High Spatial Resolution Digital Imagery for Forestry,\nVictoria, British Columbia, Canada, 11–23. Citeseer. https://d1ied5g1xfgpx8.cloudfront.net/pdfs/4583.pdf.\n\n\nHu, J., R. Liu, D. Hong, A. Camero, J. Yao, M. Schneider, F. Kurz, K.\nSegl, and X. X. Zhu. 2023. “MDAS: A New Multimodal Benchmark\nDataset for Remote Sensing.” Earth System Science Data\n15 (1): 113–31. https://doi.org/10.5194/essd-15-113-2023.\n\n\nInstitut national de l’information géographique et forestière (IGN).\n2020. “LiDAR HD.” https://geoservices.ign.fr/lidarhd.\n\n\n———. 2021. “BD ORTHO.” https://geoservices.ign.fr/bdortho.\n\n\nJucker, Tommaso, Fabian Jörg Fischer, Jérôme Chave, David A. Coomes,\nJohn Caspersen, Arshad Ali, Grace Jopaul Loubota Panzou, et al. 2022.\n“Tallo: A Global Tree Allometry and Crown Architecture\nDatabase.” Global Change Biology 28 (17): 5254–68. https://doi.org/10.1111/gcb.16302.\n\n\nKalinicheva, Ekaterina, Loic Landrieu, Clément Mallet, and Nesrine\nChehata. 2022. “Multi-Layer Modeling of Dense Vegetation from\nAerial LiDAR Scans.” https://arxiv.org/abs/2204.11620.\n\n\nKwak, Doo-Ahn, Woo-Kyun Lee, Jun-Hak Lee, Greg S. Biging, and Peng Gong.\n2007. “Detection of Individual Trees and Estimation of Tree Height\nUsing LiDAR Data.” Journal of Forest Research 12 (6):\n425–34. https://doi.org/10.1007/s10310-007-0041-9.\n\n\nLi, Yingbo, Guoqi Chai, Yueting Wang, Lingting Lei, and Xiaoli Zhang.\n2022. “ACE r-CNN: An Attention Complementary and Edge\nDetection-Based Instance Segmentation Algorithm for Individual Tree\nSpecies Identification Using UAV RGB Images and LiDAR Data.”\nRemote Sensing 14 (13). https://doi.org/10.3390/rs14133035.\n\n\nOuaknine, Arthur, Teja Kattenborn, Etienne Laliberté, and David Rolnick.\n2023. “OpenForest: A Data Catalogue for Machine Learning in Forest\nMonitoring.” https://arxiv.org/abs/2311.00277.\n\n\nPollock, Richard James. 1996. “The Automatic Recognition of\nIndividual Trees in Aerial Images of Forests Based on a Synthetic Tree\nCrown Image Model.” PhD thesis, The University of British\nColumbia (Canada). https://dx.doi.org/10.14288/1.0051597.\n\n\nPuliti, Stefano, Grant Pearse, Peter Surový, Luke Wallace, Markus\nHollaus, Maciej Wielgosz, and Rasmus Astrup. 2023. “FOR-Instance:\nA UAV Laser Scanning Benchmark Dataset for Semantic and Instance\nSegmentation of Individual Trees.” https://arxiv.org/abs/2309.01279.\n\n\nQin, Haiming, Weiqi Zhou, Yang Yao, and Weimin Wang. 2022.\n“Individual Tree Segmentation and Tree Species Classification in\nSubtropical Broadleaf Forests Using UAV-Based LiDAR, Hyperspectral, and\nUltrahigh-Resolution RGB Data.” Remote Sensing of\nEnvironment 280: 113143. https://doi.org/10.1016/j.rse.2022.113143.\n\n\nRedmon, Joseph, Santosh Divvala, Ross Girshick, and Ali Farhadi. 2016.\n“You Only Look Once: Unified, Real-Time Object Detection.”\nIn 2016 IEEE Conference on Computer Vision and Pattern Recognition\n(CVPR), 779–88. https://doi.org/10.1109/CVPR.2016.91.\n\n\nReiersen, Gyri, David Dao, Björn Lütjens, Konstantin Klemmer, Kenza\nAmara, Attila Steinegger, Ce Zhang, and Xiaoxiang Zhu. 2022.\n“ReforesTree: A Dataset for Estimating Tropical Forest Carbon\nStock with Deep Learning and Aerial Imagery.” https://arxiv.org/abs/2201.11192.\n\n\nSafonova, Anastasiia, Emilio Guirado, Yuriy Maglinets, Domingo\nAlcaraz-Segura, and Siham Tabik. 2021. “Olive Tree Biovolume from\nUAV Multi-Resolution Image Segmentation with Mask r-CNN.”\nSensors 21 (5): 1617. https://doi.org/10.3390/s21051617.\n\n\nSun, Chenxin, Chengwei Huang, Huaiqing Zhang, Bangqian Chen, Feng An,\nLiwen Wang, and Ting Yun. 2022. “Individual Tree Crown\nSegmentation and Crown Width Extraction from a Heightmap Derived from\nAerial Laser Scanning Data Using a Deep Learning Framework.”\nFrontiers in Plant Science 13. https://doi.org/10.3389/fpls.2022.914974.\n\n\nVincent, L., and P. Soille. 1991. “Watersheds in Digital Spaces:\nAn Efficient Algorithm Based on Immersion Simulations.” IEEE\nTransactions on Pattern Analysis and Machine Intelligence 13 (6):\n583–98. https://doi.org/10.1109/34.87344.\n\n\nWang, Yunsheng, Juha Hyyppä, Xinlian Liang, Harri Kaartinen, Xiaowei Yu,\nEva Lindberg, Johan Holmgren, et al. 2016. “International\nBenchmarking of the Individual Tree Detection Methods for Modeling 3-d\nCanopy Structure for Silviculture and Forest Ecology Using Airborne\nLaser Scanning.” IEEE Transactions on Geoscience and Remote\nSensing 54 (9): 5011–27. https://doi.org/10.1109/TGRS.2016.2543225.\n\n\nWeinstein, Ben. 2023. “MillionTrees.” 2023. https://milliontrees.idtrees.org/.\n\n\nWeinstein, Ben G., Sergio Marconi, Mélaine Aubry-Kientz, Gregoire\nVincent, Henry Senyondo, and Ethan P. White. 2020. “DeepForest: A\nPython Package for RGB Deep Learning Tree Crown Delineation.”\nMethods in Ecology and Evolution 11 (12): 1743–51. https://doi.org/10.1111/2041-210X.13472.\n\n\nWeinstein, Ben G., Sergio Marconi, Stephanie Bohlman, Alina Zare, and\nEthan White. 2019. “Individual Tree-Crown Detection in RGB Imagery\nUsing Semi-Supervised Deep Learning Neural Networks.” Remote\nSensing 11 (11). https://doi.org/10.3390/rs11111309.\n\n\nWeinstein, Ben, Sergio Marconi, and Ethan White. 2022. “Data for\nthe NeonTreeEvaluation Benchmark (0.2.2).” Zenodo. https://doi.org/10.5281/zenodo.5914554.\n\n\nWulder, Mike, K.Olaf Niemann, and David G. Goodenough. 2000.\n“Local Maximum Filtering for the Extraction of Tree Locations and\nBasal Area from High Spatial Resolution Imagery.” Remote\nSensing of Environment 73 (1): 103–14. https://doi.org/10.1016/S0034-4257(00)00101-2.\n\n\nZhong, Hao, Zheyu Zhang, Haoran Liu, Jinzhuo Wu, and Wenshu Lin. 2024.\n“Individual Tree Species Identification for Complex Coniferous and\nBroad-Leaved Mixed Forests Based on Deep Learning Combined with UAV\nLiDAR Data and RGB Images.” Forests 15 (2). https://doi.org/10.3390/f15020293.",
    "crumbs": [
      "Bibliography"
    ]
  }
]